[ { "title": "Automated release with Semantic Release and commitizen", "url": "/posts/automated-release-with-semantic-release-and-commitizen/", "categories": "Automation", "tags": "typescript, release", "date": "2023-09-29 10:14:35 +0100", "snippet": "When working with JavaScript projects, managing version numbers and commit messages is important for the maintainability of the project. Since 2020 I have been the main developer of Atomic Calendar...", "content": "When working with JavaScript projects, managing version numbers and commit messages is important for the maintainability of the project. Since 2020 I have been the main developer of Atomic Calendar Reviver a highly customisable Home Assistant calendar card, I found maintaining versions and releases to be cumbersome until recently. In this article, I will introduce the commitizen and semantic-release packages for creation or appropriate commit messages and semantic versioning. I will also provide examples of how I am currently using these packages to streamline my release workflow and project maintenance.The old daysStarting out I had never developed a project like this in TypeScript, I had only ever worked on Python projects, So I was running yarn run build which would run rollup, build my js files into a dist folder, I would then commit the changes to a branch, create a PR, merge the PR then manually tag the new version and create the release on GitHub.As you can see there are quite a few manual steps to achieve this which took too much time, time I could be spending on new features or bug fixes.I knew there had to be a better way to do this, there was no way large teams were wasting this much time on releases and as a small one person dev, its even more important to save as much time as possible.What is Semantic-release and how does it work?semantic-release uses the commit message to determine the impact of changes in the codebase, with this it is able to automate updating the version number correctly and managing the release process for the project.Semantic-release performs the following basic operations: Analyses commit messages to determine whether a new version is required. If a new version is required, automatically determines the appropriate version number. Updates the CHANGELOG file and creates the relevant Git tag. Publishes a github release if required Publishes the new version to the package manager if required.There are many other actions that it can perform via a great plugin architecture, so these are not limited to github / npm.What is Commitizen and how does it work?Commitizen helps developers write commit messages in the same format, this also ensures that all commit messages follow the semantic versioning requirements.Commitizen provides an interactive interface that prompts developers for specific information relating to that change, it then generates the commit message in the correct format, ensuring compatibility with semantic versions which ensures semantic-release can read the commit messages as expected. How to setup Semantic-release and CommitizenBelow is a guide on how to use Semantic-release and Commitizen packages in your project, these are settings that I currently use but you can amend them to better suit your project. Install Semantic-release and Commitizen packagesnpm install --save-dev semantic-release commitizen cz-conventional-changelogIf you find you are unable to use the git cz command after using the above install try this:npm install -g commitizenThis will install commitizen globally which seems to resolve the issue. Create a configuration file for semantic-release (.release.rc, release.config.js or package.json) an example of how I use package.json:\"release\": { \"plugins\": [ [ \"@semantic-release/commit-analyzer\", { \"preset\": \"conventionalcommits\" } ], [ \"@semantic-release/release-notes-generator\", { \"preset\": \"conventionalcommits\" } ], [ \"@semantic-release/npm\", { \"npmPublish\": false } ], [ \"@semantic-release/exec\", { \"prepareCmd\": \"yarn run build\" } ], \"@semantic-release/changelog\", [ \"@semantic-release/git\", { \"assets\": [ \"package.json\", \"changelog\" ], \"message\": \"chore(release): ${nextRelease.version} [skip ci]\\n\\n${nextRelease.notes}\" } ], [ \"@semantic-release/github\", { \"assets\": [ { \"path\": \"dist/atomic-calendar-revive.js\" } ] } ] ]}This configuration file contains settings used for semantic-release. Lets break this down: @semantic-release/commit-analyzer: Analyses commit messages and determines how the version number should be incremented (major, minor, patch). I use the conventional commits format so set the preset to inform commit-analyzer, this will look out for the ! to signify breaking changes etc. @semantic-release/release-notes-generator: Generates the release notes based on commit messages related to the new version. @semantic-release/npm: Updates the package.json file and publishes to the NPM package manager, I don’t publish to NPM manager so disabled this but i do need package.json updating with the latest version. @semantic-release/exec: This executes a command, in this case it will build my project ready for uploading to GitHub. @semantic-release/changelog: Creates or updates a CHANGELOG file based on the generated release notes. @semantic-release/git: Commits changes related to the new version to the git repository and creates the relevant git tag. I use this to commit the updated package.json into git and it also adds a commit message. @semantic-release/github: Publishes the new version to GitHub and creates the related GitHub release. I also upload the file generated by @semantic-release/exec Configure the cz-conventional-changelog adapter in package.json{ \"config\": { \"commitizen\": { \"path\": \"./node_modules/cz-conventional-changelog\" } }} Configure CI / CD, I use Github Actions in the below example:name: Releaseon: push: branches: - master - beta workflow_dispatch:jobs: release: runs-on: ubuntu-latest steps: - name: Checkout code uses: actions/checkout@v2 - name: Setup Node.js uses: actions/setup-node@v2 - name: Install dependencies run: yarn install - name: Run Semantic Release run: npx semantic-release env: GITHUB_TOKEN: $How to use semantic-release and commitizenCommitizenSimply run git add . git cz, this command will run the interactive interface of Commitizen and ask you to write a properly formatted commit message.Semantic ReleaseSemantic release by default uses the following branches: master Regular releases to the default distribution channel N.N.x or N.x.x or N.x with N being a number Regular releases to a distribution channel matching the branch name from any existing branch with a name matching a maintenance release range next Regular releases to the next distribution channel next-major Regular releases to the next-major distribution channel beta (pre-release) pre-releases to the beta distribution channel alpha (pre-release) pre-releases to the alpha distribution channel from the branchYou don’t need to use all of these branches, for example I currently only use the beta and master branches, All my development happens in beta, on commit it releases a pre-release version then once i’m happy its working I merge beta into master which will create the latest production release.How are version numbers determined from commit messagesSemantic release wil analyze the commit message to determine what the new version number should be. This process works by analysing the words and prefixes in the commit message. Commitizen facilitates this by ensuring the conventional commits format is followed.The conventional commit format states that commit messages should be formatted as follows:&lt;type&gt;([optional scope]): &lt;summary&gt;[optional body][optional footer(s)] ****: Indicates the type of change (e.g., fix, feat, chore, docs, refactor, test, etc.). [optional scope] (optional): Describes the part of the project that the change is applied to. &lt;summary&gt;: A concise description of the change. [optional body] (optional): A more detailed description of the change if required. [optional footer(s)] (optional): Add tags to issues / reviewers etc.Semantic-release uses this information to determine how to update the version number: If at least one of the commit messages is feat type, the new version number will be subject to a “minor” increase (0.1.0). If at least one of the commit messages contains BREAKING CHANGE in the body or ! after the type, the new version number will be subject to a major increase (1.0.0). In other cases, especially if at least one of the commit messages is fix type, the new version number will be subject to a patch increase (0.0.1).Final ThoughtsWith the help of semantic-release and commitizen packages, you can increase the quality and maintainability of your project by using automated versioning and creating appropriate, easy to understand commit messages. Due to this being compatible with CI/CD it also makes the development process more efficient saving precious time to be spent on improving the project or drinking coffee." }, { "title": "Hell Let Loose, better performance and visibility settings", "url": "/posts/hll-better-performance-visibility/", "categories": "Gaming", "tags": "hell let loose, hll, performance, visibility", "date": "2023-08-18 10:14:35 +0100", "snippet": "For anyone playing Hell Let Loose, you will have come across the annoyance of never being able to spot your enemy but they always seem to get you, Well the chances are they were using the settings ...", "content": "For anyone playing Hell Let Loose, you will have come across the annoyance of never being able to spot your enemy but they always seem to get you, Well the chances are they were using the settings I’m about to share with you.Launch OptionsFirst open steam, right click on Hell Let Loose and select Properties.Enter the following in the launch options:-dx12 -USEALLAVAILABLECORESThis will force the game to use DirectX 12 which will help increase FPS and force the game to use all of the available CPU Cores, by default the game will only use one single core which will limit the performance available.Other OptionsThere are other options recommended in other articles, however I don’t recommend setting these as they wont assist in the performance and in some cases could make it worse.-refresh 75 --malloc=systemPower PlanChanging the power plan to high performance wont increase the performance, but will increase your power consumption, this will basically tell the computer to always keep the resources available even when not needed for games etc. so is unnecessary.Nvidia UsersFor Nvidia users there are some additional options available which will give you the overall advantage to an AMD user.To do this: right click desktop &gt; nvidia control panel Go to Manage 3D Settings Select Program Settings tab Select the program Hell Let Loose you may need to add this manually via the add button. Update the following settings Power Management Mode - Prefer Maximum Performance Texture filtering - Quality - High Performance Vertical Sync - Off Game SettingsThese settings are to squeeze out the most FPS out of the game and the highest visibility we can get.Gameplay Tab Dead bodies despawn delay - 0.5 minOptionalThese optional settings are down to personal preference but this is how I set mine up: Hud Display Mode - Always On Player Nameplate Opacity - 70% / 75% this helps it not get in the way of players in front.Video Tab Brightness - 130%, but change this for what works for you. Texture Quality - Low Shadow Quality - Low Anti-Aliasing Method - Community TAA Anti-Aliasing Quality - High, Anything lower will make things blurred FX Quality - Low, makes it easier to see enemies through smokes / fires etc. View Distance - High Foliage Quality - Medium, any higher and additional bushes will be rendered making it harder to spot enemies in bushes etc. Postprocess Quality - Medium SSAO Motion Blur - OffIs this cheating? The following settings should probably be blocked by the developers but at this time are not against the rules ofSome people argue that changes outside of the game could be construed as cheating, the reason being that you are modifying the settings further than what the game developers intended.However, using every advantage available to you without modifying the game through another program e.g. aimbots etc. in my eyes is just being smart. Also if all other competitive teams use all the advantages available to them, then you should be too.Additional Nvidia SettingsGo back into the Nvidia control panel, but this time into the Adjust desktop color settings tab. Be aware these settings will change across the whole computer and not just whilst in the game Digital Vibrance - 65% / 70%, change this to preferred valueIn Game FiltersIf you have Nvidia Geforce Experience installed you are able to take advantage on the In Game filters.Adding these filters is very much personal preference, however below are the settings that I use which I feel work quite well: Press Alt-z or Alt-F3 Game Filters Add Filter Brightness / Contrast Exposure - 10% Contrast - 15% Shadows - -10% Colour Temperature - -11.6% Vibrance - 10.5% Details Sharpen - 22% Clarity - 100% HDR Cloning - 61% Tarkov have asked Nvidia to remove the game from these filters, so it is likely that HLL may also follow this and remove this ability in the futureFinal ThoughtsSome of these settings are quite controversial and some players class this as cheating, however until HLL make changes to ban these settings, other players will be using them and will always have an advantage over you.Let me know if you have any additional settings to assist with visibility of HLL" }, { "title": "Setup the Sunsynk Power Flow Card with a Lux Inverter", "url": "/posts/sunsynk-power-flow-card-with-lux-inverter/", "categories": "Home Automation", "tags": "home-assistant, sunsynk, lovelace", "date": "2023-08-15 10:14:35 +0100", "snippet": "If you haven’t already, I recommend going back to the first article in the series and following it through otherwise some options in this article may not work as expected.Card InstallThe card can b...", "content": "If you haven’t already, I recommend going back to the first article in the series and following it through otherwise some options in this article may not work as expected.Card InstallThe card can be installed via HACS with this linkOr if you dont use HACS, a manual installation can be found hereConfigurationAdd the Custom: Sunsynk Power Flow Card to your Dashboard view with the following base configuration:type: custom:sunsynk-power-flow-cardcardstyle: liteshow_solar: &#39;yes&#39;battery: energy: 12800 shutdown_soc: 1 show_daily: &#39;yes&#39;solar: show_daily: &#39;yes&#39; mppts: twoload: show_daily: &#39;yes&#39;grid: show_daily_buy: &#39;yes&#39; show_daily_sell: &#39;yes&#39; show_nonessential: &#39;no&#39; invert_grid: &#39;yes&#39;entities: inverter_voltage_154: sensor.lux_grid_voltage_live load_frequency_192: sensor.lux_grid_frequency_live inverter_current_164: sensor.inverter_output_current inverter_status_59: sensor.luxpower inverter_power_175: sensor.lux_battery_flow_live day_battery_charge_70: sensor.lux_battery_charge_daily day_battery_discharge_71: sensor.lux_battery_discharge_daily battery_voltage_183: sensor.lux_battery_voltage_live battery_soc_184: sensor.lux_battery battery_power_190: sensor.lux_battery_flow_live battery_current_191: sensor.lux_battery_capacity_ah grid_power_169: sensor.lux_grid_flow_live day_grid_import_76: sensor.lux_power_from_grid_daily day_grid_export_77: sensor.lux_power_to_grid_daily grid_ct_power_172: sensor.lux_grid_flow_live day_load_energy_84: sensor.lux_power_from_inverter_to_home_daily essential_power: sensor.lux_home_consumption_live nonessential_power: none aux_power_166: sensor.aux_output_power day_pv_energy_108: sensor.lux_solar_output_daily pv_total: sensor.lux_solar_output_live pv1_power_186: sensor.lux_solar_output_array_1_live pv2_power_187: sensor.lux_solar_output_array_2_live pv1_voltage_109: sensor.lux_solar_voltage_array_1_live pv1_current_110: none pv2_voltage_111: sensor.lux_solar_voltage_array_2_live pv2_current_112: none radiator_temp_91: sensor.lux_radiator_1_temperature_live dc_transformer_temp_90: sensor.lux_radiator_2_temperature_live remaining_solar: sensor.forecast_remaining_today energy_cost: sensor.octopus_energy_electricity_20e5081533_2380002009185_current_rateSystem specific settingsSome of these settings will need to be adjusted based on the solar system it is for: battery.energy - This should be the total battery capacity in Watts battery.shutdown_soc - This should be the percentage of battery that remains in the system when depleted solar.mppts - this is the number of solar arrays in the system energy_cost - this will be the entity for your energy provider price, if you don’t have this it can be removed " }, { "title": "Integrate Solcast API with Home Assistant", "url": "/posts/solcast-api-home-assistant/", "categories": "Home Automation", "tags": "home-assistant, solcast", "date": "2023-08-14 10:14:35 +0100", "snippet": "As I further integrate my solar solution into my home, I have now added the Solcast API.This API Allows you to plan your PV usage and if you should use cheap rate grid power to charge batteries in ...", "content": "As I further integrate my solar solution into my home, I have now added the Solcast API.This API Allows you to plan your PV usage and if you should use cheap rate grid power to charge batteries in preparation for a low solar day, also with the use of specific Home Assistant cards, you can see how much solar is left for the day.SetupHead over to Solcast and create a new account, selecting the home user option.Once an account is created and have verified, login and add a new site: It is not currently possible to implement two strings or two azimuth’s into the API, so I added two sites, one for East and one for West as I have two strings of 12 panels East and West facing.Solcast PV solar HA integrationIn order to see the data in Home Assistant add the following integration Solcast PV Solar OR if using HACS add a custom repository oziee/ha-solcast-solar and install the integration.In Home Assistant go to Settings -&gt; Devices &amp; Services -&gt; Add Integration search for Solcast PV Forecast and select it, when prompted enter the API key from the solcast websiteThis will then add the following entities to HA: " }, { "title": "Integrating a Lux Power Inverter with Home Assistant", "url": "/posts/integrate-lux-inverter-with-home-assistant/", "categories": "Home Automation", "tags": "home-assistant, lux-powertek, greenlinx, hanchuess", "date": "2023-08-08 12:05:51 +0100", "snippet": "After having solar installed I wanted to get it integrated into home assistant to enable historical monitoring and also automation around my home.At this time I haven’t implemented any automation,...", "content": "After having solar installed I wanted to get it integrated into home assistant to enable historical monitoring and also automation around my home.At this time I haven’t implemented any automation, that will come in the future but for now I will show you how I integrated it with Home Assistant and implemented a nice card to display usageLux Inverter SetupBy default, the datalogger plugged into the Lux sends statistics about your inverter to LuxPower in China. This is how their web portal and phone app knows all about you.We need to configure it to open another port that we can talk to. Open a web browser to your datalogger IP (might have to check your DHCP server to find it) and login with username/password admin/admin. Click English in the top right click Network Setting in the menu. You should see the network settings, the top one is populated with LuxPower’s server IP Address, the second one we can use.Configure it to look like the below and save: After the datalogger reboots, port 8000 on your inverter IP is accessible. You should be sure that this port is only accessible via your LAN, and not exposed to the Internet, or anyone can control your inverter.Home Assistant IntegrationFor this I use LuxPython to get access fill out this formInstall LuxPythonDownload the repository and copy the folder custom_components/luxpower to home assistant /config folder.Once copied REBOOT HA this step is required otherwise it wont work.Setup the integrationOpen up Settings &gt; Devices and Services &gt; Add Integration and search for LuxPower Inverter If it doesn’t show up, clear your browser cache as it’s very likely your browser is the issue! Fill in your IP, Port (8000), dongle serial and inverter serial This can be found on the Lux website at server.luxpowertek.com Once you have added this into HA, you should see some sensors in HA: Use Developer Tools to view sensor.luxpower. Initially, the state will be Waiting but after a few minutes when the inverter pushes an update the state will change to ONLINE and data will be populated in the attributes.Manual RefreshTo manually refresh data, add the following button to the dashboard:show_name: trueshow_icon: truetype: buttontap_action: action: call-service service: luxpower.luxpower_refresh_registers service_data: dongle: BA******** target: {}entity: ''icon_height: 50pxicon: mdi:cloud-refreshname: Refresh LUX Datashow_state: false replace BA******** with your dongle serialInverter disconnects oftenThe inverter dongle often disconnects, to solve the issue of data not flowing please import the reconnection blueprint.It will allow you to reconnect if the inverter doesn’t report for X minutes (I would set it to 20 minutes but absolutely no lower than 10)ConclusionIf you have followed all of this, you should now have your Lux inverter linked to home assistant and have your usage visible on the dashboard card.If you have any problems let me know in the comments section or discord and I will assist where possible." }, { "title": "A Pythonic way to alias methods?", "url": "/posts/python-method-aliases/", "categories": "Python", "tags": "python, aliases, method", "date": "2023-08-08 10:39:37 +0100", "snippet": "An alias method allows accesses to the original method via a different name. You can define your own alias method by adding the statement a = b to your class definition. This creates an alias metho...", "content": "An alias method allows accesses to the original method via a different name. You can define your own alias method by adding the statement a = b to your class definition. This creates an alias method a() for the original method b().For my situation this wasn’t really ideal, I wanted to standardise all of my method names easily with a deprecation warning when the old method was used. To do that, I decided the easiest way would be to use decorators with the ability to add the version the alias would be deprecated.I did this with the following code:import functoolsfrom typing import Any, Callable, Dict, Optional, Setimport warningsclass FunctionWrapper: &quot;&quot;&quot;Function wrapper&quot;&quot;&quot; def __init__(self, func: Callable[..., Any]) -&gt; None: self.func = func self._aliases: Set[str] = set()class alias(object): &quot;&quot;&quot;Add an alias to a function&quot;&quot;&quot; def __init__(self, *aliases: str, deprecated_version: str = None) -&gt; None: &quot;&quot;&quot;Constructor Args: deprecated_version (str, optional): Version number that deprecation will happen. Defaults to None. &quot;&quot;&quot; self.aliases: Set[str] = set(aliases) self.deprecated_version: Optional[str] = deprecated_version def __call__(self, f: Callable[..., Any]) -&gt; FunctionWrapper: &quot;&quot;&quot;call&quot;&quot;&quot; wrapped_func = FunctionWrapper(f) wrapped_func._aliases = self.aliases @functools.wraps(f) def wrapper(*args: Any, **kwargs: Any) -&gt; Any: &quot;&quot;&quot;Alias wrapper&quot;&quot;&quot; if self.deprecated_version: aliases_str = &quot;, &quot;.join(self.aliases) msg = f&quot;{aliases_str} is deprecated and will be removed in version {self.deprecated_version}. Use {f.__name__} instead.&quot; warnings.warn(msg, DeprecationWarning) return f(*args, **kwargs) wrapped_func.func = wrapper # Assign wrapper directly to func attribute return wrapped_funcdef aliased(aliased_class: Any) -&gt; Any: &quot;&quot;&quot;Class has aliases&quot;&quot;&quot; original_methods: Dict[str, Any] = aliased_class.__dict__.copy() for name, method in original_methods.items(): if isinstance(method, FunctionWrapper) and hasattr(method, &quot;_aliases&quot;): for alias in method._aliases: setattr(aliased_class, alias, method.func) # Also replace the original method with the wrapped function setattr(aliased_class, name, method.func) return aliased_classThis code allows for the following to be added to your class and method to add an alias, in my case, the method would be renamed to the new method name and then the alias would contain the old name.@aliasedclass MyClass(): @alias(\"old_method_name\", deprecated_version=\"v2.0.0\") def new_method_name(): print \"foo\"Are alias methods pythonic?Well, using aliases is not very pythonic, The Zen of Python clearly states that there should be one, and only one, way to accomplish a thing.&gt;&gt;&gt; import thisThe Zen of Python, by Tim PetersBeautiful is better than ugly.Explicit is better than implicit.Simple is better than complex.Complex is better than complicated.Flat is better than nested.Sparse is better than dense.Readability counts.Special cases aren't special enough to break the rules.Although practicality beats purity.Errors should never pass silently.Unless explicitly silenced.In the face of ambiguity, refuse the temptation to guess.There should be one-- and preferably only one --obvious way to do it.Although that way may not be obvious at first unless you're Dutch.Now is better than never.Although never is often better than *right* now.If the implementation is hard to explain, it's a bad idea.If the implementation is easy to explain, it may be a good idea.Namespaces are one honking great idea -- let's do more of those!This is why I added the deprecated_version option, as ideally the alias wont be around for long, but long enough for teams to migrate their code.Hopefully this will assist someone else who finds themselves, in an ideal world we would never use this, but sometimes its unavoidable to have a little mess to get to a tidier situation." }, { "title": "Add series links to Jekyll posts", "url": "/posts/jekyll-post-series-links/", "categories": "Website", "tags": "Jekyll, how-to", "date": "2023-06-09 09:06:40 +0100", "snippet": "Creating blog posts for my website I sometimes find that I want top create multiple articles as part of a series, usually because I have done some research and got to a stage that makes sense to ha...", "content": "Creating blog posts for my website I sometimes find that I want top create multiple articles as part of a series, usually because I have done some research and got to a stage that makes sense to have an article to itself, something like my recent post on Proxmox Template with Cloud Image and Cloud Init.Rather than having to manually link to other articles related to the series, I thought it would be better to have a section at the top that lists all articles related to the series.The metadataFor this to work each post that is required to be part of a series should contain some metadata with a name for that series. For example:---series: Automating Proxmox Deployments---With this metadata we are able to search for all articles that contain the series that matches Automating Proxmox Deployments. Each article must contain the same name exactly, if there are any spaces or punctuation that is different it will not work.Create an includeNow personally I like to add features like this as includes, it keeps the main posts.html file much cleaner and easier to understand.So go ahead and create a new include called post-series.html and add the code below:{% if page.series %}{% assign posts = site.posts | where: \"series\", page.series | sort: 'date' %}&lt;div class=\"row justify-content-center\" id=\"post-series\"&gt; &lt;div class=\"card series-list\"&gt; &lt;div class=\"card-header\"&gt; {{ page.series | upcase }} ({{ posts | size }} part series) &lt;/div&gt; &lt;ul class=\"list-group list-group-flush\"&gt; {% for post in posts %} {% if post.title == page.title %} &lt;a href=\"#\" class=\"list-group-item list-group-item-action active\" aria-current=\"true\"&gt;{{ post.title }}&lt;/a&gt; {% else %} &lt;a href=\"{{ post.url | relative_url }}\" class=\"list-group-item list-group-item-action\"&gt;{{ post.title }}&lt;/a&gt; {% endif %} {% endfor %} &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;{% endif %}This works as follows: Check that the page has the series medatada Get all posts that have series that match. Sort these in ascending order. Display a card with: The series name How many parts there are in the series Clickable link to the other posts in the series Add to layoutWe have everything we need to get this working, but we need to add it to the layout for our posts, edit the post.hhml file and add the include as follows:{% include post-series.html %}You can add this anywhere you would like it to appear on your post, for my website, I have it appear after the meta but before the article begins as per the below screenshot: Final ThoughtsWe now have a great new feature on our blog, super easy to add to the website and this is one of the reasons I love using Jekyll for my website, so much is possible and with very little effort.There are many additional features that could be added with this small snippet, for example you could create a page that shows all of the series that you have or you could add the series to a menu rather than just the top of the post page.Hope this helped!" }, { "title": "Last4Solar - My solar nightmare!", "url": "/posts/last4solar-my-solar-nightmare/", "categories": "Musings", "tags": "solar, battery", "date": "2023-06-02 22:13:35 +0100", "snippet": "At the end of 2022 I came into some inheritance, with the massive energy price increase in the UK I decided to spend this money on a solar and battery installation on my house.With the decreasing p...", "content": "At the end of 2022 I came into some inheritance, with the massive energy price increase in the UK I decided to spend this money on a solar and battery installation on my house.With the decreasing price of solar systems and the increase in price, the return on investment is getting smaller and making the option much more reasonable.Choosing a company for the installationI spent a long time looking at different solar companies in my area, the majority had extremely long waiting times around 6+ months before they could install. I then found YouTube recommendations for a local company that had brilliant TrustPilot reviews, they had been around for quite some time and their website was very informativeMost other companies I saw looked like a single person outfit which for me wasn’t really an ideal option as aftercare may become an issue especially if they retire or stop doing solar. in hindsight this shouldn’t have been a deciding factor.At this point I had decided that I would go with First4Solar, they were the clear winners in price and the initial delivery…The quoting processAfter getting in touch with First4Solar they asked some initial questions: What our yearly power consumption was The orientation of our roof Where we would like the panels If we wanted battery storage and how much What access was like to the roofsI told them all of this, additionally asking for automated whole house failover in case of a grid failure and as many panels as possible on the roof along with around 11kw batteries.So far so good.Until I got the first quote, which didn’t match what I had asked for, I assumed that this was just a miss-communication, they had provided a quote for a standard package: 16x 415W Jinko N-Type Panels GivEnergy 9.5kWh Battery with 100% DOD 3.6kW Charge / Discharge Can be used with off-peak tariffs Emergency Power Supply Compatible (manual)The issue I had with this was I wanted as many panels as possible on the roof, which this didn’t do, when adding the extra panels, the GivEnergy inverter wasn’t powerful enough, it would basically be at full capacity based on the above, and in my eyes its always better to have a bigger inverter to support the addition of more panels or other technology in the future, there was no growing room here which again wasn’t what I requested.Part of their reasoning for using the GivEnergy inverter was that its under 3.6kWh which means no DNO G99 application is required speeding up the process.I then asked for a custom quote that doesn’t follow the standard package, they then quoted for the following: 24x Jinko Tiger 415W N-Type Black Framed Mono SolaX G4 7.5kW Hybrid Inverter SolaX Triple Power HV 5.8kWh (Master) V2 SolaX Triple Power HV 5.8kWh (Slave) V2 EPS - Manual ChangeoverAgain you can see they didn’t add the automated failover, however they did say that if the power draw of the house was higher than the inverter could handle, it would stop until the load dropped, which I agreed made sense as I could shut off none essential items, but not really what I wanted as I have servers to keep running.In the end this second quote is what I went with after they persuaded me that it was best, I guess they were the experts so I would go with their recommendation.Paying the depositNow we are at the stage of paying the deposit, all seemed good and even the director called me to explain the process and to get my deposit paid ASAP to not miss my install date. So I called to try pay by card but “the card machine wasn’t working” and they recommend paying by BACS anyway. At the time I thought nothing of it, so I transferred the money via BACS.This was the first mistake that I made, however I was totally unaware at the time.The second mistake was believing being told once paid I would automatically be registered with HEIS so my deposit would be secure and covered by the HEIS insurance, which turned out not to be the case and I should have been contacted by HEIS.Radio SilenceAfter paying the deposit (7th Dec 2022) I had total radio silence, If I contacted them the person I needed was “on another call” or “off sick” or one of many other excuses.I finally managed to speak with someone who gave me an install date of 4th &amp; 5th May 2023, which they would later call and re-arrange for the 18th &amp; 19th May 2023.I then received a further call to re-arrange again, but told them if they moved the install date I would cancel the order and go elsewhere. After this I never heard from them again.There was no mention of the DNO G98/99 application and the install was about two weeks off, so again I tried contacting them around ten times which eventually got me through to someone who said it was in hand and they were waiting for the DNO to get back to them, I have contacted the DNO to see if they got the application and am currently awaiting a call back, although I suspect that there was never an application sent.Concern starts to set inOther family members were also having issues, dates being set back and different excuses every time. After a while the bad reviews started to pour into TrustPilot at this point I knew something was wrong and began to try get a refund, but all the phones had stopped working and were going to automated systems, even though surprisingly the sales line was still working and deposits were still being taken from customers!I never got a refund and never spoke to another person at F4S, I did find a facebook group with hundreds of people who were having the exact same issues.Is this fraud?The company took my deposit at a point where they must have known they were trading insolvent, but continued to take people’s money, I have heard from other customers that their credit cards were used to pay other suppliers of F4S.Raised with HEIS who I found out I was not registered and so my £3.5k was not covered by their insurance and I would need to take this up with the bank, the bank also would not touch this as I made a BACS payment (an expensive mistake to make)The TakeoverSo now we are at a state where i’m 3.5k down and potentially no longer able to afford a solar install, but there was some light at the end of the tunnel.A company called Contact Solar had purchased the customer list and agreed to do what they could to help the customers that F4S had left in limbo and without their deposits. This was an absolutely brilliant thing for them to do for all these customers, however I had concerns that with reports of 1500+ customers they would struggle to keep up with the installs and I could be waiting months again, this really did unsettle me.That said, I had no evidence that this was the case and Contact Solar provided a very competitive price and a good proposal based on the money left on the contract.Decision TimeI now had to choose between Contact Solar 24 x 405w JA Solar Panels SunSynk 8kw Hybrid Inverter 10.64kWh Battery Storageor ese group 24 x 405w Longi Solar Panels Lux Powertek LXP 7.6Kw Inverter 12.8kWh Greenlinx LXP LV-L3.2-1p battery storage Pigeon proofing 15 Year maintenance planAs you can see both offered quite a good deal, with reputable equipment that integrate well with my Home Assistant and Octopus EnergyThe main difference I found was the Depth of Discharge (DoD) on the SunSynk batteries was 90% but on Greenlinx batteries its 100%, plus the batteries are larger and the addition of the 15 year maintenance from ESE just made the deal a little better for me.I also found ESE seemed to have more time to answer my questions, they would always call back when they said and were very helpful, with contact solar, it was all via email which had slow responses and there wasn’t much of a personal touch that made you feel like they wanted your business.So as you can probably tell, my business went to ese groupSales AftercareOn 1st June 2023 I was at the stage of paying a deposit, getting DNO Approval, scheduling the survey and installation dates.After agreeing to continue I was passed over to make payment, recommended by ESE to pay by Credit Card for the added protection (I would have insisted on this anyway, but it was nice that they mentioned the added protection it offers and that it was their recommended payment method).Once I had paid I was asked when I would like the install, I asked ASAP due to being so delayed from my previous provider. The install date provided was the 22nd June! not even 4 weeks and they could have the installation completed, crazy to think how long I had been waiting that they could do this so quick.I agreed and was told I would get a call the following day to arrange a survey, sure enough at 09:30 I had a call to say that someone was in the area today doing another survey and could come do mine after, the surveyor came to check everything and said that it would either be him or one of the other members of the team that would carry out the install.The InstallI have now had my install completed, the team came and had the inverter, batteries and 1st string of panels installed on the first day, on the second day they got the final string installed and everything was done. The job was very tidy and looks great.I did have one issue where the battery kept draining itself and power usage was jumping all over the place, but on investigation I found that the CT Clamp was on upside down, easy mistake to make and it was easy enough for me to fix.The electrician got the dongle hooked up to my WiFi and helped setup the app on my phone. I did however have to contact ESE as by default they don’t leave the ESS Greenlinx battery dongles in, this means its not possible to individually monitor the batteries and should this be required in the future I would be stuck if ESE went into administration. (I’m now waiting for these to be shipped to me)AftercareGenerally ESE have been great, the install was quick and efficient.However I do believe I was slightly miss-lead by them, I went with them as the contract stated I would be able to use any energy provider to get SEG payments via the flexi-orb certificate which they said is the same as MCS but an alternative.The way this was worded lead me to believe I would be find with ANY energy supplier, that’s not the case, currently only 5 energy suppliers accept Flexi-orb and Octopus is not one of these. This is partially my fault as I didn’t double check or specifically ask if Octopus would accept it, but believe that the contract shouldn’t state SEG payments can be from any supplier I choose when in fact what they meant was any supplier that accepts Flexi-Orb certificates.The resultsHere you can see my solar, battery usage along with my import / export from the grid. The first full month of the install I have spent £6.95 on electricity and that was with some quite bad weather days. I’m still not on an export tariff so unsure how much additional funds this will yield but judging by the amount I have generated it should be a nice bit of money which will hopefully cover the standing charges.Things to check / be aware ofTo ensure that you don’t get stuck in the same situation, there are a few things I would highly recommend you take into consideration: Pay your deposit by Credit Card. If they insist on a Bank Transfer, REFUSE! They may give excuses like the card machine isn’t working, if they don’t offer you to pay another day, walk away! (I can’t stress this enough!) Bank Transfers (BACS) payments are not protected by your bank only credit cards are. Not all solar companies are MCS Certified, some issue Flexi-Orb Certificates instead. Not all energy suppliers accept flexi-orb, at the time of writing only five of the major suppliers accept them E.ON Scottish Power British Gas SSE OVO Energy It is likely that once Flexi-Orb is accredited that all energy suppliers will accept it, but at time of writing this is something to be aware of. HEIS will email you within 48 hours to confirm registration and your cover If you don’t get an email from HEIS, contact them immediately to ensure you have been registered, failure to do so will mean you are not protected by them. Honestly, from what I have heard, HEIS protection isn’t worth the paper its written on. You are only covered by HEIS for 120 days, if your install is going to be after this time, you wouldn’t be covered. If your install gets delayed and will breach the 120 days, contact HEIS and ask what could be done to ensure you are still protected You will need a DNO application approved before installation: DNO G98 - For installs under 16A per phase, which is the equivalent of 3.68kWp for a single-phase supply DNO G99 - For installs greater than 16A per phase Ensure the electrician is qualified ideally being registered with NICEIC Without a qualified electrician doing the install you will be unable to get a valid certificate Ensure you are provided with the necessary electrical certificates to ensure your install is legal Without this you would need to have an electrician do an EICR on this Don’t rely on your installer doing things correctly, check everythingFinal ThoughtsMy overall solar experience has been stressful to say the least, I’m glad that its finally getting sorted but its been a horrible situation for me and the other 1000 First 4 Solar customers that have been conned out of money, likely never to see it again!If you are looking for solar I have been very impressed with ese group so far, obviously I will update this based on the install but my Dad had his completed by them and they did a great job.Were you impacted by this nightmare? let me know in the comments how your install went." }, { "title": "Automating deployments using Terraform with Proxmox and ansible", "url": "/posts/automating-proxmox-with-terraform-ansible/", "categories": "Automation", "tags": "terraform, proxmox, ansible, automation", "date": "2023-05-06 10:03:29 +0100", "snippet": "Over the years my home lab has grown and become more and more difficult to maintain, especially because some servers I build and forget as they function so well.I have found recently though that mo...", "content": "Over the years my home lab has grown and become more and more difficult to maintain, especially because some servers I build and forget as they function so well.I have found recently though that moving to newer versions of operating systems can be difficult for the servers that I cant easily containerise at the moment.For this reason I have moved over to using Terraform with Proxmox and ansible.Telemate developed a Terraform provider that maps Terraform functionality to the Proxmox API, so start by defining the use of that provider in provider.tfterraform { required_version = \"&gt;=0.13.0\" required_providers { proxmox = { source = \"telmate/proxmox\" version = \"2.9.14\" } }}provider \"proxmox\" { # Configuration options pm_api_url = var.proxmox_api_url pm_api_token_id = var.proxmox_api_token_id pm_api_token_secret = var.proxmox_api_token_secret # Optional: skip TLS Verification pm_tls_insecure = true pm_parallel = 2 pm_timeout = 1200}Here we see two sections, the first of which contains the configuration for Terraform, it specifies the version of Terraform that is required along with all of the required providers and their required versions.The second section contains the provider itself, and the configuration, a full list of arguments can be found hereA provider is a plugin that allows Terraform to manage external APIs (such as proxmox)Now this is no good without some servers or “resources”, I create a file per resource for my lab, this keeps it simple for me, however you may want to do this differently depending on your requirements. Create a file with the resource name bastion.tf There is a lot more going on in this file, so I have added comments to it.# Specify the resource type, and then the nameresource \"proxmox_vm_qemu\" \"bastion\" { name = \"td-bast01\" # Name to call the VM desc = \"Bastion\" # Description for the VM target_node = var.proxmox_host # Proxmox target node clone = var.template_name # The name of the template that this resource will be created from agent = 1 # is the qemu agent installed? os_type = \"cloud-init\" # The OS type of the image clone cores = 2 # number of CPU cores sockets = 1 # number of CPU sockets cpu = \"host\" # The CPU type memory = 4096 # Amount of memory to allocate onboot = true # start the VM on host startup scsihw = \"virtio-scsi-pci\" # Scsi hardware type bootdisk = \"scsi0\" # The boot disk scsi # This section contains the disk configuration, it can be duplicated for additional disks disk { size = \"30G\" type = \"scsi\" storage = \"local-thin\" iothread = 0 } # if you want two NICs, just copy this whole network section and duplicate it network { model = \"virtio\" bridge = \"vmbr0\" tag = 20 } ipconfig0 = \"ip=172.16.20.43/24,gw=172.16.20.1\" nameserver = \"172.16.20.1\" serial { id = 0 type = \"socket\" } # sshkeys set using variables. the variable contains the text of the key. sshkeys = &lt;&lt;EOF ${var.ssh_key} EOF # Terraform has provisioners that allow the execution of commands / scripts on a local or remote machine. # Here I execute a command to update the VM. provisioner \"remote-exec\" { inline = [\"sudo apt update\", \"sudo apt upgrade -y\", \"echo Done!\"] connection { host = \"172.16.20.43\" type = \"ssh\" user = \"ubuntu\" private_key = file(var.private_key_path) } }}resource \"null_resource\" \"bastion-ansible\" { provisioner \"local-exec\" { command = \"ANSIBLE_HOST_KEY_CHECKING=False ansible-playbook -u ${var.ansible_user} -l bastion -i ../home-deploy/inventory --private-key ${var.private_key_path} -e 'pub_key=${var.public_key_path}' --ssh-extra-args '-o UserKnownHostsFile=/dev/null' ../home-deploy/main.yml\" } depends_on = [ proxmox_vm_qemu.bastion ]} I now use a null_resource for executing ansible the reason for this is that it will run every time even if the resource has not changed.Through all of the files creates you will have noticed variables have been used against various configuration parameters, before they will work they need to be defined in a file, we will call this vars.tf# provider varsvariable \"proxmox_api_url\"{ type = string}variable \"proxmox_api_token_id\" { type = string}variable \"proxmox_api_token_secret\" { type = string}# resource varsvariable \"proxmox_host\" { type = string default = \"td-vh01\"}variable \"template_name\" { type = string default = \"jammy-template\"}variable \"ansible_user\"{ default = \"ubuntu\" type = string}variable \"private_key_path\"{ type = string}variable \"public_key_path\"{ type = string}variable \"ssh_key\" { type = string default = \"ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAINm8L9RZ4lDsRDm6Zx7jqOrQx9mO7FphqcrV5teyGVJN\"}Now that we have defined the variables create a credential file to store all our variable information:proxmox_api_url = \"https://127.0.0.1:8006/api2/json\"proxmox_api_token_id = \"\"proxmox_api_token_secret = \"\"private_key_path = \"~/.ssh/id_ed25519\"public_key_path = \"~/.ssh/id_ed25519.pub\"ansible_user = \"\" Don’t commit this file to Git as it contains sensitive informationAny variables in vars.tf that have a default value don’t need to be defined in the credential file if the default value is sufficient.The Cloud-Init templateThe configuration that is used utilises a cloud-init template, check out my previous post (Proxmox template with cloud image and cloud init) where I cover how to set this up for use in Proxmox with Terraform.UsageNow all of the files we require are created, lets get it running: Install Terraform and Ansible apt install -y terraform ansible Enter the directory where your Terraform files reside Run terraform init, this will initialize your Terraform configuration and pull all the required providers. Ensure that you have the credential.auto.tfvars file created and with your variables populated Run terraform plan -out plan and if everything seems good terraform apply. Use terraform apply --auto-approve to automatically apply without a prompt To destroy the infrastructure, run terraform destroyFinal ThoughtsThere is so much more potential using Terraform and Ansible. I have just scratched the surface, but you could automate everything up to firewall configuration as well, this is something I still need to look into, but it would be great to deploy and configure the firewall based on each individual device.If you have any cool ideas for using Terraform and Ansible please let me know in the comments below!Until next time…" }, { "title": "Use Python pandas NOW for your big datasets", "url": "/posts/use-python-pandas-now/", "categories": "Python", "tags": "pandas, bigdata, dataframes, performance", "date": "2023-03-29 17:00:00 +0100", "snippet": "Over the past few years I have been working on processing large analytical data sets requiring various manipulations to produce statistics for analysis and business improvement.I quickly found that...", "content": "Over the past few years I have been working on processing large analytical data sets requiring various manipulations to produce statistics for analysis and business improvement.I quickly found that processing data of this size was slow, some taking over 11 hours to process which would only get worse as the data grew.Most of the processing required multiple nested for loops and addition of columns to json formatted data, this had some large processing requirements and multi threaded processing wouldn’t help in these scenarios.I knew there had to be a better way to process this data faster, and so I looked into using pandas.What is pandas?pandas is a software library written for the Python programming language for data manipulation and analysis. In particular, it offers data structures and operations for manipulating numerical tables and time series.Test resultsI ran some testing on 100 rows of data, one using for loops and one using pandas. With for loops the test took 19.09s to complete, with pandas an impressive 1.21s an improvement of 17.88s. When I run this on the full dataset which currently sits at around 16,500 rows it takes 33.15s seconds, an impressive improvement from a full run with for loops (which I had to cancel after 3 hours, it took too long for my requirements).Pandas first stepsInstall and importPandas is an easy package to install. Open up your terminal program (for Mac users) or command line (for PC users) and install it using either of the following commands:poetry install pandasORpip install pandasAlternatively, if you’re currently viewing this article in a Jupyter notebook you can run this cell:!pip install pandasThe ! at the beginning runs cells as if they were in a terminal.To import pandas we usually import it with a shorter name since it’s used so much:import pandas as pdSeries and DataFrameThe two main components of pandas are Series and DataFrame.A Series is a column and a DataFrame is a table made up of a collection of Series. DataFrames and Series are quite similar in that most operations that you can do with one you can do with the other.Creating a DataFrameDataFrames can be created multiple ways, I use PostgreSQL and BigQuery, using the pandas read_sql and read_gbq the data is automatically placed into a DataFrame.Also we can manually create a DataFrame, using Python dict and list which is great for testing.Taking our example in the diagram above, we have how many cars have sold each day. To organize this as a dictionary for pandas we could do something like:sales_data = { 'Audi': [3,5,0,1], 'BMW': [0,2,4,3]}sales = pd.DataFrame(sales_data)We would then see the data in the DataFrame format below: As you can see, each key from the dictionary is converted to a column, and the values are all put into a new row on the corresponding columnThe Index of this DataFrame is given on creation, as numbers 0-3, but we could also create our own when we initialize the DataFrame.sales = pd.DataFrame(data, index=['Jan', 'Feb', 'Mar', 'Apr'])Iterating dataNormally with Python you would create iterate through data using a for loop to check every column, to do this with a DataFrame it would looks like this:for sale in sales.itterrows(): if sale['Audi'] &gt; 5: print(f\"Audi Targets hit with {sale['Audi']} sales.\") if sale['BMW'] &gt; 3: print(f\"BMW Targets hit with {sale['BMW']} sales.\")This way of iterating through data is slow, I found this when I needed to do multiple nested for loops over multiple DataFrames.However, there is a much faster and more efficient way to do this using the pandas loc function. So now we could locate a specific month’s order by using the month name:sales.loc['Feb']Audi 3BMW 0Name: Jan, dtype: int64This example is quite simple, but we can get more complex with different data, for example if you had a list of servers with their operating systems you could find all server OS’s with “Windows” in the name and create a new column “OS Type” with “Windows” as the value to quickly filter by all Windows devices.servers.loc[~servers['os'].isna() &amp; servers['os'].contains(\"Windows\").lower(), \"OS Type\"] = \"Windows\" servers - is our DataFrame .loc[] - Allows access to a group of rows and columns by labels or boolean array ~servers['os'].isna() - checks that the os column has a value servers['os'].contains(\"Windows\").lower() - Checks that the os column has windows in the name and converts it to lower case to ensure all matches are not case sensitive. OS Type is the column to update if the statements match = \"Windows\" is the value to update the column withBelow would be the expected output from this Dataframe manipulation. Merging DadaFramesMuch like doing a join with a SQL table, you can merge DataFrames based on specific columns:pd.merge( devices, os_data[ [ \"device_name\", \"operating_system\" ] ], on=[\"device_name\"], how=\"left\", )Final ThoughtsThere is much more that can be done with pandas and DataFrames, this just scratches the surface and gives a very basic overview. The main reason for writing this article is to show what a difference in performance is made from using pandas, if you aren’t using this for your data yet, I recommend that you do!" }, { "title": "How I host this site", "url": "/posts/how-i-host-this-site/", "categories": "how-to", "tags": "jekyll, github, pages, actions", "date": "2023-02-25 19:22:00 +0000", "snippet": "My site isn’t anything special, but I thought I’d like to share how I create and host things for others who may be interested in sharing their own words with a very simple and easy to maintain stru...", "content": "My site isn’t anything special, but I thought I’d like to share how I create and host things for others who may be interested in sharing their own words with a very simple and easy to maintain structure.Motivation for hosting this siteI have hosted a blog site in some form for the past 10+ years. The idea being to share my experience with others and hopefully help others with some of the issues I have come across through my career working for one of the largest MSPs in the world.Sharing on social media has come more recently, but this site still serves as the main location for all of my content. Even more so in the past few months, social media platforms have shown that they are not a certain thing. Accounts get suspended, ownership changes kill services, rules change, etc. hosting my own site I have total control over the content with no risk of losing anything, which for me is well worthwhile.Ultimately, I wanted this site to be one place where you can always find my projects, regardless of what other platforms may do. I don’t make any money off my content so keeping it low cost is important. Seeing others lose their work due to account issues or frustration with a platform served as motivation to own my content.If you are a content creator, I encourage you to keep platform agnostic, allowing you to easily recover if for some reason an account is suspended.The siteLets get to the bones of it, this site is built using Jekyll, a Ruby based tool that is able to convert markdown into a static website, for my use case it was the perfect fit.Here are some of the things that I like about it: Small footprint - I used Wordpress for my last site, but found it was massively bloated for my needs. Along with update issues and other administrative overheads. Jekyll being static removes a lot of this complexity. Security - Wordpress and its plugins, due to its popularity sees a lot of vulnerabilities exploited. Again another benefit of using a static site generated by Jekyll this risk is significantly reduced. CDN Friendly - Having static content means that the site is able to be cached, handling incredible loads at low cost across the globe. Simple Format - Using markdown for all of the posts means that the content is pretty easy to move around. They can be used with other frameworks or easily converted to different formats if needed. Git Friendly - I hold my entire site in Git, so backups are easy along with the history of any changes.Jekyll also supports additional features through plugins, like RSS, Sitemaps, metadata, pagination and much more. If there isn’t a plugin to meet your needs its simple to create something.It’s also incredibly fast at building a site and generates predicable, easy to host results. If you haven’t looked at Jekyll, you might give it a whirl!Jekyll though needs two things to make it really work: A way to build the site A place to host the siteBuilding the siteBuilding a Jekyll site is easy, you just run jekyll build, but to make things even easier I utilize GitHub actions to automate the builds and deploy whenever changes happen.Using actions is pretty simple, the documentation is also great so easy to learn, here is my workflow for this site:name: Build and deploy Jekyll site to GitHub Pageson: push: branches: - master workflow_dispatch:jobs: jekyll: runs-on: ubuntu-latest steps: - name: 📂 setup uses: actions/checkout@v2 with: fetch-depth: \"0\" - name: 💎 setup ruby id: ruby uses: ruby/setup-ruby@v1 with: ruby-version: 3.1.2 - name: 🔨 install dependencies &amp; build site if: steps.ruby.outcome == 'success' id: deps uses: limjh16/jekyll-action-ts@v2 env: JEKYLL_GITHUB_TOKEN: $ with: enable_cache: true - name: 🚀 deploy if: steps.deps.outcome == 'success' uses: peaceiris/actions-gh-pages@v3 with: github_token: $ publish_dir: ./_siteThere are four steps: Checkout the repository Setup Ruby Install Dependencies &amp; Build Site - Installs all of the site dependencies / plugins etc. and then builds the site static content Deploy - Deploys the generated site to GitHub PagesAs you can see this runs whenever changes are pushed to the master branch, or I can manually run the workflow with workflow_dispatchHostingThe last thing we need is somewhere to host the site. The beauty about this is that Jekyll is just creating static HTML content, making loads of options available. In my case, to keep costs down I use Github Pages, its totally free, comes with SSL Certificates and seems to perform well enough for most small static sites.If you wanted something more performant, you could use Amazon S3, Digital Ocean Spaces Object Storage, or some other cloud-based solution.Final ThoughtsI understand that this site is basic, but keeping it this way helps me focus on other things, I have no need to worry about keeping patching and the onslaught of spam. It just works! Since its hosted on GitHub Pages I don’t need to worry about the hosting, but should the site be suspended for some reason, I can easily take my content and move it elsewhere with little hassle.Hopefully, if you’re looking to create new content and save yourself some hassle you would look at doing this option. The point (for me at least) is just to share what I think is cool and what I work on." }, { "title": "Home Assistant medication notification using Node-RED", "url": "/posts/home-assistant-medication-notification-node-red/", "categories": "Home Automation, Node-RED", "tags": "nodered, ha, home-assistant, automation, notify, mqtt, stateless", "date": "2023-01-06 19:22:00 +0000", "snippet": "For around 4 years I have had to take medication for Rheumatoid Arthritis once every two weeks, I always forget when I last took the medication and end up skipping which causes me pain.Due to this ...", "content": "For around 4 years I have had to take medication for Rheumatoid Arthritis once every two weeks, I always forget when I last took the medication and end up skipping which causes me pain.Due to this I decided I needed a way to log when I take my medication and then a notification on my phone when im due to take it again.I ended up creating a workflow in Node-RED that will do the following after I scan an NFC tag located on my fridge where I keep the medication: Update a input_datetime in Home Assistant with the current date and time Check every 60 minutes if the medication date is over 13 days ago On Monday, check if its been 10 days since last medication, then send a notification reminding me to take my medication that week After 14 days, if the input_datetime hasn’t been updated, send a notification to my mobile and TV every hour until it is reset. Lets look at how I made this.Home Assistant ConfigurationSome changes need to be made within home assistant to make this workInput DatetimeAdding the input_datetime entity requires editing the configuration.yaml file directly.Add the following to your configuration:input_datetime: name: \"Medication Taken Date\" icon: \"mdi:needle\" has_time: true has_date: true You may change the name and icon to something different hereGo to Developer Tools -&gt; YAML -&gt; Check ConfigurationIf that worked hit restart for the new configuration to take.NFC TagRegister an NFC tag (you could also use a button or some other mechanism for this) I recommend having the Home Assistant app installed to write the NFC tag In home assistant: Settings -&gt; Tags -&gt; Add Tag Type a name for the NFC tag (e.g. Medication) click create Once created, in the app click the button that has an arrow into a box Hold the phone next to the NFC tag to register the dataYour NFC tag is now created.Node-RED WorkflowsNFC Tag updates Add a HA tag node with the tag created earlier, ensure the msg.payload is set to expression $now(): Link this to a Date/Time Formatter node, with the Output Format set to YYYY-MM-DD HH:mm:ssLink this to a HA call service node: Domain: input_datetime Service: set_datetime Entity: Select the entity you created earlier Data: Expression {\"datetime\": payload} Now click deploy, Scan the NFC tag and it should update the input_datetime entity that was created earlier.Notifications I have two notifications setup, first a notification at the beginning of the week and then a notification every hour on the 14th day until the tag is scanned.Notify at beginning of the week after 10 daysAdd an inject node, under “Repeat” select at a specific time set a time, and the day you would like it to run.Link this to a current state node, to get the current date if the input_datetime field: Link this to a function node, with the following On Message code:var now = Date.now();var last = new Date(msg.payload)msg.payload = now - last.valueOf();return msg;This will calculate the amount of time since the input_datetime was set.Link this to a switch add a switch &gt;= with string value 864000000 (10 days) change the value to the number of millisecond’s when you want the notificationLink a call service node with the following: Domain: notify Service: The device you want the notification to go to Data: The message to send see example below:{\"message\":\"You need to take your medication this week.\",\"title\":\"This Week: Take Medication\",\"data\":{\"color\":\"#2DF56D\"}}Notify every 60 minutes after 14 daysThis workflow is essentially the same as the 10 day notification with a few tweaks so you can copy the previous workflow and make these changes:In the inject node select interval or interval between times then every X minutes and select all the days you want it to run.In the function node change the value to 1209600000 for 14 days, or as required for your notification.I also amended the message, and added an additional notify to my TV, this way it will popup on my TV every 60 minutes to annoy me into getting my medication.{\"message\":\"You need to take your medication.\",\"title\":\"Take Medication\",\"data\":{\"color\":\"#2DF56D\"}}That is everything done, you can now deploy and test.Final thoughtsI have now been using this workflow for around 2 months and it has been working great.The notifications to the TV even annoy my wife which really does make me get my medication out quicker!If you have any ideas on how I could improve this workflow further, please leave a comment." }, { "title": "Creating a standalone zigbee2mqtt hub with alpine linux", "url": "/posts/creating-standalone-zigbee2mqtt-hub-with-alpine-linux/", "categories": "Home Automation", "tags": "zigbee2mqtt, zigbee, alpine, raspberry, pi, mqtt, stateless", "date": "2022-12-15 14:13:00 +0000", "snippet": "I have began sorting out my smart home again, I let it run to ruin a year or so ago and now I’m getting solar installed I wanted to increase my automation to make life easier and utilise my solar m...", "content": "I have began sorting out my smart home again, I let it run to ruin a year or so ago and now I’m getting solar installed I wanted to increase my automation to make life easier and utilise my solar more efficiently once it’s installed.As part of my automation I used to run deconz with some zigbee IKEA Tradfri lights around the house, I found deconz limiting at the time and it doesn’t seem to have progressed much, whereas zigbee2mqtt seems to have moved a long way and has a lot of support.I also had the issue that Home Assistant now runs on a virtual machine in my loft, where the conbee II signal didn’t reach my devices, so to combat this I wanted to utilise an old raspberry pi and create a zigbee hub that is easy to maintain in a set and forget fashion, if it stops working, reboot and it works again.This is when I came up with ZigQt, an Alpine overlay that will fully configure a Zigbee2mqtt controller on a Raspberry Pi in a stateless manner. Through this article I will show you how to setup this great little ZigQt hubHardwareFor this I have used the following hardware: Raspberry Pi 3b plus POE+ Hat (Optional) Micro SD Card Conbee II (can use other zigbee dongles)OS InstallationFor the OS I have used Alpine Linux, by default Alpine is a diskless OS, meaning it loads the whole OS into memory and this makes it lightning fast.Create a bootable MicroSD card with two partitionsThe goal is to have a MicroSD card containing two partitions: The system partition: A fat32 partition, with boot and lba flags, on a small part of the MicroSD card, enough to store the system and the applications (suggested 512MB to 2GB). The storage partition: A ext4 partition occupying the rest of the MicroSD card capacity, to use as persistent storage for any configuration data that may be needed.Creating the partitions (assuming you re using Linux)Mount the SD card (this should be automated, if not, you probably know how to do that and you probably don’t need that tutorial)List your disks:sudo fdisk -lDisk /dev/sda: 7624 MB, 7994343424 bytes, 15613952 sectorsThe disk you just inserted should be available in the list. It most likely should be called /dev/sda.Create 2 partitions: 512MiB partition for Alpine itself (have to be of type 0x0c - W95 FAT32 (LBA)) All other available space on a partition for your var foldersudo fdisk /dev/sdan - p - 1 - +512M - t - 1 - c - a - wn - p - 3 - - - wFormat you first partition to fat and the second to ext4sudo mkfs.vfat -F 32 /dev/sda1sudo mkfs.ext4 /dev/sda2sudo mkfs.ext4 /dev/sda3Mount the system partitionType the command fdisk -l to list the disks that are accessible to the computer. Make note of the device name for the SD card.Type the command mkdir /mnt/SD to create a mount point for the SD card. You can replace /mnt/SD in any directory that you prefer.Type the command mount -t vfat /dev/sdc1 /mnt/SD to mount the SD card. -t vfat tells the operating system that it is a Windows file system. /dev/sdc1 Replace with the device name from the fdisk output. /mnt/SD Replace with the name of the directory you created for where the disk will be accessed.Type the command cd /mnt/SD to access the files on the SD card.Copy Alpine Linux onto the MicroSD CardDownload the Alpine Linux Raspberry Pi version from Alpine LinuxExtract the tarball into to system (bootable fat32 2GB) partition.tar -zxvf ~/Downloads/alpine-rpi-3.14.0-aarch64.tar.gzSince it will be a headless install (without an external monitor plugged in) you can setup minimum memory usage for the GPU, maximizing available memory, via a user custom configuration file:(Make sure you’re still on the same working directory as the previous command. Check with pwd.)echo \"gpu_mem=32\" &gt; usercfg.txtAdd ZigQt OverlayI have created an overlay that adds everything needed to get this zigbee hub up and running. Doing this manually takes some time and would be another blog post to explain it all.Download the latest zigqt.apkovl.tar.gz from ZigQt Releases.Now you can copy this to the root of the SD Card where you placed the Alpine Linux files.At this point you could boot up the device and everything should work, however there are some additional steps you can undertake to add further customisation to the ZigQt hub.Additional ConfigurationAdd-on files can be copied to the root of the OS partition and will be copied to the correct location on ZigQt, these allow further customisations.InterfacesIf you would like to set a static IP Address for the hub, create an interfaces file in the root of the OS partition with the IP details:auto loiface lo inet loopbackauto eth0iface eth0 inet static address 10.42.0.10 netmask 255.255.255.0 gateway 10.42.0.1auto loiface lo inet loopbackauto eth0iface eth0 inet dhcpauto wlan0iface wlan0 inet static address 10.42.0.10 netmask 255.255.255.0 gateway 10.42.0.1 The lo interface is recommended, but you only need add the specific interface you plan on using after this e.g. eth0 or wlan0 or usb0interfaces sampleWireless Network ConfigurationIf using wireless, you will need to create a wpa_supplicant.conf file.Zigbee2MQTT ConfigurationA default zigbee2mqtt configuration is created during install, however this may not suite your needs in this case you can create a custom configuration.yaml file. Further configuration options can be found hereFurther customisationThis repository may be forked/cloned/downloaded. The main script file is headless.sh. Execute ./make.sh to rebuild zigqt.apkovl.tar.gz with any of the changes made.On your PiInitial BootEach time the hub reboots, the initial boot sequence will be run, this ensures that the OS is the same on every boot greatly reducing the risk of changes to the OS causing issues with the hub.The following directories are mapped to persistent storage: /var /etc/zigbee2mqttThis ensures certain configuration is not lost on reboot.User/Password managementThe root user will have no password by default. It isn’t currently possible to update the password without breaking the way the overlay works, however in theory you could launch a copy of Alpine Linux without the zigqt overlay, setup a password and alternative used, run lbu commit to save the changes and then merge the required files with those in zigqt.apkovl.tar.gzIf I manage to figure out an easier way to do this I will be sure to update this article.Zigbee2mqttIf everything has worked, zigbee2mqtt should be accessible at the following address: http://zigqt.local:8080.Any configuration changes made in the web interface will be saved to the persistent storage, so will still be in effect after a reboot.UpdatesTo update to newer versions, simply reboot, the latest available zigbee2mqtt will be installed.Final thoughtsAt the moment this is the best solution I could think of to provide a fully functioning and maintenance free version of Zigbee2mqtt on a standalone Raspberry Pi.I hope to have a solution for the user and password management someday, but if you know a way to get around this please do let me know." }, { "title": "Configuring Homer Dashboard", "url": "/posts/configuring-homer-dashboard/", "categories": "Containers", "tags": "homer, docker, container, dashboard", "date": "2022-10-16 10:15:00 +0100", "snippet": "In my last article I talked about how to setup Homer dashboard with Docker, now I will walk through some of the features and how to use them.Main FeaturesSome of Homers main features are: Yaml fil...", "content": "In my last article I talked about how to setup Homer dashboard with Docker, now I will walk through some of the features and how to use them.Main FeaturesSome of Homers main features are: Yaml file configuration Search Grouping Theme customisation Service Health Checks Keyboard shortcutsConfigurationTo begin configuration navigate to the homer data folder that we created in the previous article dockerfiles\\homer\\data, you will store all the files you require here, but first open config.yml.The initial configuration gives you an idea of how to layout your dashboard, each section has a great explanation on how to use it.One thing that isn’t covered is the service checks, we will look at that later.To setup a basic section and URL you would need something like this:services: - name: \"//Media\" icon: \"fas fa-clapperboard\" items: - name: \"Plex\" logo: \"assets/tools/plex.png\" subtitle: \"Watch Movies &amp; TV\" tag: \"media\" url: \"https://192.168.1.100:32400\" target: \"_blank\"To add more items, just copy the first item and change its details for the second service that you wish to link out to.For custom icons, you need to add the files to the tools folder and then update the logo line in the configuration.I recommend checking out dashboard-icons which contains a huge list of icons that work great with Homer.Service ChecksAdditional checks can be added to an item, these are called Custom Services, some applications have direct integration, others can only use ping. A full list of the supported services and how to configure them is listed hereCustom ThemesYou can add custom CSS to homer in order to have a personal look similar to the one I have used from Walkxcode called homer-themeEasier UpdatesSometimes updating via terminal using nano/vim can be a pain, I personally use VS Code for the majority of my editing, so I setup Remote SSH which allows me to connect to my docker server file system from VS Code and edit the configuration files directly in VS Code.Hopefully this information was useful for you, If you have any questions about this article, share your thoughts and comment in the discussion below or head over to my Discord." }, { "title": "Homer dashboard with Docker", "url": "/posts/homer-dashboard-with-docker/", "categories": "Containers", "tags": "homer, docker, container, dashboard", "date": "2022-10-15 22:57:00 +0100", "snippet": "Recently I have decided to get my home network in order, One of the things I realised was that I spend a lot of time trying to remember the IP addresses or URLs for services within my home, especia...", "content": "Recently I have decided to get my home network in order, One of the things I realised was that I spend a lot of time trying to remember the IP addresses or URLs for services within my home, especially ones that I access infrequently.At one point I did have a dashboard that was HTML but I never updated it and I decided to remove it a year or so ago.After sitting on YouTube for a few hours watching rubbish I came across Homer, A simple to use Docker container that hosts am easily configurable dashboard with customisable designs.Homer is configured using YAML making it very familiar to myself having used Docker for a number of years now.Directory setupIn order to use Homer with Docker first I created a directory to store the configuration file and any other assets such as images. Mine are on an NFS share but this would also be the same for local files. My file structure is as follows:📦dockerfiles ┣ 📂homer ┃ ┗ 📂data ┗ 📂portainer ┗ 📂dataAs you can see, I create a directory for each container, then within that a subdirectory for each volume mapped to a container folder, usually this is just data, but some containers require more.DockerThe container can then be launched one of two ways, via command, or via docker-compose.I use portainer, linked to a GitHub repository that will re-deploy any time that I update the docker-compose.yml file on GitHub (more on that in another article!)CMDdocker run -d \\ -p 8080:8080 \\ -v /dockerfiles/homer/data:/www/assets \\ --restart=always \\ b4bz/homer:latestHere we are creating a container without loading it to shell with -d, then we link the docker host port with the docker container port -p 8080:8080, next, map the data folder on our docker host to the assets on the container -v /dovkerfiles/homer/data:/www/assets the assets folder contains the config file as well as images. We always want the container to restart if the host reboots --restart=always and lastly we specify the image and what version we would like to use b4bz/homer:latestComposeHere we essentially have the same configuration as the docker command, the one small change I have made though is to use a Docker Volume to map the volumes from OS to Container, this is the preferred method.version: \"3.6\"services: homer: image: b4bz/homer container_name: homer hostname: homer volumes: - homer_data:/www/assets ports: - 8080:8080volumes: homer_data: driver_opts: type: none device: /mnt/nfs/dockerfiles/homer/data o: bindOnce you have the docker-compose.yml file you can run docker-compose up -d which will create the volume, download the image and start the container.Accessing the Homer dashboardNow that the container is up and running you can access it via:http://&lt;docker-host-ip-address&gt;:&lt;port&gt;So in my case this would be:http://172.16.20.4:8080If everything has worked as expected you should see the following demo dashboard: For more information on how to configure this dashboard check out this article where I cover the configuration of the dashboards in more detail.Hopefully this information was useful for you, If you have any questions about this article, share your thoughts and comment in the discussion below or head over to my Discord." }, { "title": "Proxmox Template with Cloud Image and Cloud Init", "url": "/posts/proxmox-template-with-cloud-image-and-cloud-init/", "categories": "Automation", "tags": "proxmox, ubuntu, cloud-init, cloud-image, linux, clone, template", "date": "2022-10-04 19:54:00 +0100", "snippet": " Updated to latest Ubuntu image &amp; Added enable for qemu serviceUsing Cloud images and Cloud init with Proxmox is the quickest, most efficient way to deploy servers at this time. Cloud images a...", "content": " Updated to latest Ubuntu image &amp; Added enable for qemu serviceUsing Cloud images and Cloud init with Proxmox is the quickest, most efficient way to deploy servers at this time. Cloud images are small cloud certified that have Cloud init pre-installed and ready to accept configuration.Cloud images and Cloud init also work with Proxmox and if you combine this with Terraform you have a fully automated deployment model. See Automating deployments using Terraform with Proxmox and ansible for instructions on how to do this.intGuideDownload imageFirst you will need to choose an Ubuntu Cloud ImageRather than downloading this, copy the URL.Then SSH into your Proxmox server and run wget with the URL you just copied, similar to below:wget https://cloud-images.ubuntu.com/jammy/current/jammy-server-cloudimg-amd64.imgThis will download the image onto your proxmox server ready for use.Install packagesThe qemu-guest-agent is not installed on the cloud-images, so we need a way to inject that into out image file. This can be done with a great tool called virt-customize this is installed with the package libguestfs-tools. libguestfs is a set of tools for accessing and modifying virtual machine (VM) disk images.Install:sudo apt update -y &amp;&amp; sudo apt install libguestfs-tools -yInstall qemu-guest-agent into the downloaded image:sudo virt-customize -a jammy-server-cloudimg-amd64.img --install qemu-guest-agent --run-command 'systemctl enable qemu-guest-agent.service'You can also install other packages at this point.Adding users to the image (Optional)It is possible to also add a user and SSH keys with the virt-customize. This is useful for automation such as terraform.sudo virt-customize -a jammy-server-cloudimg-amd64.img --run-command 'useradd simone'sudo virt-customize -a jammy-server-cloudimg-amd64.img --run-command 'mkdir -p /home/simone/.ssh'sudo virt-customize -a jammy-server-cloudimg-amd64.img --ssh-inject simone:file:id_ed25519.pubsudo virt-customize -a jammy-server-cloudimg-amd64.img --run-command 'chown -R simone:simone /home/simone' Adds the user to the image Makes the SSH Key directory Injects the SSH Key. simone is the user the key will apply to, file:id_ed25519.pub is the file on the local host where the SSH Key is located Makes sure the user simone owns the home folderCreate a virtual machine from the imageNow we need to create a new virtual machine:qm create 9000 --memory 2048 --core 2 --name jammy-template --net0 virtio,bridge=vmbr0Import the downloaded Ubuntu disk to the correct storage:qm importdisk 9000 jammy-server-cloudimg-amd64.img local-lvmAttach the new disk as a SCSI drive on the SCSI Controller:qm set 9000 --scsihw virtio-scsi-pci --scsi0 local-lvm:vm-9000-disk-0Add cloud init drive:qm set 9000 --ide2 local-lvm:cloudinitMake the could init drive bootable and restrict BIOS to boot from disk only:qm set 9000 --boot c --bootdisk scsi0Add serial console:qm set 9000 --serial0 socket --vga serial0Turn on guest agent:qm set 9000 --agent enabled=1 DO NOT POWER ON THE VMConvert the VM to a TemplateNow, Create a template from the image you just created:qm template 9000Clone the template to a VMNow you have a fully functioning template, which can be cloned as much as you want. But it makes sense to set some of the settings.First, clone the VM (here we are cloning the template with ID 9000 to a new VM with ID 999):sudo qm clone 9000 999 --name test-cloud-initNext, set the SSH keys (if you didn’t add yours earlier) and IP address:sudo qm set 999 --sshkey ~/.ssh/id_rsa.pubsudo qm set 999 --ipconfig0 ip=10.10.1.20/24,gw=10.10.1.1It’s now ready to start up!sudo qm start 999You should be able to log in without any problems (after trusting the SSH fingerprint). Note that the username is ubuntu, for the key set here. If you added your own user earlier, you can use that insteadssh ubuntu@10.10.1.1Once happy with the template, you can stop the VM and clean up the resources:sudo qm stop 999 &amp;&amp; sudo qm destroy 999rm jammy-server-cloudimg-amd64.imgReferenceshttps://registry.terraform.io/modules/sdhibit/cloud-init-vm/proxmox/latest/examples/ubuntu_single_vmClosingHopefully this information was useful for you, If you have any questions about this article and share your thoughts head over to my Discord." }, { "title": "Type hinting and checking in Python", "url": "/posts/type-hinting-and-checking-in-python/", "categories": "Python, Code Quality", "tags": "type hints, typing, python, best practices, code quality, standards", "date": "2022-07-05 14:50:00 +0100", "snippet": "Type hinting is a formal solution that statically indicates the type of a value within your Python code. Specified byPEP 484 and then introduced to Python 3.5.Type hints help to structure your proj...", "content": "Type hinting is a formal solution that statically indicates the type of a value within your Python code. Specified byPEP 484 and then introduced to Python 3.5.Type hints help to structure your projects better, however they are just hints, they don’t impact the runtime.As your code base gets larger or you utilise unfamiliar libraries type hints can help with debugging and stopping mistakes from being made when writing new code. When utilising an IDE such as VSCode (with extensions) and PyCharm you will be presented with warning messages each time an incorrect type is used.Pros and ConsAdding Type hints comes with some great pros: Great to assist in the documentation of your code Enable IDEs to provide better autocomplete functionality Help discover errors during development Force you to think about what type should be used and returned, enabling better design decisions.However, there are also some downsides to type hinting: Adds development time Only works with Python 3.5+. (although this shouldn’t be an issue now) Can cause a minor start-up delay in code that uses it especially when using the typing module Code can be harder to write, especially for complex typesWhen should type hinting be added: Large projects with multiple developers Design and development of libraries, type hints will help developers that are not familiar with the library If you plan on writing tests it is recommended to use type hintingFunction TypingType hints can be added to a function as follows: After each parameter, add a colon and a data type After the function add an arrow function -&gt; and data typeA function with type hints should look similar to the one below:def add_numbers(num1: int, num2: int) -&gt; int: return num1 + num2Here you can see that the data types are all int so if a float ia supplied you would be presented with a warning in your IDE or if you use mypy an error would be displayed.Return types can get more complex when expecting multiple different types, for this we usually would need to add assert to make sure mypy knows which type to expect and when.Variable type hintingVariables can also have type hints, in the same way that we add them to functions we would issue a colon, the type then our variable data, as the below example:my_int: int = 1my_string: str = \"string\"my_dict: dict = {\"item1\": \"value1\"}my_list: list = [a,b,c]Optional and Union typesSome objects may contain a couple of different types of objects. Union allows us to indicate that several different types are accepted. Optional indicates that an object may be given or None. If we take the example from earlier, we are able to make it accept both a float and int.def add_numbers(num1: Union[int, float], num2: Union[int, float]) -&gt; Union[int, float]: return num1 + num2With this updated example if we used add_numbers(1.1, 1.2) the output would work without error and type hints would not display a warning.Static Type Checking - MypyMypy will run against your code and print out any type errors that are found. Mypy doesn’t need to execute the code, it will simply run through it much the same as a linter tool would do.If no type hinting is present in the code, no errors will be produced by Mypy.Mypy can be run against a single file or an entire folder. I also utilise pre-commits which wont allow code to be committed if there are any errors present. I also introduced these checks with Github Actions to ensure any contributions to my projects follow these requirements.Final ThoughtsType hints are a great way to ensure your code is used in the correct manner and to reduce the risk of errors being introduced during development. Although they are not required by Python, I feel that type hints should be added to all projects as it assists with clean code and reduces errors.The following resources are great for additional help with type hinting: Mypy type hinting cheatsheet Python’s typing module documentation" }, { "title": "Creating the perfect Python project", "url": "/posts/creating-the-perfect-python-project/", "categories": "Python, Code Quality", "tags": "python, project, best practices, code quality, team, standards", "date": "2022-03-22 23:00:00 +0000", "snippet": "Working on a new project its always exciting to jump straight in and get codingwithout any setup time. However spending a small amount of time to setup the projectwith the best tools and practices ...", "content": "Working on a new project its always exciting to jump straight in and get codingwithout any setup time. However spending a small amount of time to setup the projectwith the best tools and practices will lead to a standardised and aligned codingexperience for developers.In this article I will go through what I consider to be the best python project setup.Please follow along, or if you prefer to jump straight in, you can usecookiecutterto generate a new project following these standards, install poetry then create a newproject.Poetry: Dependency ManagementPoetry is a Python dependency management and packaging systemthat makes package management easy!Poetry comes with all the features you would require to manage a project’s packages,it removes the need to freeze and potentially include packages that are not requiredfor the specific project. Poetry only adds the libraries that you require for thatspecific project.No more need for the unmanageable requirements.txt file.Poetry will also add a venv to ensure only the required packages are loaded.with one simple command poetry shell you enter the venv with all therequired packages.Lets get setup with poetrypip install poetrypoetry initpoetry add &lt;package&gt;poetry shellpoetry run python your_script.pySo, that’s a few commands! but what do they all do? pip install poetry - Installs the poetry package to your machine poetry init - Adds poetry to an existing project (for a new project use poetry new &lt;projectName&gt;) poetry add &lt;package&gt; - Adds a single or multiple python packages poetry shell - Activates the poetry venv poetry run python your_script.py - Runs the script your_script.py within the poetry venvBlack: Code Formattingblack is an uncompromising code formatterin Python. If your code violates pep8 then Black will notify or resolve the issuesLets get that installed as a development dependency:poetry add black --devWe also need to add some additional configuration for Black to the end ofpyproject.toml[tool.black]line-length = 88target_version = ['py38']include = '\\.pyi?$'exclude = '''( /( \\.eggs # exclude a few common directories in the | \\.git # root of the project | \\.hg | \\.mypy_cache | \\.tox | \\.venv | _build | buck-out | build | dist )/)'''These settings can be changed to your preferences, for example I like the line lengthto be 88, but you may prefer this shorter / longer.To use black you can run the following command:black . --checkThis will check the formatting of the files in the current directory and its subfolders,if you remove the --check option, it will automatically reformat your python code. isort: Import Sortingisort is a Python library that automatically sortsimported libraries alphabetically and separates them into sections and types.Lets get that installed as a development dependency:poetry add isort --devisort and black don’t get along, their configurations conflict with each other, so to getaround this issue we need to add some configuration to the end of pyproject.toml:[tool.isort]profile = \"black\"force_sort_within_sections = trueknown_first_party = [ \"tests\",]forced_separate = [ \"tests\",]Adding the profile = \"black\" option ensures that iSort respects changes made by Black,It is also advisable to add the folders for known_first_party files, this enables iSortto group those imports together in order.To use isort run the following command:isort . --diffThis will check the order of the imports and let you know if it is correct, if youwould like isort to automatically fix the ordering, remove the --diff option.flake8: Style Enforcementflake8 is a python tool that checks the styleand quality of your Python code. It checks for various issues not covered by black.Lets get this added to our project:poetry add flake8 --devflake8 also has some configuration that is recommended with black, create a new filecalled .flake8 and place the below configuration:[flake8]max-line-length = 88max-complexity = 15exclude = build/*extend-ignore = # See https://github.com/PyCQA/pycodestyle/issues/373 E203,ignore = E203, E266, E501, W503, W605select = B,C,E,F,W,T4This configuration will ensure that type errors that conflict with black will beignored.To use flake8 you can run the below command:flake8 . --fixThis will run flake8 and fix any issues on all files in the current directory andsubdirectories, if you just want to see the issues remove the --fix option. MyPy: Static Types CheckerMypy is an optional static type checkerfor Python that aims to combine the benefits of dynamic (or “duck”) typing andstatic typing.MyPy does require that the static types are installed for each library, if a libraryhas no static types that will cause mypy to error.poetry add mypy --devAdditional configuration can be added to the pyproject.toml file if required similarto below:[tool.mypy]python_version = \"3.8\"warn_return_any = truewarn_unused_configs = true[[tool.mypy.overrides]]disallow_untyped_defs = trueTo use mypy simply enter the following command:mypy .Interrogate: DocString standardisationinterrogatechecks your codebase for missing docstrings.Docstrings provide the ability to automatically document and also assist developersallowing then to quickly and easily see what a specific class or function is used for.To install interrogate, type:poetry add interrogate --devAdditional configuration for the pyproject.toml file:[tool.interrogate]ignore-init-method = trueignore-init-module = falseignore-magic = falseignore-semiprivate = falseignore-private = falseignore-property-decorators = falseignore-module = trueignore-nested-functions = falseignore-nested-classes = trueignore-setters = falsefail-under = 95exclude = [\"setup.py\", \"docs\", \"build\"]ignore-regex = [\"^get$\", \"^mock_.*\", \".*BaseClass.*\"]verbose = 0quiet = falsewhitelist-regex = []color = trueTo run interrogate use the following command:interrogate -vvRemove the -vv to just see a success or fail message without a list of files Pre-Commit hooksNow we have all of these tests, but we don’t want to run them manually every time wemake changes to code. This is where pre-commit hooks come into play.Pre-commit hooks allow you to run multiple checks against code before git commitwill be applied, if any of the tests fail, the commit will not apply until theissues raised are resolved.This feature is great for a few reasons: You don’t need to remember to run all of the above manually each time you wish to check code Github Actions based on code quality should continue to succeedSetup pre-commit hooksFirst we create the configuration file in root .pre-commit-config.yaml:repos: - repo: https://github.com/pycqa/isort rev: 5.8.0 hooks: - id: isort name: isort (python) - repo: local hooks: - id: black name: black stages: [commit] language: system entry: black types: [python] - repo: https://gitlab.com/pycqa/flake8 rev: 4.0.1 hooks: - id: flake8 additional_dependencies: [flake8-bugbear] - repo: https://github.com/compilerla/conventional-pre-commit rev: v1.2.0 hooks: - id: conventional-pre-commit stages: [commit-msg] args: [] # optional: list of Conventional Commits types to allow - repo: https://github.com/econchick/interrogate rev: 1.5.0 hooks: - id: interrogateInstall pre-commit &amp; tell pre-commit to register our config:poetry install pre-commit --devpre-commit install -t pre-commit -t commit-msgNow when you run a commit you will see each hook running, this will then show any errorsprior to committing, you can then fix the issues and try the commit again.You can also see I have conventional-pre-commit applied with the -t commit-msg tagthis enforces the use of conventional commit messages for all commits, ensuring thatour commit messages all follow the same standard. Final ThoughtsThis method of utilising cookie cutter, and pre-commit hooks has saved me a lot of time,I think there is more to be explored with pre-commit hooks such as adding tests for mycode etc. that will come with time on my development journey.With these methods I know my commit messages are tidy, and my code is cleaner than beforeits a great start with more to come.I also execute these as github actions on my projects, that way anyone else who contributesbut doesn’t install the pre-commit hooks will be held accountable to resolve any issues priorto merging their pull requests.Hopefully some of this information was useful for you, If you have any questions about this article and share your thoughts head over to my Discord." }, { "title": "Cookiecutter: Automate project creation!", "url": "/posts/cookiecutter-automate-project-creation/", "categories": "Automation", "tags": "templating, best practices, code quality", "date": "2021-11-09 23:00:00 +0000", "snippet": "As I move closer to the world of development within my career I have beenlooking for more efficient ways to spend my time, along with assisting my colleaguesand myself follow the programming, docum...", "content": "As I move closer to the world of development within my career I have beenlooking for more efficient ways to spend my time, along with assisting my colleaguesand myself follow the programming, documenting and best practices we have set.When we create a new project there are many repetitive tasks that take place,such as creating pyproject.toml, directory structures, documentation foldersand many other tasks, these tasks are time consuming, repetitive and prone touser error.Some contextStarting a new repository for a new project is always a chore, specially whenworking with large teams where others are collaborating with you. You have tofollow the same standards and coding practices to ensure all developers knowwhat is happening.Working in large teams means that with many different projects and repositoriesit is very likely that none of them will follow the same base structure that isexpected. To help alleviate this problem and fulfil these expectations I createdproject templates that anyone can follow to ensure all base projects are the same.What is CookiecutterCookiecutter is a CLI toolbuilt in Python that creates a project from boilerplate templates(mainly available on Github). It uses the templating systemJinja2 to replace and customize folders and/or files names, as well as their content.Although built with Python, you are not limited to templating Python projects,it can easily be implemented with other programming languages. However, to dothis you will need to know or learn some Jinja and if you want to implementhooks this will need to be done in Python.Why use cookiecutterWell simply put, to save time building new project repositories, to avoidmissing files or commit checks and probably one important step, to makelife easier for new team members who will be expected to create projects.We also use it as a way to enforce standards, providing the developer withthe necessary structure to ensure the rules are followed: write documentationperform tests, follow specific syntax standards by giving them the base structurein a boilerplate code, it makes it easier for developers to follow standards.In certain projects you may have a lot of repetitive code, such as creating Flaskwebsites, with a cookiecutter template, you would be able to duplicate thatcode with ease and little time spent.How to use CookiecutterCookiecutter is super simple to use, you can either use one of the manytemplatesthat already exist online, or you can create one that suits your own needs.You can access templates from various locations: Git repository Local folder Zip file If working with Git repositories, you can even start a template from any branch!To try out cookie cutter, first it needs to be installed:pip install -U cookiecutterOnce installed run the following command:cookiecutter gh:totaldebug/python-package-templateThis repository follows the standards that I use for my Python Package repositories.When you execute it, you will be prompted for values, for example:full_name [marksie1988]: Steve Marksemail [totaldebug@example.com]: youremail@gmail.comgithub_username [totaldebug]:version [0.1.0]:use_pytest [n]: y...For a default value, you just press return, or to amend the value you simply typeit and hit return. This prompt is created based on the values defined incookiecutter.json, all cookiecutter templates have this file in the root.Once you have answered all of the prompts, the template will be converted intoa new location using the data provided during the prompts.How Cookiecutter templates workA basic Cookiecutter template looks like this:📦template ┣ 📂hooks ┃ ┣ 📜pre_get_project.py ┃ ┗ 📜post_get_project.py ┗ 📂 ┃ ┣ 📜your-project-files-here ┃ ┣ 📂 ┃ ┃ ┗ 📂3d-printer-axes-calibration ┗ 📜cookiecutter.jsonWhat happens here:template: this is the root of the repository or folderhooks: Python scripts that execute before and after the generation of the repository. Pre-hooks are generally used to validate inputs from the promptsand the post-hooks to remove files that are not require for this specific project.{{ cookiecutter.project_slug }}: is the directory for your project to be stored.Anything stored in this directory will be copied to the new project.For a python package you would have another subdirectory with the package namethis would usually be the {{ cookiecutter.project_slug }}/{{ cookiecutter.project_slug }} directory.This is the minimum required file structure, you can then add as required foryour projects, or copy an existing template and amend the areas that you require.cookiecutter.jsonTo allow flexibility with a template you add variables to cookiecutter.jsonthis will create a prompt when executing the template for a value which will changethe output to the template.For each variable within here a default text value, boolean or list of optionsare required. Example:{ \"full_name\": \"marksie1988\", \"email\": \"totaldebug@example.com\", \"github_username\": \"totaldebug\", \"project_name\": \"Python Boilerplate\", \"project_slug\": \"{{ cookiecutter.project_name.lower().replace(' ', '_').replace('-', '_') }}\", \"minimal_python_version\": [3.7, 3.8, 3.9], \"use_black\": \"y\"}Input validity can be checked with pre_gen_project hooks, the below example validatesthe data supplied in the project_slug value:MODULE_REGEX = r'^[_a-zA-Z][_a-zA-Z0-9]+$'module_name = ''if not re.match(MODULE_REGEX, module_name): print('ERROR: The project slug (%s) is not a valid Python module name. Please do not use a - and use _ instead' % module_name)As you can see from the examples, you can either create a very simple templateor add Jinja / Python for more complex and error validation.Final ThoughtsCookiecutter has saved me a lot of time in the creation or projects, also a lotof the boring template work is taken out of starting a new project which isalways a bonus.Now all of my projects start in a good standard and should be easier to keep that way.If you would like to check out cookiecutter you could start by checking mypython-package-templateI have added things like Github actions and pre-commits to check work alongwith other python best practices that I hope to cover in my next article.Hopefully some of this information was useful for you, If you have any questionsabout this article and share your thoughts head over to my Discord." }, { "title": "Sqitch, Sensible database change management", "url": "/posts/sqitch-sensible-database-change-management/", "categories": "Database, Automation", "tags": "sqitch, db, change, management, database, automation", "date": "2021-07-07 00:00:00 +0100", "snippet": "OverviewRecently I have been working on a few projects that utilise PostgreSQL databases,as the projects have grown our team has found it increasingly more difficult to manageall of the database ch...", "content": "OverviewRecently I have been working on a few projects that utilise PostgreSQL databases,as the projects have grown our team has found it increasingly more difficult to manageall of the database changes between dev / staging / prod without missing parts of functionsor missing table columns, especially over long development periods.Due to this I spent the past month looking into many different ways to manage this, we endedup landing on sqitch, it wasn’t the first product tested and below I will run throughsome of the others that I found and the issues we saw with them.ExpectationsSo what did our team expect would be delivered by the database change management tool?Well here is the list: Native SQL support No limitations on SQL functionality Open Source, or have a feature rich community edition that is well supported Easily managed version control, ideally without need for new SQL files for each change Ability to rollback changes to specific versions Unix command line utility for easy automationThe testing phaseOver about a month I tested the following products:FlywayFlyway was very close to being the chosen product, it had most of our requirements with afew limitations, but it was the best I had found.Pros: Uses native SQL Easy file namingCons: A new file is required for every change, this would lead to hundreds of version files Inability to rollback to a specific version in time Heavily limited functionality on the community edition More complex implementationLiqibaseLiqibase was looking great, until I discovered that the main language used is XML, SQL issupported, however most documentation is XML based and I didn’t have the time to spendlearning the XML format to eventually find out that some specific feature we use isn’tsupported by this format.All in I found that it was more complex to get started than Flyway and the documentationwasn’t the best.Pros: More features in the free version than Flyway Diff feature to compare two databases Rollback is free Utilises one file for migrationsCons: XML is the primary language used Targeted rollback is an addonSQL AlchemyAs this is an ORM it was removed from the running fairly quickly, there is no native SQLsupport, which means a high chance of missing SQL functionality, one such feature wasthe ability to create and update Postgres functionsPros: Uses Python so can be baked into projects Development Teams don’t need to know/learn SQLCons: Functionality limited to what the developers implement Risk of compatibility issues in the future No support for native SQL filesSqitchSqitch was the last option on the table, I found this tool when searching YouTube when avery early version was being presented.The idea of Sqitch is to use Version control to track the changes in files, forour requirements this was perfect. It meant I could update existing SQL files andSqitch would know a change was made and could then be deployed.One downside to this plan is that not all these features are implemented yet. Althoughthe developers working on the project are making massive strides and I feel it wontbe long until they have achieved the original goal they set out for.Pros: Uses native SQL Utilises a git like version control system You always edit the original file Open source allowing you to customise as needed Very responsive community Ability to support almost any databaseCons: Some expected features are not implemented yet No commercial support, only community basedImplementationNow that we have tested and decided that Sqitch is the product for us, its time toimplement the solution.Installation is super simple, its written in Perl so can be installed on almost anysystem, or you can use it within a Docker container.I won’t cover the installation as its easy enough and documented well on thesqitch website.One thing that I would recommend is to change the default location of the files, bydefault Sqitch will add deploy, revert and verify to the root directory. YourSQL goes inside these directories. I prefer to have these in a separate directory tokeep the root directory tidy, to do this you would run a command similar to below wheninitialising your repository:sqitch init myApp --top-dir sql --uri https://github.com/totaldebug/sqitch_demo --engine pgThis command will tell Sqitch that you want to init a sqitch project within the directorysql for the GitHub repository sqitch_demo and with the engine pg (PostgreSQL) thereare other options and databases supported all listed hereOnce you have initialised the project you are ready to add a change. The basic pattern is: Create a branch Add SQL changes Modify the code as needed Commit Merge to masterSo when first starting out you would want to create the schema to do this you would: Create a branch in your Git repo Run sqitch add appschema Edit sql/deploy/appschema.sql, sql/revert/appschema.sql and sql/verify/appschema.sql Run sqitch deploy db:pg://user@127.0.0.1:5432/sqitch_demo to deploy the changes Edit any code as normal Run any tests Commit your changes Merge the changes back to the main branchIn order to ensure that your revert SQL is working as expected, it is a good idea torevert and redeploy your changes:sqitch rebase --onto @HEAD^ -yThis command will revert the last change, and redeploy it to the database. This isessentially a shorter way of running:sqitch revert --to @HEAD^ -y &amp;&amp; sqitch deploy db:pg://user@127.0.0.1:5432/sqitch_demoWhen the deploy command is issued, sqitch will run down the plan file and execute eachchange that is required.If this is the first time deploying Sqitch to a database, it will automatically createall the required tables to track future deployments and changes.ConclusionI’ve barely scratched the surface of Sqitch’s capabilities. To say how long Git and changemanagement has been around, its amazing that its taken this long for someone to get it right.If you are having issues with managing database change, I highly suggest that you try Sqitch." }, { "title": "Using CloneZilla to migrate multiple disk server", "url": "/posts/using-clonezilla-to-migrate-multi-disk-server/", "categories": "Virtualisation, Proxmox, Migration", "tags": "clonezilla, proxmox, vmware, migration", "date": "2021-05-15 00:00:00 +0100", "snippet": "OverviewI recently decided to migrate all of my home servers to Proxmox from VMware ESXi, many factors at play but the main being that new versions of ESXi don’t support my hardware.For a normal mi...", "content": "OverviewI recently decided to migrate all of my home servers to Proxmox from VMware ESXi, many factors at play but the main being that new versions of ESXi don’t support my hardware.For a normal migration I would just use CloneZilla’s remote-source to remote-dest feature, however I could only get this to work for a single source disk, which is fine for the majority of my servers, however I do have some with multiple disks which became an issue.What was the problem?At its core CloneZilla is designed to clone a single disk to multiple other disks, you can do this many different ways, however if you have a machine with multiple disks then it is not possible to do this in the traditional way that most tutorials online show you.I really struggled to find any information on this subject, and most of my research turned up how to clone a single disk to multiple disks rather than how to clone multiple disks to multiple disks!Its easy to see how this could be difficult for CloneZilla, I mean how would it know which two disks to clone the source data to on the destination? without some form of GUI where you need to pair up all the disks it would be difficult.The solutionIn order to overcome this issue, I created a CloneZilla image this was cloned onto an NFS share. Once complete, I was able to load the image on the destination machine, as there were only two disks in the destination server the image was applied without any issue, on boot I could see that both disks had been cloned over from the image.The only thing I didn’t like about this is that I had to first create the image, then deploy that image, when I only have one server to clone and not need for an image it would be nice for CloneZilla to implement something in the remote-source / remote-dest that allows this functionality.Final ThoughtsCloneZilla is an excellent tool for performing these migrations, It’s very easy to use and clones the images quite quickly. In my opinion it is much easier than other solutions provided on the Proxmox website, in fact other methods using the OVF Tool never worked for me (there are also lots of reports of other users having the same issues) which is why I ended up going with CloneZilla.If you have had any experience with Proxmox migrations using CloneZilla or have a trick that makes the OVF Tool migrations work please let me know over on my Discord." }, { "title": "Use Git like a pro!", "url": "/posts/use-git-like-a-pro/", "categories": "Git", "tags": "best practices, code quality, versioning, git", "date": "2021-01-03 23:00:00 +0000", "snippet": "Over the past few months I have been using Git &amp; GitHub more frequently, both in my professional and personal work,with this came many questions about what the “correct” way is to use Git.There...", "content": "Over the past few months I have been using Git &amp; GitHub more frequently, both in my professional and personal work,with this came many questions about what the “correct” way is to use Git.There are obviously many ways to create workflows using Git, however below is the way that I have started to manage my workflow,this is likely to change over time as it is only my first workflow but this is a start!What to solve?There are many things that I didn’t like about the way I used Git in the past and so these are some of the issues I am aiming to solve: Versioning Standardised git commit messages How best to utilise Branches When should Pull Requests be used How can the workflow be AutomatedWhy solve them?Well this is quite straight forward, to improve the readability of my Git Repos especially in open source projects,but also to keep my mind clear and organised.How were these issues solved?Below I have split each area to solve out, this explains how I solved the issues I was experiencing.VersioningVersioning was something that I never thought about, I increased when I wanted to based on what I thought was right.Then I started doing code professionally and was introduced to the Semantic Versioning specification.This made much more sense by adding a relationship between each different increment.A version number would be MAJOR.MINOR.PATCH, Increments as below: MAJOR version when changes are mede that would break previous functionality. MINOR version when functionality is added in a backwards compatible manner. PATCH version where you make backwards compatible bug fixes.by using this method people are now able to easily identify what type of change has been implemented and if it is likely to break their current project.Conventional CommitsMy commit records were… well… a total mess, Looking at other repos this is quite common and not many projects follow a standard.I was looking for a better way to provide commit messages that just make sense and are easy to read, in my research I found a standard calledConventional Commits.Conventional Commits is a specification for adding human and machine readable meanings to commit messages, this then allows the creation ofChangeLogs through Automation and makes life easier for a human to tell what has changed!The specification is real simple so doesn’t take much to get your head around:&lt;type&gt;[optional scope]: &lt;description&gt;[optional body][optional footer]The commit contains the following structural elements: fix: a commit of the type fix resolves a bug in the codebase (PATCH in SemVer) feat: a commit of the type feat adds a new feature to the codebase (MINOR on SemVer) Breaking Changes: a commit that appends ! after the type/scope, where a breaking change is introduced (MAJOR in SemVer). A breaking change can be part of commits of any type. Other types are allowed for example: build:, ci:, docs:, style:, test: and others. Body &amp; Footers may be provided to include Breaking Change as well as other information.Some examples:fix(core): error handling of CLI CommandThis example shows a fix for the core scope and the fix was for error handling of CLI Commandfeat(core)!: Updated API to add more functionality to xxxThis example adds a feat to the core scope, its a breaking change and shows that more functionality was added to the API.As you can see this is such a simple and easy to understand convention which is very easy to use in automation if needed.BranchesWhen I first started using Git Branches were something of a mystery to me, I didn’t understand why you would want to commit to a branch and then have to push to master.After using Git for a few years I have realised their huge benefits and how they assist me with automation of tasks.For every issue that is reported I will create a new branch, the branch will be named as follows:&lt;issue number&gt;-&lt;short_description&gt;Example:311-softLimitBy doing this I am able to quickly link a branch to a specific issue in the project. Branches also enable me to make multiple commits at smaller increments, which I then use Pull Requests to merge with MasterPull RequestsI now utilise Pull Requests to move my branch into the master, the pull request has various checks using GitHub Actions depending on the project typeThis would be things like: Version check: confirm that the version in the project files has been incremented since the last release Tests: Check that the code functions as expected Linting: Check that the code still adheres to the relevant standardsWith all of my Repos I will only enable Allow squash merging this allows me to create one good commit message that covers the issues fixed for the specific branch we are merging, rather than all the commits from the development lifecycle (keeping my master commits clean)Version TagsOnce I have completed all of the pull requests for a specific release I will then add a version tag to the master.This version tag creates a point in time reference along with triggering my release automation once it is pushed.Automated WorkflowIn order to streamline my delivery to release I have started to utilise GitHub Actions, This allows me to have endless automation capabilities.Currently I utilise Actions for the following: Linting Tests Version Checks ChangeLog Generation Release creation Push to external artifactories (e.g. Docker Hub, Ansible Galaxy etc.)The changelog and release process is something that I have just started doing, I was manually writing out my changelog forany new releases which was time consuming and required a lot of manual back and forth to confirm what was changed,not an issue whilst a project is small, but if it grows that would quickly become out of control.Final ThoughtsI believe that at this time for the work I am doing this is the best workflow for myself, If you have any thoughts on waysthis could be further improved, please let me know over on my Discord" }, { "title": "Use GitHub pages with unsupported plugins", "url": "/posts/use-github-pages-with-unsupported-plugins/", "categories": "Website", "tags": "github, pages, jekyll, plugins, actions", "date": "2020-12-08 23:00:00 +0000", "snippet": "I have recently migrated my website over to Github Pages, however in doing so I have found that there are some limitations, the main one being that not all Jekyll plugins are supported.Due to this ...", "content": "I have recently migrated my website over to Github Pages, however in doing so I have found that there are some limitations, the main one being that not all Jekyll plugins are supported.Due to this I needed to find a workaround, which I wanted to share with you allAdvantages of this methodControl over gemset Jekyll Version - Instead of using the version forced upon you by GitHub, you can use any version you want Plugins - You can use any Jekyll plugins irrespective of them being supported by GitHubWorkflow Management Customization - By using GitHub Actions, you are able to customize the build steps however you need them Logging - The build log is visible and can be adjusted, so it is much easier to debug errorsSetting up the GitHub ActionGitHub actions are created by adding a YAML file in the directory .github/workflows. Here we will create our action using the Jekyll Action from the Marketplace.Create a workflow file github-pages.yml, then add the below information:name: Build and deploy Jekyll site to GitHub Pageson: push: branches: - master workflow_dispatch:jobs: github-pages: runs-on: ubuntu-16.04 steps: - uses: actions/checkout@v2 - uses: helaili/jekyll-action@2.0.1 env: JEKYLL_PAT: $This workflow is doing the following: We trigger on.push to master, or by a manual dispatch workflow_dispatch The checkout action clones your repository. Our action is specified along with the required version helaili/jekyll-action@2.0.1 We set an environment variable for the action to use JEKYLL_PAT a Personal Access TokenProviding permissionsThe action needs permissions to push the Jekyll data to your gh-pages branch (this will be created if it doesn’t exist)In order to do this, you must create a GitHub Personal Access Token on your GitHub profile, then set this as an environment variable using Secrets. On your GitHub profile, under Developer Settings, go to the Personal Access Tokens section. Create a token. Give it a name like “GitHub Actions” and ensure it has permissions to public_repos (or the entire repo scope for private repository) — necessary for the action to commit to the gh-pages branch. Copy the token value. Go to your repository’s Settings and then the Secrets tab. Create a token named JEKYLL_PAT (important) and paste your token into the valueDeploymentOn pushing changes onto master the action will be triggered and the build will start.You can watch the progress by looking at the actions that are currently running via your repositoryIf all goes well you should see a green build status on the gh-pages branch.If this is a new repository you will also need to setup the pages to use the new gh-pages branch instead of master. this can be found in the repository settings." }, { "title": "Docker Overlay2 with CentOS for production", "url": "/posts/docker-overlay2-with-centos-for-production/", "categories": "Docker, Overlay2", "tags": "docker, overlay2, centos, production", "date": "2020-05-05 00:00:00 +0100", "snippet": "The following short article runs through how to setup docker to use overlay2 with Centos for use in productionPre-Requisites Add an extra drive to CentOS (this could also be freespace on the exist...", "content": "The following short article runs through how to setup docker to use overlay2 with Centos for use in productionPre-Requisites Add an extra drive to CentOS (this could also be freespace on the existing disk) Have docker installed (services stopped)SetupFirst we need to find our new disk:fdisk -lOnce we have our new disk, we can start to create a our logical volume:pvcreate /dev/sdb -fvgcreate docker_vg /dev/sdblvcreate -n docker_xfs -l 100%FREE docker_vgNow that we havve our logical volume, check that it doesnt have xfs on it already:xfs_info /dev/docker_vg/docker_xfsNow we can create our XFS and mount the new volume:mkfs.xfs /dev/docker_vg/docker_xfs -f -n ftype=1mkdir /var/lib/dockermount /dev/docker_vg/docker_xfs /var/lib/dockerAdd this to fstab in order to ensure it mounts on reboot vi /etc/fstab/dev/docker_vg/docker_xfs/ /var/lib/docker xfs rw,relatime,seclabel,attr2,inode64,noquota 0 0Now we can start our docker servicessystemctl start dockerTo test that this has worked, run the following, you should see that now you are using Overlay2 as the storage driver:docker info" }, { "title": "3d Printer Axes Calibration", "url": "/posts/3d-printer-axes-calibration/", "categories": "3D Printer, Axes Calibration", "tags": "3d, calibration, ender3, octoprint, pronterface", "date": "2020-01-27 23:00:00 +0000", "snippet": "One of the most difficult things I found out about 3d printing was that you must calibrate it! This isn’t something that I was aware of, I assumed once everything was tightened that it would just w...", "content": "One of the most difficult things I found out about 3d printing was that you must calibrate it! This isn’t something that I was aware of, I assumed once everything was tightened that it would just work, I was so wrong!The good news is, its quite a simple process once you know how and in this article im going to share with you, how I calibrate my printer and get perfect prints almost every time.I use an Ender 3 with a lot of upgrades, but the process is the same for almost all 3d printers , so you should be able to follow this article without issue.What you will need: 3d Printer Correctly tensioned belts (they should make a nice twang sound) Pronterface or Octoprint Digital Calipers Ruler (calipers sometimes get in the way but you may be ok) Tape or marker Filament Something to take notes onAxes Diagram: Setup Software:First we need to gather all the current settings, to do this you must first send a command to the printer, this can be done with either:PronterfaceYou must plug the USB into the printer and a computer, then launch pronterface, it should auto detect the printer, then click ConnectYou can now enter commands in the right window next to the Send buttonOctoprintOnce Octoprint is setup go to the terminal tab and you can enter commands hereGather Initial Info:Issue the command: M92 then press enter or hit send. you should see something like this:echo: M92 X80.00 Y80.00 Z400.00 E93.00Make a note of this information somewhere as we will be referring back to these values quite often.Now we can begin to calibrate each of our motors.X&amp;Z-Axis CalibrationFirst start by homing your X axis and the Z axis. I will use the stop switch as the measuring point as this doesn’t move, however you can use any fixed point from the relevant axis.First measure the distance from the stop switch to the edge of the moving part (X = Printhead, Z = Gantry) , if yours is touching the stop switch then the distance is 0mm.Now tell your printer to move the Axis 100mm (you can set this to smaller or larger number as the calculation will still work) The further you move the axis the more accurate your calibration should be. Now with your calipers measure from the stop switch to the same point on the printhead, write down the measurement as “ActualDistance” you will need to do this for both the X &amp; Z AxisIf you measured 100mm then you don’t need to do anything else, your axis is calibrated. However, you likely wont get exactly 100mm so we will need to adjust for this.E Axis CalibrationThere are two ways that you can calibrate the E Axis. With the HotEnd attached or without. Personally I prefer to remove the bowden tube from the extruder and measure this way, I find its much more accurate. Some people prefer to heat the HotEnd and let the filament flow through it.First, remove your filament and disconnect the bowden tube, then we will need to push the filament through the extruder until you just see the end of it flat with the edge where the bowden tube attaches.Now send 100mm to the E Axis to extrude (you will need to heat the HotEnd or it wont work)Once this finishes, measure with your calipers the distance from the end of the filament to the extruder this should be 100mm, if not make a note of the measurement (ActualDistance)CalculationsIn order to calculate the Axis we need the following calculation, the calculation is the same no matter which Axis you are working on:NewValue = 100mm / ActualDistance * CurrentValueSo if you have the below for your X Axis: CurrentValue = 80.00 (M92 for the Axis)ActualDistance = 93 (how far the Axis actually moved)DesiredDistance = 100 (the amount we told it to move)86.02 = 100 / 93 * 80Based on the above, the new value for X would be 86.02Applying New Values for all AxisNow we need to apply all these values by running the M92 command again with your new values as below:M92 X86.02 Y81.20 Z400.00 E149.00M500Also we add an M500 which will save the configuration, if you want to make sure the values have saved, restart your printer and issue M92 again you should see the new values." }, { "title": "I won a Ender 3 3D Printer and i'm addicted", "url": "/posts/i-won-a-ender-3-3d-printer-and-im-addicted/", "categories": "3D Printer, Ender3", "tags": "3d, ender3, competition", "date": "2020-01-12 23:00:00 +0000", "snippet": "About 6 months ago I entered a competition with DrZzs (highly recommend his channel for home automation) and BangGood to win a Creality Ender 3 3D Printer.To my surprise a few weeks later I receive...", "content": "About 6 months ago I entered a competition with DrZzs (highly recommend his channel for home automation) and BangGood to win a Creality Ender 3 3D Printer.To my surprise a few weeks later I received an email from Banggood stating that I had won and to email over my address, at first i thought that it was just a spam email.After a few weeks of waiting the printer arrived, I couldn’t believe it, I just got a £300 printer for FREE!On with the build!I then unboxed and went through building the printer. I followed the instructions which were very comprehensive (other than a few confusing sentences).It took me roughly 2 hours to build the printer.I ran through a test print, printing out a benchy to make sure that everything was working as expected. It looked perfect so ordered some more filament for future projects.The addiction begins…So now it starts, I spend the rest of my time glued to thingiverse deciding what to print! Although my son made this easier by constantly bugging me to print him a mini combine harvester. This was a difficult print, the thing prints as one, but the combine kept on fusing to the harvester so it wouldn’t spin. After a lot of calibration I finally made it work and he was delighted to get a new toy for free!Now I have the printer running, and have printed multiple useful prints, Laptop wall mount, Google Home wall mount, Microphone stand for my large desk etc. (and more toys)Time for the upgrades!The Ender3 is a brilliant little printer for the price, however it does have some issues that can be easily resolved with a few upgrades.Printed UpgradesUpper Filament Guide –Unfortunately not a great upgrade, and one that I scrapped. I have put this here to recommend avoiding this. (I have better options below)It is supposed to keep the filament further away from the printer and stop it wearing the extruder arm. However I found that it made horrible squeaking noises so that was out.Lower Filament Guide –This brilliant little print stops the filament from rubbing against Z Screw and getting grease on it which ruins prints.I recommend the linked guide as it doesn’t curl over the filament, again I found any that curled over the top would rub and cause a horrible squeak.Fan Cover –I found that little bits of filament would drop from the hot end into the fan that was open on the ender 3 case, this covers that up and also puts the directions for bed up and down.Hero Me Gen3 remix –Parts don’t get cooled fast enough with the standard cooler, therefore I printed this one, it focuses the air perfectly under the nozzle for really good cooling.This version is for the new Ender3’s as they have smaller screws than the older models.Extruder Knob –This print was one I didn’t know I needed until I printed it, this allows for easier retraction and extrusion by twisting the knob it will move the gear to easily feed filament.If you plan to upgrade to the MK8 Dual Gear Extruder Arm this part wont fit due to larger gears.Side Spool Holder –I had issues with the filament dragging and getting stuck due to the sharp angle that it was pulling at, this causes unnecessary wear on the extruder arm and gears. I found this side spool holder which moves the spool to the side of the printer, next to the extruder arm causing much less force to be required when extruding and less rubbing on the extruder arm.Spool Holder –This spool holder uses bearings to allow the filament to roll around much easier, reducing the drag on the extruder and in turn reducing the wear on the stepper motor.Purchased UpgradesSKR 1.3 -A great upgrade to silence those stepper motors, not only that the SKR has a 32bit chip which means more space for new features and faster gcode processing. The TMC2209 Stepper Motor Drivers really do make a massive difference when it comes to the noise of the printer.Now the only annoying thing are the fans… still on my to do list.Capricorn Bowden Tube –This Bowden Tube appears to be much better than the one shipped with the Ender 3, it is much more slick to the touch and a little more sturdy, this means the filament passes through it with ease, also the tight diameter means the filament has little room to flex and cause retraction issues.I found that the shipped bowden tube had also melted at the end and had filament stuck to it which leads me to believe it wasn’t installed very well at the factory.MK8 Dual Gear Extruder Arm –My extruder arm broke within a few months of use, I didn’t notice until I was having bad under extrusion and also seeing slippage on the extruder gear. I took the arm apart to clean the gear and found a tiny crack near the screw for the idler wheel, this was enough to stop the arm working at all due to the flex it added. This can be combated by printing a new extruder arm but I decided to upgrade to a dual gear option which results in: Even less potential slippage Stronger spring for the extruder arm Metal body stops wear from filament rubPEI Magnetic Bed –This is an excellent upgrade from the stock bed, makes prints super smooth on the bed and sticks really well. I haven’t had a single failed print due to adhesion on this surface, also you can easily take stuff off once cooled without much effort or by flexing the magnetic plate for larger prints.ConclusionAll in I think I have spent no more than £100 on upgrades and have a brilliant printer, the prints that I get out now are near perfect bed adhesion is excellent with the stock bed, however I ripped mine by overheating a print during testing and am awaiting a new PEI Magnetic build plate which I think will be my last upgrade for a little while!It would also be great to hear about anyone else experience with this printer." }, { "title": "CentOS 8 Teaming with WiFi Hidden SSID using nmcli", "url": "/posts/centos-8-teaming-with-wifi-hidden-ssid-using-nmcli/", "categories": "Linux, CentOS, Networking, Wireless", "tags": "linux, centos, networking, wireless, nmcli", "date": "2019-11-01 23:00:00 +0000", "snippet": "I have had a lot of issues when setting up teaming with WiFi, mainly because of lack of documentation around this, im guessing that teaming ethernet and WiFi is not a common occurrence especially w...", "content": "I have had a lot of issues when setting up teaming with WiFi, mainly because of lack of documentation around this, im guessing that teaming ethernet and WiFi is not a common occurrence especially with a hidden SSID.As part of my home systems I am utilising an old laptop as my home assistant server, this allows for battery backup and network teaming, if my switch dies, my WiFi will still work etc.Lets get to the meat and potatoes!So the first thing that we need to do is check our devices are available:&gt; nmcli d statusDEVICE TYPE STATE CONNECTIONeno1 ethernet connected Ethernet connection 1wlp3s0 wifi connected Wi-Fi connection 1lo loopback unmanaged --Now that we see all our devices (they may not all be listed as connected yet) we can create our team:&gt; nmcli c a type team con-name team0Connection 'team-TeamA' (4387966d-715b-4636-b307-03d2b92476bf) successfully added.NetworkManager will set its internal parameter connection.autoconnect to yes and as no IP address was given ipv4.method will be set to auto. NetworkManager will also write a configuration file to /etc/sysconfig/network-scripts/ifcfg-team-TeamA where the corresponding ONBOOT will be set to yes and BOOTPROTO will be set to dhcp.Now if we show the connections we should see team0 listed:&gt; nmcli c showNAME UUID TYPE DEVICEEthernet connection 1 520e1441-f8d9-43c1-8c3d-a0d56227b6b9 ethernet eno1Wi-Fi connection 1 ddef5993-b469-4916-961d-3082b1f41ec1 wifi wlp3s0team0 c742d9e4-73ec-4506-82ff-b2a93727cc3a team nm-teamCurrently the team isn’t doing anything, so we need to add our ethernet interface:&gt; nmcli c a type team-slave ifname eno1 master team0Connection 'team-slave-eno1' (d2f0f253-0b42-4fdb-828b-c983b7ad59f4) successfully added.Once done if we show the connections again we will see the two team-slaves:NAME UUID TYPE DEVICEEthernet connection 1 520e1441-f8d9-43c1-8c3d-a0d56227b6b9 ethernet eno1Wi-Fi connection 1 ddef5993-b469-4916-961d-3082b1f41ec1 wifi wlp3s0team0 c742d9e4-73ec-4506-82ff-b2a93727cc3a team nm-team1team-slave-eno1 d2f0f253-0b42-4fdb-828b-c983b7ad59f4 ethernet --These commands automatically create corresponding files for the team under:/etc/sysconfig/network-scripts/ifcfg-team0/etc/sysconfig/network-scripts/ifcfg-team-slave-en01 If you edit these files manually, you will need to run nmcli con reload so that network manager reads the config changes.The team is now setup, however it is recommended to use a static IP Address, we can manually specify this by running the below commands:nmcli c m team0 ipv4.method manualnmcli c m team0 ipv4.addresses 10.10.10.20/24nmcli c m team0 ipv4.gateway 10.10.10.1nmcli c m team0 ipv4.dns 10.10.10.1, 1.1.1.1Enabling the teamTo enable the team we must bring up our Ethernet interface with this command:&gt; nmcli c up team-slave-eno1Connection successfully activated (D-Bus active path: /org/freedesktop/NetworkManager/ActiveConnection/5)What about the WiFi?So we have setup our connections and our team with an IP Address, however we havent specified any WiFi configurations at all, so lets do that;First lets make sure the wifi module is installed, by default this isn’t:dnf install NetworkManager-wifisystemctl restart NetworkManagerWe can discover any wifi in the area by running:&gt; nmcli d wifi listIN-USE SSID MODE CHAN RATE SIGNAL BARS SECURITY my_wifi_net Infra 1 195 Mbit/s 34 ▂▄__ WPA2to connect to one of these networks simply type:nmcli d wifi connect my_wifi_net password &amp;lt;wifi-passwordHidden WiFi NetworkA hidden network is a standard WiFi network that doesnt broadcast its SSID, this means that its name cannot be searched and must be known in order to configure it.To do this we must enter the following commands:nmcli c a type 802-11-wireless ifname wlp3s0 master team0 utoconnect yes ssid &amp;lt;your-Wifi-SSID&gt; 802-11-wireless-security.key-mgmt wpa-psk 802-11-wireless-security.psk &amp;lt;wifi-password&gt; 802-11-wireless.hidden yes wlp3s0 may be different in your command, it will depend what your wireless slave connection is calledNow that we have our wifi interface connected and configured we can bring it online with the below command&gt; nmcli c up team-slave-wlp3s0Connection successfully activated (D-Bus active path: /org/freedesktop/NetworkManager/ActiveConnection/6)After you have brought the connections up your team should automatically come online, to test you can run the following:This command will list the team and connections, all should be up and the team should show an IP Addressip aThis command will show all your connections, the ones for the team should all be greennmcli cI would also recommend doing some testing with the team to make sure it functions as expected, I normally run a continuous ping and then disconnect one of the interfaces, if all is well you should only see increased latency for one ping or a single dropped ping.Modifying the teamWhen we first setup a team it will default to roundrobin this can be seen by running the below command: (substitute nm-team with your team)&gt; teamdctl nm-team statesetup: runner: roundrobinports: eno1 link watches: link summary: up instance[link_watch_0]: name: ethtool link: up down count: 0 wlp3s0 link watches: link summary: up instance[link_watch_0]: name: ethtool link: up down count: 0This shows the runner which is how the traffic is managed across the interfaces, we can also see the interfaces that are part of the team, along with their states.The possible runners are: roundrobin: This is the default option, it will send packets to all interfaces one at a time per interface, in a round robin manner broadcast: All traffic will be sent over all ports activebackup: One interface will be used as the Active interface and the rest as a backup, the team will monitor for failures and automatically failover the link should one go offline loadbalance: Traffic will be balanced across all interfaces based on Tx traffic, the load should be equal across all interfacesYou can change the runner that you are using when you create the team:nmcli c a type team con-name team0 config '{\"runner\": {\"name\": \"broadcast\"}, \"link_watch\": {\"name\": \"ethtool\"}}'or you can modify the connection with this command:nmcli c m team0 config '{\"runner\": {\"name\": \"broadcast\"}, \"link_watch\": {\"name\": \"ethtool\"}}'once the above command is run a reboot will be required to apply the changes, now we can confirm the changes using:&gt; teamdctl nm-team statesetup: runner: broadcastports: eno1 link watches: link summary: up instance[link_watch_0]: name: ethtool link: up down count: 0runner: active port: eno1 More information on runners can be found here" }, { "title": "Continuous Integration and Deployment", "url": "/posts/continuous-integration-and-deployment/", "categories": "Development, Continuous Integration", "tags": "CI, CD, continuous, integration, deployment", "date": "2019-10-11 00:00:00 +0100", "snippet": "I have recently been looking into CI and CD, mainly for use at home with my various projects etc. but also to further my knowledge.Over the years I have built up quite an estate of servers that ove...", "content": "I have recently been looking into CI and CD, mainly for use at home with my various projects etc. but also to further my knowledge.Over the years I have built up quite an estate of servers that over time become more difficult to manage and maintain, mostly I will spend a long time researching and deploying a solution, but when it breaks weeks / months later i struggle to remember how it was all built.There must be a better way!So now im looking for the best way to deploy / re-deploy and test all of my servers and services with minimum effort and without breaking them if I do something wrong.I started by building out Ansible playbooks, one for each of my servers, this works great for deploying my servers with all the apps that I require, However this doesn’t help with things like home assistant configuration changes, if I change my config I have to do it via atom with a remote plugin that allows FTP on changes. This works… but if i make a mistake i take home assistant offline which doesn’t go down well with the family!After this I thought how can I update my configuration, keep it backed up, have the ability to roll it back and also test it before I put it on my server?So I have now started using GitHub to store my configuration, this gives me a backup in case my server dies and also helps the HA Community see examples of the configuration for their own deployments.I also want to check the new configuration when it gets committed to GIT but before I download it to home assistant, for this I use gitlab. Whenever gitlab detects a commit on the GIT repository it will begin a pipeline on gitlab that checks my latest configuration for various things: MarkdownLint - Checks any files with markdown in to make sure it is valid YAMLlint - Checks YAML files for formatting and validation JSONlint - Checks any JSON files for formatting and validation HA Stable / Dev / Beta - My Home Assistant configuration is then checked against the different buildsBy doing all of the above checks I will know that the code works as expected and I can also tell that it will work with all the current releases of HomeAssistant.Once the configuration has been checked the pipeline will trigger a webhook back to my Home Assistant server which then pulls the latest commit from GitHub and restarts HomeAssistant.Now I have gone from roughly 15 / 30 minutes for testing and troubleshooting, along with potential outages down to around 2 minutes and no long outage for my Home Assistant.ConclusionBy doing this I have saved myself 13 / 28 minutes per configuration change, when you add that up over weeks / months of changes I have very quickly saved a days worth of configuration change! If you then add the time saved by using Ansible, I can deploy a brand new Home Assistant server in around 10 minutes which is fully configured and functional." }, { "title": "UniFi L2TP: set a static IP for a specific user (built-in Radius Server)", "url": "/posts/unifi-l2tp-set-a-static-ip-for-a-specific-user-built-in-radius-server/", "categories": "Networking, Unifi", "tags": "unifi, l2tp, vpn, radius", "date": "2019-10-07 17:19:00 +0100", "snippet": "When using my L2TP VPN with the Unifi I realised that it was assigning a different IP Address to my client when it connected sometimes.This wouldn’t normally be a problem if the remote client was o...", "content": "When using my L2TP VPN with the Unifi I realised that it was assigning a different IP Address to my client when it connected sometimes.This wouldn’t normally be a problem if the remote client was only taking to my internal network, however I run a server that my internal network communicates out to via IP Address, so if this changes it all stops working.This article walks through how to setup a static IP Address for an L2TP Client.First we need to get a dump of our configuration from the USG, to do this we need to SSH to the USG and run a dump:mca-ctrl -t dump-cfgOnce we have this I recommend copying it into your favourite text editor. We want to delete everything except the following:{ \"service\": { \"radius-server\": { \"user\": { \"myl2tpuser\": { \"password\": \"password\", \"tunnel-param\": \"3 1\" } } } }}Now that we only have our user configuration we need to modify it to assign the IP Address:{ \"service\": { \"radius-server\": { \"user\": { \"myl2tpuser\": { \"ip-address\": \"192.168.10.10\" } } } }}Once we have this we are able to add this to a config file on our controller which, when the controller re-provisions the USG will apply. (you can also manually force a provision)The file needs to be saved to the site location, this will be something similar to:/opt/UniFi/data/sites/default/once in this directory create a new file called config.gateway.json and paste the above configuration into it.To test the new configuration file you can run this command:python -m json.tool config.gateway.jsonYou shouldn’t see any errors if this is correct.We now can re-provision the USG which will pickup the configuration from the Controller and update the VPN settings." }, { "title": "Ubiquiti UniFi USG Content Filter Configuration", "url": "/posts/ubiquiti-unifi-usg-content-filter-configuration/", "categories": "Networking, Unifi", "tags": "usg, filter, website, configuration", "date": "2019-09-17 15:00:00 +0100", "snippet": "Recently I had a requirement to setup a content filter on the USG for a client. I couldn’t find much information online so have decided to write this article to show others how to do thisFirst we n...", "content": "Recently I had a requirement to setup a content filter on the USG for a client. I couldn’t find much information online so have decided to write this article to show others how to do thisFirst we need to logon to the USG via SSH, On windows I recommend PuttyOnce we have logged in, run the below command:update webproxy blacklistsThis will download all of the content filter categories to the USG, this can take some time as there is approx. 100MB (70-80MB is “adult”)When this is completed run the following:configureset service webproxy url-filtering squidguard block-category &amp;lt;insert caregory&amp;gt;This will set the categories that you wish to block, repeating the command will add more categories. pressing ? will display a list of all available categoriesWe now need to set the web proxy listener address for the network we wish to filter:set service webproxy listen-address &amp;lt;-usg-lan-ip-&amp;gt;You are also able to set a redirect URL:set service webproxy url-filtering squidguard redirect-url &amp;lt;url&amp;gt;The redirect URL is google.com by default, however you could create a custom “Blocked Website” page to make users aware.Now we need to commit these changes to the USG:commitThe below example shows how we set this up on the network 10.10.10.1/24configureset service webproxy url-filtering squidguard block-category adultset service webproxy listen-address 10.10.10.1set service webproxy url-filtering squidguard redirect-url spottedhyena.co.ukcommitTo make this a permanent change you can create a configuration file on the controller, run the command:mca-ctrl -t dump-cfgFind the service section and delete all content other than the web proxy, it should looks similar to below:\"service\": { \"webproxy\": { \"cache-size\": \"0\", \"default-port\": \"3128\", \"listen-address\": { \"10.10.10.1\": \"''\" }, \"mem-cache-size\": \"5\", \"url-filtering\": { \"squidguard\": { \"block-category\": [ \"adult\" ], \"default-action\": \"allow\", \"redirect-url\": \"http://spottedhyena.co.uk\" } } }}Save this information into a file on your controller File Location: /opt/UniFi/data/sites/[site name/default]/ File Name: config.gateway.jsononce you have done this whenever you make any changes to your USG the Content Filtering will be re-applied.Hopefully this article has assisted you with your configuration. Any questions please let me know." }, { "title": "vCloud Director 8.10 – Renew SSL Certificates", "url": "/posts/vcloud-director-8-10-renew-ssl-certificates/", "categories": "Virtualisation, VMware", "tags": "vcloud, director, ssl", "date": "2018-09-20 00:00:00 +0100", "snippet": "Today I had to renew SSL certificates for a vCloud Director 8.10 cell which had expired.I could not find a working guide explaining the steps so this post covers everything required to replace expi...", "content": "Today I had to renew SSL certificates for a vCloud Director 8.10 cell which had expired.I could not find a working guide explaining the steps so this post covers everything required to replace expiring / expired certificates with new ones.First Cell StepsFirst we lets check that the Cell doesn’t have any running jobs:/opt/vmware/vcloud-director/bin/cell-management-tool -u &amp;lt;AdminUser&amp;gt; cell --statusYou will be prompted for your administrator account password.Once you have done this you should see the following output:Job count = 1Is Active = trueIn Maintenance Mode = falseWe must now stop the task scheduler to quiesce the cell by running the command:/opt/vmware/vcloud-director/bin/cell-management-tool -u &amp;lt;AdminUser&amp;gt; cell --quiesce true This command prevents new jobs from being started. Existing jobs continue to run until they complete or are cancelled.When the Job Count = 0 and Is Active = false, it is safe to shut down the cell by running the command:./cell-management-tool -u &amp;lt;AdminUser&amp;gt; cell --shutdownCopy our old certificate store:cp /usr/local/vmware/certificates.ks /usr/local/vmware/certificates-new.ksNow we need to list the certificates in our new keystore:/opt/vmware/vcloud-director/jre/bin/keytool -storetype JCEKS -keystore /usr/local/vmware/certificates-new.ks -list -storepass &lt;password&gt; The keystore location may be different on your serverWe now need to delete the expired http and consoleproxy certificates from the keystore. Note that the root and intermediate certificates may not have expired so you can leave these in place/opt/vmware/vcloud-director/jre/bin/keytool -delete -alias http -keystore /usr/local/vmware/certificates-new.ks -storetype JCEKS -storepass &lt;password&gt;/opt/vmware/vcloud-director/jre/bin/keytool -delete -alias consoleproxy -keystore /usr/local/vmware/certificates-new.ks -storetype JCEKS -storepass &lt;password&gt;Run the following to generate new certificates for HTTP and ConsoleProxy:/opt/vmware/vcloud-director/jre/bin/keytool -keystore /usr/local/vmware/certificates-new.ks -storetype JCEKS -storepass &lt;password&gt; -genkey -keysize 2048 -keyalg RSA -alias http/opt/vmware/vcloud-director/jre/bin/keytool -keystore /usr/local/vmware/certificates-new.ks -storetype JCEKS -storepass &lt;password&gt; -genkey -keysize 2048 -keyalg RSA -alias consoleproxyNow we must generate our CSR files:/opt/vmware/vcloud-director/jre/bin/keytool -keystore /usr/local/vmware/certificates-new.ks -storetype JCEKS -storepass &lt;password&gt; -certreq -alias http -file ~/http.csr/opt/vmware/vcloud-director/jre/bin/keytool -keystore /usr/local/vmware/certificates-new.ks -storetype JCEKS -storepass &lt;password&gt; -certreq -alias consoleproxy -file ~/consoleproxy.csrOnce the files are created you will need to copy the contents to your SSL Provider in order to get your certificate. When you have the .cer file from them you can continue through this article.Copy your new certificate cer files to your server, this can be done by copying the contents to a new file on the server or via a program like winSCPImport the Certificates into the keystore:/opt/vmware/vcloud-director/jre/bin/keytool -storetype JCEKS -storepass &lt;password&gt; -keystore /usr/local/vmware/certificates2018.ks -import -alias http -file http2018.cer/opt/vmware/vcloud-director/jre/bin/keytool -storetype JCEKS -storepass &lt;password&gt; -keystore /usr/local/vmware/certificates2018.ks -import -alias consoleproxy -file consoleproxy2018.cerNow we need to replace the existing certificates with the new certificates:./cell-management-tool certificates -j -k /usr/local/vmware/certificates-new.ks -w &lt;password&gt;./cell-management-tool certificates -p -k /usr/local/vmware/certificates-new.ks -w &lt;password&gt;-j = Replace the keystore file named certificates used by the http endpoint.-p =  Replace the keystore file named proxycertificates used by the console proxy endpoint.Start the Cell:service vmware-vcd startMultiple CellsIf you have multiple cells, simply copy the keystore to the other servers using an application like winSCPThen run the following:./cell-management-tool certificates -j -k /usr/local/vmware/certificates-new.ks -w &lt;password&gt;./cell-management-tool certificates -p -k /usr/local/vmware/certificates-new.ks -w &lt;password&gt;Re-start the Cell:We must now stop the task scheduler to quiesce the cell by running the command:/opt/vmware/vcloud-director/bin/cell-management-tool -u &lt;AdminUser&gt; cell --quiesce true This command prevents new jobs from being started. Existing jobs continue to run until they complete or are cancelled.When the Job Count = 0 and Is Active = false, it is safe to shut down the cell by running the command:./cell-management-tool -u &lt;;AdminUser&gt; cell --shutdownservice vmware-vcd start" }, { "title": "Docker install on CentOS & basic Docker commands", "url": "/posts/docker-install-on-centos-basic-docker-commands/", "categories": "Linux, CentOS", "tags": "centos, docker", "date": "2018-05-20 21:56:27 +0100", "snippet": "In this video I will take you through installing Docker on CentOS and some of the most common basic commands you will need to work with Docker.", "content": "In this video I will take you through installing Docker on CentOS and some of the most common basic commands you will need to work with Docker." }, { "title": "What is Docker? - Overview", "url": "/posts/what-is-docker-overview/", "categories": "Containers, Docker", "tags": "docker, container, overview", "date": "2018-05-20 00:00:00 +0100", "snippet": "In this video I talk about what Docker is, how it can be used and how containerisation differs from virtualisation.For anyone just getting into Docker this video and my Docker series will take you ...", "content": "In this video I talk about what Docker is, how it can be used and how containerisation differs from virtualisation.For anyone just getting into Docker this video and my Docker series will take you through that Journey." }, { "title": "Install, Configure and add a repository with Git on CentOS 7", "url": "/posts/install-configure-and-add-repository-with-git-on-centos-7/", "categories": "Linux, CentOS", "tags": "centos, code, commit, git, github, init, repo, repository", "date": "2018-04-07 08:36:46 +0100", "snippet": "Git is an open source, version control system (VCS). It’s commonly used for source code management by developers to allow them to track changes to code bases throughout the product lifecycle, with ...", "content": "Git is an open source, version control system (VCS). It’s commonly used for source code management by developers to allow them to track changes to code bases throughout the product lifecycle, with sites like GitHub offering a social coding experience, and multiple popular projects utilising it great functionality and availability for Open Source sharing.First off lets make sure that CentOS is up to date:yum update -yThen we can install Git, it couldn’t be simpler, just run the below command:yum install -y gitIf you want to see which version of Git has been installed then you can issue the below command:git --versionWhen you commit code Git will show errors if you don’t configure global user details, to do this you should run the following commands, change the user and email to your details:git config --global user.name \"MyUsername\"git config --global user.email \"MyEmail@example.com\"To confirm that the configuration has set as expected we can run the following, this will take us to our root directory and then show the contents of the file:cdcat .gitconfigor you can use the command:git config --listNow that we have Git installed and configured we need to create our first repository, a repository is where you place all of your code ready for committing to github. It is also possible to clone repositories from github and provide contributions to other peoples projects.Creating a RepositoryFirst we need to create a new folder where we will be storing our new project. Once this is done head into your directory:mkdir MyProjectcd MyProjectOnce we are inside our folder we need to initialise the repository, this will add all the files required for git to track the project:git initNow we have our repository! its that simple…To add files to our repository ready for the commit we would simply type:git addIf we want to commit this code up into github then we would need a couple of extra commands, these will add the remote origin where our code will be committed, then it will push our code up to the remote origin.git remote add origin https://github.com/username/new_repogit push -u origin masterCloning a RepositoryIn some cases you may already have a repository that you would like to clone and then change the existing code, well that is simple to do too. Get the URL for the clone from GitHub or any other Git SVN and type the following:git clone &lt;URL TO REPOSITORY&gt;This will then download the contents from the repository and onto your CentOS ServerHopefully this tutorial has been useful for you, please feel free to ask me any questions that you may have, or if you would like a more in depth article for further functions of Git." }, { "title": "JUNOS: Monitor Log Files", "url": "/junos-monitor-log-files/", "categories": "Networking", "tags": "juniper, junos, junos log moniroting, logs, match, monitor, start, stop, tail juniper logs, tail junos logs", "date": "2017-11-15 16:58:29 +0000", "snippet": "When working with JUNOS Switches etc. you may want to monitor the logs over a period of time without loading them every few minutes and scrolling to the bottom?Well these few commands show you how ...", "content": "When working with JUNOS Switches etc. you may want to monitor the logs over a period of time without loading them every few minutes and scrolling to the bottom?Well these few commands show you how to do this.In order to start the monitoring run the following command:monitor start &lt;log-file-name&gt;Here is an example command:monitor start messagesAny changes to the log file will automatically be posted to your screen.If you want to filter the logs to only show records with certain words then use the following command:monitor start messages | match errorIn order to stop the logs:monitor stopHopefully this article will assist you in viewing your logs with more ease." }, { "title": "CentOS 6/7 IPSec/L2TP VPN client to UniFi USG L2TP Server", "url": "/posts/centos-67-ipsecl2tp-vpn-client-unifi-usg-l2tp-server/", "categories": "Linux, Unifi", "tags": "centos, ipsec, vpn, l2tp, openswan, unifi, usg, xl2tpd", "date": "2017-08-06 21:30:46 +0100", "snippet": "Working with CentOS quite a lot I have spent time looking for configurations that work for various issues, one I have seen recently that took me a long time to resolve and had very poor documentati...", "content": "Working with CentOS quite a lot I have spent time looking for configurations that work for various issues, one I have seen recently that took me a long time to resolve and had very poor documentation around the net was setting up an L2TP VPN.In Windows or iOS its a nice simple setup where you enter all the required details and it sorts out the IPsec and L2TP VPN for you, In CentOS this is much different.First we need to add the EPEL Repository:Now we need to install the software:yum -y install epel-releaseNow we need to install the software:sudo yum -y install xl2tpd openswanMake sure to start the openswan service:systemctl start ipsec.serviceservice ipsec start If you don’t start this service first you will receive the error connect(pluto_ctl) failed: No such file or directoryOpenSwan (IPSec) Where you see %local change this to your Client local IP Address, where you see %server change this to the FQDN / IP of the VPN Server public IPConfigure IPSec VPN:config setup virtual_private=%v4:10.0.0.0/8,%v4:192.168.0.0/16,%v4:172.16.0.0/12 nat_traversal=yes protostack=netkeyconn L2TP-PSK authby=secret pfs=no auto=add keyingtries=3 dpddelay=30 dpdtimeout=120 dpdaction=clear rekey=yes ikelifetime=8h keylife=1h type=transport# Replace %local below with your local IP address (private, behind NAT IP is okay as well) left=%local leftprotoport=17/1701# Replace IP address with your VPN server's IP right=%server rightprotoport=17/1701This file contains the basic information to establish a secure IPsec tunnel to the VPN server. It enables NAT Traversal for if your machine is behind a NATing router (most people are), and other options that are required to connect correctly to the remote IPsec server.Create a file to contain the Pre-Shared Key for the VPN:%local %server : PSK \"your_pre_shared_key\"Remember to replace the local (%local) and server (%server) IP addresses with the correct numbers for your location. The pre-shared key will be supplied by the VPN provider and will need to be placed in this file in cleartext form.Add the connection so that we can use it:ipsec auto --add L2TP-PSKxl2tpd (L2TP)First we need to edit the configuration of xl2tpd with our new VPN:[lac vpn-connection]lns = %serverppp debug = yespppoptfile = /etc/ppp/options.l2tpd.clientlength bit = yesNow we need to create our options file:ipcp-accept-localipcp-accept-remoterefuse-eaprequire-mschap-v2noccpnoauthidle 1800mtu 1410mru 1410defaultrouteusepeerdnsdebuglogfile /var/log/xl2tpd.logconnect-delay 5000proxyarpname your_vpn_usernamepassword your_passwordPlace your assigned username and password for the VPN server in this file.Create the control file for xl2tpd:mkdir -p /var/run/xl2tpdtouch /var/run/xl2tpd/l2tp-controlThis completes the configuration of the applicable software suites to connect to a L2TP/IPsec server. To start the connection do the following:systemctl start ipsecsystemctl start xl2tpdsystemctl enable ipsec.servicesystemctl enable xl2tpd.serviceservice openswan startservice xl2tpd startchkconfig openswan onchkconfig xl2tpd onipsec auto --up L2TP-PSKecho \"c vpn-connection\" &gt; /var/run/xl2tpd/l2tp-controlAt this point the tunnel is up and you should be able to see the interface for it if you type:ifconfig ppp0RoutingNow that our tunnel is up and running we need to be able to route to our respective networks, this can be done two ways:Add the route manually each time the VPN restarts:route add -net xxx.xxx.xxx.xxx/xx dev ppp0 Replace the x with the server local network and subnet mask e.g. 192.168.10.0/24Add the route automatically: Edit the if-up file and add this before exit 0:case in 192.168.10.1) # VPN - IP ROUTE BEING ADDED AT RECONNECTION route add -net xxx.xxx.xxx.xxx/xxx dev ppp0; ;;esac Here we need to change 192.168.10.1 to the ppp0 gateway, also change the x with the server local network and subnet mask e.g. 192.168.10.0/24To check the route is added simply type:routeThis will display a list of routes and your new route should be listedTroubleshootingThe main logs to check are:/var/log/xl2tpd.log/var/log/messagesOpenSwan IPSecTo check that OpenSwan IPSec is working as expected run ipsec verify this will output similar to below:Verifying installed system and configuration filesVersion check and ipsec on-path [OK]Libreswan 3.15 (netkey) on 2.6.32-696.3.1.el6.x86_64Checking for IPsec support in kernel [OK] NETKEY: Testing XFRM related proc values ICMP default/send_redirects [NOT DISABLED] Disable /proc/sys/net/ipv4/conf/*/send_redirects or NETKEY will act on or cause sending of bogus ICMP redirects! ICMP default/accept_redirects [NOT DISABLED] Disable /proc/sys/net/ipv4/conf/*/accept_redirects or NETKEY will act on or cause sending of bogus ICMP redirects! XFRM larval drop [OK]Pluto ipsec.conf syntax [OK]Hardware random device [N/A]Checking rp_filter [ENABLED] /proc/sys/net/ipv4/conf/default/rp_filter [ENABLED] /proc/sys/net/ipv4/conf/lo/rp_filter [ENABLED] /proc/sys/net/ipv4/conf/eth0/rp_filter [ENABLED] rp_filter is not fully aware of IPsec and should be disabledChecking that pluto is running [OK] Pluto listening for IKE on udp 500 [OK] Pluto listening for IKE/NAT-T on udp 4500 [OK] Pluto ipsec.secret syntax [OK]Checking 'ip' command [OK]Checking 'iptables' command [OK]Checking 'prelink' command does not interfere with FIPSChecking for obsolete ipsec.conf options [OK]Opportunistic Encryption [DISABLED]ipsec verify: encountered 9 errors - see 'man ipsec_verify' for helpIf you see the same results as above create the following script to resolve this:#!/bin/bashecho 1 &amp;gt; /proc/sys/net/ipv4/ip_forwardfor each in /proc/sys/net/ipv4/conf/*do echo 0 &amp;gt; $each/accept_redirects echo 0 &amp;gt; $each/send_redirects echo 0 &amp;gt; $each/rp_filterdoneRun ipsec verify again to make sure all are green, ideally you shouldn’t encounter any errors.Startup / Shutdown Script#!/bin/bashif ! ifconfig | grep ppp0;then sudo ipsec auto --up L2TP-PSK sleep 3 sudo echo \"c vpn-connection\" &amp;gt; /var/run/xl2tpd/l2tp-controlfiif ! route | grep xxx.xxx.xxx.xxx;then sudo route add -net xxx.xxx.xxx.xxx/xx dev ppp0fiThis can then be created as a cron job to make sure the vpn is always up and running." }, { "title": "Upgrade your Linux UniFi Controller in minutes!", "url": "/upgrade-linux-unifi-controller-minutes/", "categories": "Ubiquiti", "tags": "bash, centos, controller, linux, script, ubiquiti, unifi, upgrade", "date": "2017-02-25 21:03:16 +0000", "snippet": "Ubiquiti’s provide a Controller version for other distributions of linux but only display debian on their site, but if you’re running CentOS or another Linux distribution, you’ll have to use the ge...", "content": "Ubiquiti’s provide a Controller version for other distributions of linux but only display debian on their site, but if you’re running CentOS or another Linux distribution, you’ll have to use the generic controller package. The upgrade provess is so simple! (i have also written this script that makes it even quicker)I previously explained how to install your own UniFi Controller on CentOS in this article. Once you have it up and running, it’s even easier to upgrade to a newer version. The process takes less than 3 minutes with these steps.This upgrade was tested on version 5.3.11 to 5.4.11 but should be the same for all versionsUPDATE: I have also upgraded 5.4.11 to 5.5.11 with no issuesStop the UniFi Controller service:systemctl stop unifiTake a backup of the current unifi folder:cp -R /opt/UniFi/ /opt/UniFi_bak/Download the new version:cd ~ &amp;&amp; wget http://dl.ubnt.com/unifi/5.4.11/UniFi.unix.zipUnzip the downloaded file into the correct directory:unzip -q UniFi.unix.zip -d /optCopy the old data back into the UniFi folder, this allows historical data to be kept:cp -R /opt/UniFi_bak/data/ /opt/UniFi/data/Restart the UniFi Controller service:systemctl start unifiWait a little while for your controller to load back up, once completed you can login as normal and you should still have all your legacy data still visible.thats it youre done, simple!" }, { "title": "Setup Ubiquiti UniFi USG Remote User VPN", "url": "/posts/setup-ubiquiti-unifi-usg-remote-user-vpn/", "categories": "Networking, Unifi", "tags": "remove, ubiquiti, unifi, user, usg, vpn", "date": "2017-02-03 17:20:22 +0000", "snippet": "I have recently had loads of trouble setting up a Ubiquiti UniFi USG remote user VPN, the USG requires a RADIUS server in order to function correctly, the following article covers this setup freeRA...", "content": "I have recently had loads of trouble setting up a Ubiquiti UniFi USG remote user VPN, the USG requires a RADIUS server in order to function correctly, the following article covers this setup freeRADIUS SetupOnce RADIUS is setup the easy part is configuring the USG through the UniFi controller. First you will need to login to your UniFi Controller Go to the settings  Then select networks Create a new network Add a name for the VPN Select Remote User VPN for the Purpose Enter and IP Address with CIDR e.g. 192.168.10.1/24 Enter the IP Address for your RADIUS Server Enter the port for your RADIUS Server (Default is 1812) Enter your RADIUS Servers Secret Key / Password Click SaveThat is all you need to do! In version 5.3.11 and below P2TP is not supported which means it will not work with iPhones / iPads etc. this is supposed to be resolved in the next release." }, { "title": "Install UniFi Controller on CentOS 7", "url": "/posts/install-unifi-controller-centos-7/", "categories": "Linux, Unifi", "tags": "centos, centos7, controller, ubuquiti, unifi", "date": "2017-02-01 09:50:25 +0000", "snippet": "This is a short simple guide to assist users with installing the Ubiquiti UniFi Controller required for all UniFi devices on a CentOS 7 Server.First we need to update our CentOS server and disable ...", "content": "This is a short simple guide to assist users with installing the Ubiquiti UniFi Controller required for all UniFi devices on a CentOS 7 Server.First we need to update our CentOS server and disable SELinux:yum -y updatesed -i /etc/selinux/config -r -e 's/^SELINUX=.*/SELINUX=disabled/g'systemctl reboot You don’t need to disable SELinux however may experience issues if it isn’t setup correctly.Now we need to make sure we have EPEL Repo:yum -y install epel-releaseInstall services required by the Controller:yum -y install mongodb-server java-1.8.0-openjdk unzip wgetCreate our service user account:useradd -r ubnt -s /sbin/nologinWe put the -s /sbin/nologin so that this user cannot be logged in as a user, only the service will be able to use this account.Download and extract the UniFi Controller software:cd ~ &amp;&amp; wget http://dl.ubnt.com/unifi/5.3.11/UniFi.unix.zipunzip -q UniFi.unix.zip -d /optchown -R ubnt:ubnt /opt/UniFiAt the time of writing the latest version was v5.3.11, change the version number in the download link to the latest version.Create a startup script for use with Systemd:## Systemd unit file for UniFi Controller#[Unit]Description=UniFi ControllerAfter=syslog.target network.target[Service]Type=simpleUser=ubntWorkingDirectory=/opt/UniFiExecStart=/usr/bin/java -Xmx1024M -jar /opt/UniFi/lib/ace.jar startExecStop=/usr/bin/java -jar /opt/UniFi/lib/ace.jar stopSuccessExitStatus=143[Install]WantedBy=multi-user.targetConfigure Firewalld:systemctl stop firewalld.service&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;service version=\"1.0\"&gt; &lt;short&gt;unifi&lt;/short&gt; &lt;description&gt;UniFi Controller&lt;/description&gt; &lt;port port=\"8081\" protocol=\"tcp\"/&gt; &lt;port port=\"8080\" protocol=\"tcp\"/&gt; &lt;port port=\"8443\" protocol=\"tcp\"/&gt; &lt;port port=\"8880\" protocol=\"tcp\"/&gt; &lt;port port=\"8843\" protocol=\"tcp\"/&gt; &lt;port port=\"10001\" protocol=\"udp\"/&gt; &lt;port port=\"3478\" protocol=\"udp\"/&gt;&lt;/service&gt;Once the firewall rules xml file is created we need to add this to our firewall zones, the default zone will be public but you should know for your firewall.systemctl start firewalld.servicefirewall-cmd --permanent --zone=public --add-service=unifiEnable the service to run when the server boots:systemctl enable unifi.serviceRemove the zip and reboot the server:rm -rf ~/UniFi.unix.zipsystemctl rebootOnce the server is back online you should be able to access the controller via the URL: https://FQDN\\_or\\_IP:8443 Follow the simple wizard to complete the setup of your controller, I would also recommend you register with Ubiquiti when you setup the controller, this will allow you to manage it remotely on a mobile device.Credit to: https://deviantengineer.com" }, { "title": "Install FreeRadius on CentOS 7 with DaloRadius for management &#8211; Updated", "url": "/posts/install-freeradius-centos-7-with-daloradius-for-management/", "categories": "", "tags": "", "date": "2017-02-01 00:00:00 +0000", "snippet": "I have recently purchased a load of Ubiquiti UniFi equipment, as part of this i have the UniFi USG which in order to deploy a User VPN requires a RADUIS Server for user authentication. This article...", "content": "I have recently purchased a load of Ubiquiti UniFi equipment, as part of this i have the UniFi USG which in order to deploy a User VPN requires a RADUIS Server for user authentication. This article will run through how to install and set this up.I will be using FreeRADIUS as this is the most commonly used, it supports most common authentication protocols.Disable SELinux:vi /etc/sysconfig/selinuxSELINUX=disabledFirst we need to update our CentOS server and install the required applications:yum install -y epel-releaseyum install -y http://rpms.remirepo.net/enterprise/remi-release-7.rpmyum-config-manager --enable remi-php72yum update -yyum install -y freeradius freeradius-utils freeradius-mysql nginx mariadb-server mariadb php-cli php-mysqlnd php-devel php-gd php-mcrypt php-mbstring php-xml php-pear php-fpmpear channel-update pear.php.netpear install DBsystemctl rebootWe must now enable the FreeRADIUS, MariaDB, PHP-FPM and Nginx services to run at boot:systemctl enable radiusdsystemctl enable nginxsystemctl enable mariadbsystemctl enable php-fpmsystemctl start mariadbWe need to configure MariaDB:mysql_secure_installationSet the root passwordRemove the Anonymous UserDisable root remote loginRemove Test DBsReload PrivilegesAllow local connections only: vim /etc/my.cnf [mysqld] bind-address=127.0.0.1Configure the database to work with freeRADIUS:mysql -u root -pCREATE DATABASE radius;GRANT ALL ON radius.* TO radius@localhost IDENTIFIED BY \"radiuspassword\";FLUSH PRIVILEGES;quitWe need to add Radius and HTTP ports to the firewall:systemctl start firewalldfirewall-cmd --zone=public --add-service=radius --add-service=http --permanentfirewall-cmd --reloadNow we will run Radius in debug mode to make sure it runs correctly:radiusd -XImport the Radius database scheme:mysql -u root -p radius &lt; /etc/raddb/mods-config/sql/main/mysql/schema.sqlCreate a soft line for SQL:ln -s /etc/raddb/mods-available/sql /etc/raddb/mods-enabled/configure the SQL module and change the database connection, edit the existing file, find the text below and make sure it matches:vi /etc/raddb/mods-available/sqlsql { driver = \"rlm_sql_mysql\" dialect = \"mysql\" # Connection info: server = \"localhost\" port = 3306 login = \"radius\" password = \"radiuspassword\" # Database table configuration for everything except Oracle radius_db = \"radius\"}# Set to ‘yes’ to read radius clients from the database (‘nas’ table)# Clients will ONLY be read on server startup.read_clients = yes# Table to keep radius client infoclient_table = “nas”Change the group for the SQL folder to radiusd:chgrp -h radiusd /etc/raddb/mods-enabled/sqlConfigure PHP (update the below lines in the file):vi /etc/php-fpm.d/www.conflisten = /var/run/php-fpm/php-fpm.socklisten.owner = nobodylisten.group = nobodyuser = nginxgroup = nginxConfigure Nginx (add the “location” :vi /etc/nginx/conf.d/default.confserver { ##other data here location ~ \\.php$ { try_files $uri =404; fastcgi_pass unix:/var/run/php-fpm/php-fpm.sock; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; }}Installing Daloradius:wget https://github.com/lirantal/daloradius/archive/master.zipunzip master.zipmv daloradius-master/ daloradiuscd daloradiusImport Daloradius MySQL:mysql -u root -p radius &amp;lt; contrib/db/fr2-mysql-daloradius-and-freeradius.sqlmysql -u root -p radius &amp;lt; contrib/db/mysql-daloradius.sqlMove to the httpd directory:cd ..mv daloradius /usr/share/nginx/htmlchange permissions for httpd:chown -R nginx:nginx /usr/share/nginx/html/daloradius/chmod 664 /usr/share/nginx/html/daloradius/library/daloradius.conf.phpModify configuration for MySQL:vi /usr/share/nginx/html/daloradius/library/daloradius.conf.phpCONFIG_DB_USERCONFIG_DB_PASSCONFIG_DB_NAMETo make sure everything works restart all services:systemctl restart radiusdsystemctl restart mariadbsystemctl restart php-fpmsystemctl restart nginxAccess the web interface:http://FQDN_IP_OF_SERVER/daloradius/login.phpDefault Login:User: AdministratorPass: radius" }, { "title": "Two-Factor Authentication: is it worth it, does it really add more security?", "url": "/posts/two-factor-authentication-worth-really-add-security/", "categories": "Musings", "tags": "2fa", "date": "2016-12-19 19:00:00 +0000", "snippet": "As we all move to a digital age, adding more and more personal information to the internet security has become a real issue, in recent years there have been hack attempts on well-known brands, incl...", "content": "As we all move to a digital age, adding more and more personal information to the internet security has become a real issue, in recent years there have been hack attempts on well-known brands, including LastPass, LinkedIn, Twitter and Adobe.This has cast a light on the problems that passwords bring and how vulnerable users are as a result. Most of these companies are now implementing Two-Factor authentication, but is it really as secure as we are lead to believe? what are its pitfalls?In this article i’m going to go through some of the pros and cons relating to Two-Factor authentication (or 2FA)What is 2FA?Simply put Two-Factor authentication / multi-factor authentication is the ability to employ multiple layers of authentication, in most cases this would be your password and then a token that expires after a short period of time.Other types of authentication could include but are not limited to: Finger Print Recognition Retinal scanners Face RecognitionTry this example:You have a house, with a safe, inside is a gold bar. The safe has a combination on it that only you know and the house has a door that is locked, only you have the key for this door. It takes two steps of “authentication” to get into the safe and retrieve your gold.If you added more doors with different locks this would add more “authentication” and it would make the house harder to enter to get to the safe.How does it work?There are multiple ways that 2FA tokens work, one method is time based. Both the server and client take the current time e.g. 15:15 they then turn this into a number 1515 and run it through an algorithm that hashes it into a multiple digit code, both devices use the same algorithm to generate the code and thus both generate the same code (as long as the times match), this is obviously a very simplified explanation but shows how both the server and client generate the same codes securely.To setup 2FA in most cases the website you are using will have a QR-Code that you can scan into an app such as Authy or Google Authenticator, this will then display a numbered token for around 8 seconds before expiring and a new code being generated. After you have entered your conventional username and password you would be prompted for your “Token” once entered you will be authenticated into your account. If you don’t type the token and submit it before the token expires your authentication would fail and you would need to enter the new token.How Secure is 2FA?Like any security mechanism there are ways that it can be hacked/compromised, however with two layers of authentication we are making it much harder for any hacker to gain access to our accounts, most people use the same password across multiple websites, with this method if someone does get that password but doesn’t have the 2FA Token then they aren’t getting into your accounts.Not all deployments of 2FA are as secure as others, this comes down to the algorithms that are used and the reliance on any 3rd party servers to generate the 2FA Tokens. The type of 2FA used would really depend on the application and users that would be using it. Hardware based 2FA is much more secure than software based but relies on 3rd party hardware.ConclusionPersonally I believe that 2FA should be used where possible, if you have a smartphone that can install one of the 2FA applications I see no reason to avoid this.It makes your accounts and personal information more secure and most importantly harder to hack!" }, { "title": "Snapshot changes in vSphere 6.0", "url": "/posts/snapshot-changes-vsphere-6-0/", "categories": "", "tags": "", "date": "2016-07-26 00:00:00 +0100", "snippet": "This is something that I was unaware of until recently when I was looking into the usage of V-Vols. It appears that VMware have made some major improvements to the ways we handle snapshots and cons...", "content": "This is something that I was unaware of until recently when I was looking into the usage of V-Vols. It appears that VMware have made some major improvements to the ways we handle snapshots and consolidate them in vSphere 6.0 with VVols. Most people who use VMware are aware of limitations with snapshots on VMs that have heavy IO or large snapshots attached to them. In a large number of cases we see snapshots fail to remove and then require hours of downtime to actually consolidate.Previously we would take a snapshot, this would make the VMDK Read-Only and create a new Delta file that all the new changes would be written to. this file would continue to grow and potentially would end up as big as the VM’s allocated space. Depending on the size of the snapshot we would also take helper snapshots or “Safe Removal Snapshots”, these would allow us to lower the IO on the large snapshot so that the VM didnt see as big an impact when consolidating the first larger snapshot. This would then mean we could remove the helper snapshot, in some cases though the IO was too high for this to work. This could cause VMware to “Stun” the server effectively freezing IO and allowing the snapshot removal to take over causing downtime to our end users.Eventually if you were unable to merge the snapshots to the base disk the server would need to be powered down and the snapshot removed, this could take hours…In vSphere 6.0 with VVols this has totally changed! As you can see we now take a snapshot, but the base disk is still Read/Write, multiple delta files are created with the changed original data. This means that when we remove the snapshot all we need to do is tell VMware to delete the deltas, no need to write it all to the base VMDK as its already there. This technique was first implemented for the VMware Mirror Driver in vMotion, VMware have now utilised this to provide a near seamless snapshot capability in v6.0 stopping large amounts of downtime all together. There should no longer be any noticeable stun time as we are only removing the references to the snapshot.Interesting piece of information that I thought some of you might find useful.UPDATE:I decided to do a test of snapshot removal times, Using the same VM on both VVol and a normal Datastore by writing a 10gb file to them in the same manner. The VM on VVol took 3 seconds to remove, the VM on the normal Datastore took just over 3 minutes. This doesn’t sound like a lot but this is a lab on a VM with no load, imagine a 100GB snapshot with heavy load!So it looks like there are huge benefits to be had with VVol moving forwards." }, { "title": "Teamspeak 3 on CentOS 7 using MariaDB Database (3.0.12.4)", "url": "/posts/teamspeak-3-centos-7-using-mariadb-database-3-0-12-4/", "categories": "", "tags": "", "date": "2016-05-25 00:00:00 +0100", "snippet": "This tutorial takes you through setting up Teamspeak 3 on CentOS 7, I will also be going through using a MariaDB database for the backend and a custom system services script.We are using MariaDB as...", "content": "This tutorial takes you through setting up Teamspeak 3 on CentOS 7, I will also be going through using a MariaDB database for the backend and a custom system services script.We are using MariaDB as MySQL no longer ships with CentOS and MariaDB is a fork of MySQLCheckout the video at YouTube:A few prerequisites that will be required before proceeding with this tutorial:yum update -yyum install wget perl net-tools mariadb mariadb-server -yNow we need to create a new user on our server, this user will be used for the installation and running of TeamSpeak. For security reasons this user will not have sudo.useradd ts3userpasswd ts3userOur installation is complete so we can configure MySQL with a Database and User for Teamspeak to utilise:systemctl start mariadbsystemctl enable mariadbSecure MySQL follow the wizard:mysql_secure_installationLogin to MySQL:mysql -uroot -pRun These queries:create database ts3db;grant all on ts3db.* to 'ts3user'@'127.0.0.1' identified by 'ts3password';flush privileges;Once the MySQL Database is set-up along with a user we will create a system service script for Teamspeak so that we can start the server as a service, create the script: vi /usr/lib/systemd/system/ts3server.service[Unit]Description=TeamSpeak 3 ServerAfter=network.target[Service]WorkingDirectory=/home/ts3user/bin/teamspeak3/User=ts3userGroup=ts3userType=simpleExecStart=/home/ts3user/bin/teamspeak3/ts3server_startscript.sh start inifile=ts3server.iniExecStop=/home/ts3user/bin/teamspeak3/ts3server_startscript.sh stopPIDFile=/home/ts3user/bin/teamspeak3/ts3server.pidRestartSec=15Restart=always[Install]WantedBy=multi-user.targetsudo to our TS3 user created at the beginning of this tutorial, download Teamspeak Server 3 64-bit for Linux and extract in your home directory (get the latest version here: http://www.teamspeak.com/?page=downloads)su as TS3User:su ts3userDownload TS3, extract:cdmkdir bincd binwget http://dl.4players.de/ts/releases/3.0.12.4/teamspeak3-server_linux_amd64-3.0.12.4.tar.bz2tar -xvf teamspeak3-server_linux_amd64-3.0.12.4.tar.bz2mv teamspeak3-server_linux_amd64 teamspeak3cd teamspeak3We must edit ts3server.ini which stores the configuration for the teamspeak server, we will be changing the config to work with MySQL instead of SQLite: vi ts3server.inimachine_id=1default_voice_port=9987voice_ip=0.0.0.0licensepath=filetransfer_port=30033filetransfer_ip=0.0.0.0query_port=10011query_ip=0.0.0.0dbplugin=ts3db_mariadbdbpluginparameter=ts3db_mariadb.inidbsqlpath=sql/dbsqlcreatepath=create_mariadb/logpath=logslogquerycommands=0dbclientkeepdays=30logappend=0query_skipbruteforcecheck=0We must also create a file called ts3db_mariadb.ini, this will hold your database login details:[config]host=127.0.0.1port=3306username=ts3userpassword=ts3passworddatabase=ts3dbsocket=Start Teamspeak:./ts3server_startscript.sh startYou should now see that Teamspeak 3 is installed and you will see a message on screen with a privilege token and your server query admin account details, it is important to copy these as you will need them to administer your server.Stop the server:./ts3server_startscript.sh stopCheck the logs in the log directory. if everything is OK, su back in as root:su rootEnable the plugin library, service and start it:cp redist/libmariadb.so.2 /lib64/libmariadb.so.2ldd /home/ts3user/bin/teamspeak3/libts3db_mariadb.sochmod +x /usr/lib/systemd/system/ts3server.servicesystemctl enable ts3serversystemctl start ts3serverNow our server installation is completed we can open the ports on our firewall:Voice:firewall-cmd --zone=public --add-port=9987/udp --permanentServer Query (good idea to restrict IP):firewall-cmd --zone=public --add-port=10011/tcp --permanentFile Transfer:firewall-cmd --zone=public --add-port=30033/tcp --permanentReload the firewall:firewall-cmd --reloadand connect with our TS3 Client. The first person to logon will be asked to provide a privilege key, enter the one retrieved during the installation." }, { "title": "Teamspeak 3 Recovering privilege key after first startup (MySQL/MariaDB Only)", "url": "/posts/teamspeak-3-recovering-privilege-key-first-startup-mysqlmariadb/", "categories": "Linux, Teamspeak", "tags": "ts3, mysql, mariadb, teamspeak, recover, privilege", "date": "2016-05-05 00:00:00 +0100", "snippet": "When deploying a Teamspeak3 server one thing that is vital for the first time startup is to make a note of the privilege key, but what do you do if for some reason you didn’t write it down?In this ...", "content": "When deploying a Teamspeak3 server one thing that is vital for the first time startup is to make a note of the privilege key, but what do you do if for some reason you didn’t write it down?In this article I will show you how to retrieve it! Login to your Teamspeak3 server Connect to SQL: mysql -uyouruser -p Select your TS3 Database: USE &lt;DatabaseName&gt;; Sleect the Tokens Table: SELECT * FROM tokens; You should see a privilege key copy this (token_key) columnIts as simple as that! the privilege key can only be used once, when it has been used it will be removed from the tokens table." }, { "title": "vCloud Director 8.0 for Service Providers", "url": "/posts/vcloud-director-8-0-service-providers/", "categories": "", "tags": "", "date": "2016-03-30 00:00:00 +0100", "snippet": "As most of you will now be aware VMware decided to end availability for vCloud Director and shift to only allow service providers to utilise the product.Originally the idea was that organisations w...", "content": "As most of you will now be aware VMware decided to end availability for vCloud Director and shift to only allow service providers to utilise the product.Originally the idea was that organisations would use vCloud Director for test environments but as the “Cloud” becomes cheaper and companies move their hosting out to 3rd party providers it makes sense for VMware to push consumers towards hosted platforms for cheaper billing and better support.With the release of the vRealize product suite we see the new Automation product that allows users to automate deployments on hosted vCloud platforms which is a great step forwards.So what’s new in vCloud Director 8.0? vSphere 6.0 Support: Support for vSphere 6.0 in backward compatibility mode. NSX support: Support for NSX 6.1.4 in backward compatibility mode. This means that tenants’ consumption capability is unchanged and remains at the vCloud Networking and Security feature level of vCloud Director 5.6. Organization virtual data center templates: Allows system administrators to create organization virtual data center templates, including resource delegation, that organization users can deploy to create new organization virtual data centers. vApp enhancements: Enhancements to vApp functionality, including the ability to reconfigure virtual machines within a vApp, and network connectivity and virtual machine capability during vApp instantiation. OAuth support for identity sources: Support for OAuth2 tokens. Tenant throttling: This prevents a single tenant from consuming all of the resources for a single instance of vCloud director. Ensuring fairness of execution and scheduling among tenants.So not much has changed even though the version number has jumped quite dramatically. One thing that i will be interested in seeing is if the NSX Support adds much more functionality and what the upgrade paths are from vCNS to NSX for existing providers." }, { "title": "Failed to connect to VMware Lookup Service, SSL certificate verification failed", "url": "/posts/failed-connect-vmware-lookup-service-ssl-certificate-verification-failed/", "categories": "", "tags": "", "date": "2016-03-30 00:00:00 +0100", "snippet": "Recently I have been playing in my lab with VCSA and vCNS, I found that when I tried to connect to the vCenter I received this error:Failed to connect to VMware Lookup Service.SSL certificate verif...", "content": "Recently I have been playing in my lab with VCSA and vCNS, I found that when I tried to connect to the vCenter I received this error:Failed to connect to VMware Lookup Service.SSL certificate verification failed.I was stuck for a little while as to why I was getting this error, then I noticed that the SSL Cert had a different name to the appliance due to it being deployed and then renamed. Lucky for me the fix for this is very simple! Go to http://:5480 Click the “Admin” tab Change “Certificate regeneration enalbed” to yes, this is either done with a toggle button to the right or radio button depending on the VCSA Version. Restart the vCenter Appliance Once the Appliance reboots it will re-generate the certificates Change “Certificate regeneration enalbed” to no, this is either done with a toggle button to the right or radio button depending on the VCSA Version.try to reconnect your appliance / application to vCenter and it should work no problems." }, { "title": "How to check if a VM disk is Thick or Thin provisioned", "url": "/posts/check-vm-disk-thick-thin-provisioned/", "categories": "", "tags": "", "date": "2016-03-16 00:00:00 +0000", "snippet": "There are multiple ways to tell if a virtual machine has thick or thin provisioned VM Disk. Below are some of the ways I am able to see this information:VI Client (thick client) Select the Virtual...", "content": "There are multiple ways to tell if a virtual machine has thick or thin provisioned VM Disk. Below are some of the ways I am able to see this information:VI Client (thick client) Select the Virtual Machine Choose Edit Settings Select the disk you wish to check look under TypeWeb ClientSelect your Host in Host and Cluster inventory -&gt; Related Objects -&gt; Virtual machines tab Select your Host in Host and Cluster click Related Objects click Virtual machines tabPowerCLI Launch PowerCLI type: connect-viserver Run this command: get-vm | Select Name, @{N=\"disktype\";E={(Get-Harddisk $_).Storageformat}} A list of all the virtual machines and disk types will be presentedRvTools Launch RV Tools and enter the vCenter IP Address or Name Enter the login details or tick use windows credentials Go to the “vDisk” tab" }, { "title": "Bulk configure vCenter Alarms with PowerCLI", "url": "/posts/bulk-configure-vcenter-alarms-powercli/", "categories": "", "tags": "", "date": "2016-01-17 00:00:00 +0000", "snippet": "I was recently asked if it was possible to update vCenter alarms in bulk with email details. So i set about writing the below script, basically this script will go through looking for any alarms th...", "content": "I was recently asked if it was possible to update vCenter alarms in bulk with email details. So i set about writing the below script, basically this script will go through looking for any alarms that match the name you specify and set the email as required.This is a really basic script and can easily be modified to set alarms how you want them.$MinutesToRepeat = \"10\"$alarms = @(\"Testing Alarm\")$cluster = Get-Cluster \"Test Cluster\"$AdminEmail = \"your@email.com\"foreach ($alarm in $alarms) { Set-AlarmDefinition -Name $alarm -AlarmDefinition $alarm -ActionRepeatMinutes $MinutesToRepeat | %{ $_ | Get-AlarmAction -ActionType \"SendEmail\" | Remove-AlarmAction -Confirm:$false $_ | New-AlarmAction -Email -To $AdminEmail | %{ $_ | New-AlarmActionTrigger -StartStatus Green -EndStatus Yellow $_ | New-AlarmActionTrigger -StartStatus Red -EndStatus Yellow $_ | New-AlarmActionTrigger -StartStatus Yellow -EndStatus Green } } $AlarmAction = Get-Alarmdefinition -Name $alarm | Get-AlarmAction -ActionType 'SendEmail' $AlarmAction.Trigger | Where {($_.StartStatus -eq 'Yellow') -And ($_.EndStatus -eq 'Red')} | Remove-AlarmActionTrigger -Confirm:$False $AlarmAction | New-AlarmActionTrigger -StartStatus Yellow -EndStatus Red -Repeat}To edit multiple alarms at once simply change the $alarms variable as below:$alarms = @(\"Test Alarm1\", \"Test Alarm2\")One thing you will probably notice is that we set the “Yellow” to “Red” status after everything else, the reason for this is that it is set by default when creating the alarm definition and we need to unset this before resetting with the required notification type." }, { "title": "VMware ESXi Embedded Host Client Installation &#8211; Updated", "url": "/posts/vmware-esxi-embedded-host-client-installation/", "categories": "", "tags": "", "date": "2015-08-16 00:00:00 +0100", "snippet": "In this article I will be showing you guys the new ESXi Embedded Host Client, this has been long awaited by many users of the Free ESXi host and allows much better management of the host.Check out ...", "content": "In this article I will be showing you guys the new ESXi Embedded Host Client, this has been long awaited by many users of the Free ESXi host and allows much better management of the host.Check out the latest version in this video:InstallationThe easiest way to install a VIB is to download it directly on the ESXi host.If your ESXi host has internet access, follow these steps: Enable SSH on your ESXi host, using DCUI or the vSphere web client. Connect to the host using an SSH Client such as putty Run the below command:esxcli software vib install -v http://download3.vmware.com/software/vmw-tools/esxui/esxui-2976804.vibIf the VIB installation completes successfully, you should now be able to navigate a web browser to https:///ui and the login page should be displayed.UsageThe login page is the same one used for vCenter Server, On logging in you will also see the menu structures follow this look and feel.From the interface you are able to do most of the features seen in the old VI Client. It is very responsive (compared to the vCenter versions) and seems to work well.One feature that is a little frustrating is the inability to edit settings of a powered on virtual machine. So you would either need to use command, the old VI Client or Power Off the VM.A few things that are still “under construction” are:Host ManagementAuthenticationCertificatesProfilesPower ManagementResource ReservationSecuritySwapHost -&gt; Manage -&gt; Virtual Machines View&lt;/ol &lt;/ul&gt;Virtual MachineLog BrowserNetworkingMonitor TasksRemovalTo remove the ESXi embedded host client from your ESXi host, you will need to use esxcli and have root privileges on the host. Connect to the host using and SSH Client such as putty Log into the host and run the following command:esxcli software vib remove -n esx-uiIf you have any comments, tips or tricks, please let me know over on my Discord" }, { "title": "Mikrotik OpenVPN Server with Linux Client", "url": "/posts/mikrotik-openvpn-server-with-linux-client/", "categories": "Networking", "tags": "openvpn, mikrotik, linux", "date": "2015-07-29 00:00:00 +0100", "snippet": "I spent quite some time trying to get the OpenVPN Server working on the Mikrotik Router with a Linux client, It caused some pain and I didn’t want others to go through that. I have therefore writte...", "content": "I spent quite some time trying to get the OpenVPN Server working on the Mikrotik Router with a Linux client, It caused some pain and I didn’t want others to go through that. I have therefore written this guide, taking you from certificate creation all the way to VPN connectivity.For this tutorial I will have SSH to my Mikrotik (you can use a winbox terminal), I have chosen not to use WinBox for the configuration as its easier to deploy this way.Certificate CreationFirst we need to create our certificate templates on our Mikrotik./certificateadd name=ca-template country=GB locality=Leeds organization=SpottedHyena state=WestYorkshire common-name=\"home.server.co.uk\" key-size=2048 unit=ITName (Optional): This is the visual name for the template, set this as required.Country (Optional: The country that you are in, this should be denoted by the 2 letter prefix e.g. GB for Great Britain.Locality (Optional): This is the City / Town.Organization (Optional): If you are an Organization enter the name here.State (Optional): Your State / Province.Common-Name (Required): The common name should be the FQDN of your Server / Mikrotik, this cannot be an IP Address and must be a valid domain name accessible publicly.Key-Size (Optional): Although this is optional I would recommend setting it to at lease 2048 for security 1024 is the default and not as secure.Unit (Optional): this is the department the certificate belongs to.Once we have generated our template we now need to create a certificate request and submit it to our CA/certificatecreate-certificate-request template=ca-templatePress enter and then type a passphrase, keep this safe it will be required when using the certificate.Once the certificate is created you will get 2 new files, using an FTP Client such as Filezilla connect to the Mikrotik and download these two files: certificate-request.pem certificate-requset_key.pemEdit the certificate-request.pem file in your favourite Notepad Product copy all of the contents from this file including the begin and end tags.Now we need to login to CAcert.org (if you don’t have an account create one)Once logged in select Domains Add in here type the domain name you used for your Common-Name e.g. 35.176.61.220 (you dont need the subdomain if one was used) You will then be asked to verify the domain via an email to one of a list of possible email accounts, verify the domain before continuing.When the domain is verified go to Server Certificate NewPaste the contents of certificate-request.pem into the box and submit the request.This should be accepted straight away and you will receive your certificate in text form on the next page.Copy and Paste your Certificate Response from Cacert in a notepad and save that with .pem file ( In Here : certificate-response.pem )Private KeyWe now need to convert the private key, this is generated in pkcs8 format, this is not supported by RouterOS.To import the private key use linux openssl and make a private-key file.yum install opensslUpload the certificate-requset_key.pem file to the Linux server and run the following command:openssl rsa -in certificate-requset_key.pem -textcopy and paste export String (Include Begin and End tags) to a New File eg certificate-requset_key.keyImport CertificateUpload the files Certificate-Response.pem, certificate-requset_key.key and ca.crt (root ca from CACert.org) to your Mikrotik with FilezillaWe now need to import the Certificate-response.pem with the passphrase you set earlier:/certificateimport file-name=certificate-response.pemNow import the certificate-requset_key.key/certificateimport file-name=certificate-request_key.keyNow import the Intermidiate (CA Cert) from CACert.org /certificateimport file-name=ca.crtAlways import the certificate first, then the key. You should be able to do a /certificate print and see the entries for the files you imported. In the print output, look at the flags column and verify that the line with your certificate has a T and a K. If the K is missing, import the key one more time. If that still doesn’t work, ensure that your certificate and key match.The default naming conventions used for certificates is a little confusing. You can rename a certificate by running set name=firewall.35.176.61.220 number=0 (run a /certificate print to get the right number).OpenVPN Server Configuration(Credit to major.io for parts of this section.Now we are ready to begin the configuration of our OpenVPN Server/interface ovpn-server serverset certificate=firewall.35.176.61.220 cipher=aes256 default-profile=default-encryption enabled=yesThis tells the device that we want to use our certificate we created and imported earlier along with AES256 Ciphers, there are more ciphers available however at the time of writing AES256 was the most secure available. We are also selecting default-encryption profile, we will configure this in more detail later.We now need to add an OpenVPN interface to the Mikrotik. You can have multiple OpenVPN Server profiles running under the same server. They will all share the same certificate, but each may have different configurations. Below we create the first profile:/interface ovpn-serveradd name=openvpn-inbound user=openvpnThere is now a profile with a username openvpn. That will be the username that we use to connect to this VPN server.SecretsThe router needs a way to identify the user we just created:/ppp secretadd name=openvpn password=ovpnpassword profile=default-encryptionProfilesWe have referred to this default-encryption profile and now it’s time to configure it. This is one of the things I prefer to configure using the Winbox GUI or the web interface since there are plenty of options to review. (in winbox select PPP then the profiles tab, open default-encryption profile)The most important part is how you connect the VPN connection into your internal network. You have a few options here. You can configure an IP address that will always be assigned to this connection no matter what. There are upsides and downsides with that choice. You will always get the same IP on the inside network but you won’t be able to connect to the same profile with multiple clients.I prefer to set the bridge option to my internal network bridge (which I call bridge1). That allows me to use my existing bridge configuration and filtering rules on my OpenVPN tunnels. My configuration looks something like this:/ppp profileset 1 bridge=bridge1 local-address=dhcp_pool1 only-one=no remote-address=dhcp_pool1Here we have told the router that we want VPN Connections to use the main bridge, and should get its local and remote addresses from my normal DHCP Pool. In addition we are allowing multiple connections to this profile.Firewall RuleThe following firewall rule will be required to allow traffic into the OpenVPN Port:/ip firewall filter add chain=input dst-port=1194 protocol=tcpOpenVPN ClientThis is where I got stuck the most, no articles explain where to get the Certificate for the Client from, for a seasoned pro this may seem like something very simple however to a newbie to this type of deployment its information I really could have done with!Well the answer I now know is quite simple, you use the same Certificate for a Client and Server, Lots of articles I found mentioned having a client cert but that didn’t seem to work for me.First we must install OpenVPN and create the configurationyum install openvpn -yThe configuration files live here: /etc/openvpn/cd /etc/openvpn/we will need to create firewall-auth.txt and home.up see the contents of these files below: vi firewall-auth.txtusernamepasswordThe firewall-auth.txt file holds our username and password, this will allow OpenVPN to login and restart the connection without user interaction.Required Certificates:openssl dhparam -out dh2048.pem 2048This is our Diffie–Hellman file which is required by OpenVPN.We also need to copy the following files ca.crt, certificate-response.crt and private-key.keyvi MyVpn.ovpndev tunpulltls-clientdh dh2048.pemca ca.crtcert certificate-response.crtkey certificate-request_key.keyremote firewall.35.176.61.220 1194 tcp-clientpersist-keyscript-security 3cipher AES-256-CBCauth-nocacheauth-user-pass firewall-auth.txtping 15ping-restart 45ping-timer-rempersist-tunverb 3log-append /var/log/openvpn.logThis is the main configuration file, Below I have explained what each line means:dev tun:pull:tls-client:dh dh2048.pem:ca ca.crt:cert certificate-response.crt:key certificate-request_key.key:remote firewall.35.176.61.220 1194 tcp-client:persist-keyscript-security 3cipher AES-256-CBCauth-nocacheauth-user-pass firewall-auth.txtping 15ping-restart 45ping-timer-rempersist-tunverb 3log-append /var/log/openvpn.logTestingTo test the VPN is quite simple, open 2x SSH windows to your Client Linux Server.In the first SSH Window run:openvpn /etc/openvpn/MyVpn.ovpnIn the second SSH Window run:tail -f /var/log/openvpn.logWatch the log closely, you will see errors in here which will help with troubleshooting any issues.TroubleshootingCompression: At the time of writing compression is not supported by Mikrotik, please make sure no LZO lines are present in the configuration.Certificates: Check that your certificate and key were imported properly and that your client is configured to trust the self-signed certificate or the CA you used.SecurityThere are some security improvements that could be made to this configuration, however this is to get you up and running. Limit the port access to a specific Source IP Address so that only you can connect Configure better passwords, the ones shown are examples only Consider using a separate bridge so that the VPN has its own filters and rules Change the security of the firewall-auth.txt and home.up files to 600Hopefully this will be helpful to someone out there.If you have any issues add a comment below and I will get back to you ASAP." }, { "title": "VMware Transparent Page Sharing TPS", "url": "/posts/vmware-transparent-page-sharing-tps/", "categories": "Virtualisation, VMware", "tags": "vmware, tps, page, sharing", "date": "2015-07-22 00:00:00 +0100", "snippet": "What is TPS?Transparent Page Sharing (TPS) is a host process that leverage’s Virtual Machine Monitor (VMM) component of the VMkernel to scan physical host memory to identify duplicate VM memory pag...", "content": "What is TPS?Transparent Page Sharing (TPS) is a host process that leverage’s Virtual Machine Monitor (VMM) component of the VMkernel to scan physical host memory to identify duplicate VM memory pages. The benefits of TPS are that it allows a host to reduce memory usage so you can allow more VMs onto a host, as memory is often one of the most constrained resources on a host. TPS is basically de-duplication for RAM and works at the 4KB block level.In some situations multiple virtual machines will have identical sets of memory content, TPS allows these sets to be De-duplicated thus using less overall memory on the host. As you can see from the image above, this displays a host with TPS Enabled and one with TPS Disabled. As you can see TPS uses much less memory where blocks are duplicated.What has changed?VMware recently acknowledged a vulnerability with their TPS feature that could in very specific scenarios allow VM’s to access memory pages of other VMs running on a host. It is important to note that this vulnerability is not easily exploitable and the risk is really low so most environments should not really be impacted by it. However VMware have been cautious and released patches to disable this feature by default in the following updates:ESXi 5.5, Patch ESXi550-201501001ESXi 5.1 Update 3ESXi 5.0 Patch ESXi500-201502001All versions of vSphere are vulnerable to the exploit but VMware is only patching the 5.x versions of vSphere as 4.x versions are no longer supported. These patches only disable TPS which is currently enable by default, they do not fix the vulnerability. VMware states in the KB article that Administrators may revert to the previous behaviour if they so wish.The benefits that TPS provides will vary in each environment depending on VM workloads so if you want to be PCI Compliant or are paranoid about security you will probably want to leave TPS Disabled. You can view the effectiveness of TPS in vCenter by looking at the shared and sharedcommon memory counters to see how much it is benefiting you." }, { "title": "VMware Large Snapshot Safe Removal", "url": "/posts/vmware-large-snapshot-safe-removal/", "categories": "Virtualisation, VMware", "tags": "snapshot, removal, safe, large", "date": "2015-07-21 00:00:00 +0100", "snippet": "One of the great virtualization and VMware features is the ability to take snapshots of a virtual machine. The snapshot feature allows an IT administrator to make a restore point of a virtual machi...", "content": "One of the great virtualization and VMware features is the ability to take snapshots of a virtual machine. The snapshot feature allows an IT administrator to make a restore point of a virtual machine, with the option to make it crash consistent. This feature is particularly useful when performing upgrades or testing, as if anything goes wrong during the process, you can quickly go back to a stable point in time (when the snapshot was taken).Snapshots are great for quick, short term restores, but can have devastating effects to an environment if kept long term. There are a number of reasons why snapshots should not be kept for long term or used as backups, one of the main issues is I/O performance 1008885. A list of best practices for snapshots can be found here: 1025279. This article shows 1 method to remove snapshots in a way that minimizes impact.Noticing High I/OAs mentioned earlier, one of the disasters that can occur when leaving a snapshot active for too long is that it very heavy I/O. After taking a look at the virtual machine, the “Revert to Current Snapshot” is available, so a snapshot exists.Before deleting the snapshot, check the size of the deltas to get an idea of how long the removal process will take. To do this select your virtual machine, right click the datastore and click browse.From the datastore select the folder matching your virtual machine name.As you can see from the delta (000001.vmdk) the snapshots are large. If this were a non-critical server or a small snapshot, I would just delete it, in this example the snapshot exists on a business critical server so I will take the below precautions.Why Take PrecautionsAlthough snapshot removal has been substantially improved in newer versions, it is still possible in 5.1 to stun the VM and in 5.5 to fail the removal and require consolidation. For a business critical application such as Microsoft SQL / Exchange that must remain active, the snapshot removal process cannot be cancelled once it has been initiated.One example that I experienced when I had first started working with VMware, I noticed one of our IT Staff had taken a snapshot on our Exchange server and had left it there for around 2 weeks. It was then decided we would remove the snapshot… Big Mistake! About 3 hours into the snapshot removal, Our phones were ringing off the hook, our Exchange server had became unresponsive and users could no longer access their mail. For the next 3 hours VMware was removing the snapshot and no one was able to use email.Removing a Large SnapshotAs crazy as this will seem, to remove the large snapshot we must first create a new snapshot… yes you did read that correctly. The reason for this is that it stops VMware writing to the old snapshot delta thus allowing VMware to write it back to the main VMDK without interruption. We then have a much smaller new snapshot that can be easily removed.Uncheck the “Snapshot the Virtual machine’s memory” option and name this: Safe Snapshot Removal. By unchecking the box shown below, this will assist in removing the “Safe Snapshot” once the other snapshot is removed, as we are not expecting to restore to this snapshot it is not required.We now have 2 snapshots, one from the upgrade (the old large snapshot) and our new Safe Removal Snapshot.Next, remove the large “Upgrade” snapshot. This will roll the snapshot back into the parent and will no longer cause any downtime. Note that this can potentially cause greater I/O penalties, so calculate the risks before proceeding with this method.Once the Upgrade snapshot has been deleted, I verify that the Safe Removal Snapshot is fairly small. If not, repeat the process. If it is, the Safe Removal Snapshot can be deleted." }, { "title": "Dell VMware 5.5 FCoE Errors", "url": "/posts/dell-vmware-5-5-fcoe-errors/", "categories": "", "tags": "", "date": "2015-07-20 00:00:00 +0100", "snippet": "Recently I have seen an issue after upgrading some of our Dell R6xx hosts to 5.5 U2, they started showing FCoE in the storage adapters and booting took a really long time.I looked into this and fou...", "content": "Recently I have seen an issue after upgrading some of our Dell R6xx hosts to 5.5 U2, they started showing FCoE in the storage adapters and booting took a really long time.I looked into this and found that the latest Dell ESXi image also includes Drivers and scripts that enable the FCoE interfaces on cards that support it.To see if you have this problem check the below steps:on boot press ALT + F12, this will show what ESXi is doing on boot, you will then begin to see the following errors multiple times:FIP VLAN ID unavail. Retry VLAN discoveryfcoe_ctlr_vlan_request() is doneto resolve this issue connect via SSH to the host and run the below commands:esxcli software vib remove -n scsi-bnx2fccd /etc/rc.local.d/rm 99bnx2fc.shesxcli fcoe nic disable -n=vmnic4esxcli fcoe nic disable -n=vmnic5This will remove the FCoE VIB, delete a script that runs to check for the VIB and then disable fcoe on the vmnics required.Hopefully this will help someone else as it took me a long time to find this solution and resolve the issue." }, { "title": "Add vCenter Logs to Syslog Server (GrayLog2)", "url": "/posts/add-vcenter-logs-to-syslog-server-graylog2/", "categories": "", "tags": "", "date": "2015-07-06 00:00:00 +0100", "snippet": "In this article I will be showing you how to add vCenter logs to a syslog server, I currently use GrayLog2 as its a great free syslog server and does everything that I require.First we want to inst...", "content": "In this article I will be showing you how to add vCenter logs to a syslog server, I currently use GrayLog2 as its a great free syslog server and does everything that I require.First we want to install NxLog on our vCenter Server, This will be our syslog client.To configure NxLog go to: c:\\Program Files (x86)\\nxlog\\conf and edit nxlog.conf with a word editor.Add the following configuration into the file:define ROOT C:\\Program Files (x86)\\nxlogModuledir %ROOT%\\modulesCacheDir %ROOT%\\dataPidfile %ROOT%\\data\\nxlog.pidSpoolDir %ROOT%\\dataLogFile %ROOT%\\data\\nxlog.log&lt;Extension gelf&gt; Module xm_gelf&lt;/Extension&gt;&lt;Input EventLog_In&gt; Module\tim_msvistalog# For windows 2003 and earlier use the following:# Module im_mseventlog&lt;/Input&gt;&lt;Input Vpxd_In&gt; Module\tim_file File\t\"C:\\\\ProgramData\\\\VMware\\\\VMware VirtualCenter\\\\Logs\\\\vpxd-*.log\" SavePos TRUE Exec\t$Hostname = 'myserver.local.com'; Exec\tif $raw_event =~ /\\s([a-z]+)\\s/ { \t\\\t\t\t$Severity = $1;\t\t\t\\\t\t} Exec\tif $raw_event =~ /\\'([a-zA-Z\\.]+)\\'/ { \t\\\t\t\t$Component = $1;\t\t\\\t\t} Exec\t$FileName = file_name(); Exec\t$SourceName = 'vCenter VPXD';&lt;/Input&gt;&lt;Output EventLog_Out&gt; Module om_udp Host 192.168.88.30 Port 60001 OutputType\tGELF&lt;/Output&gt;&lt;Output Vpxd_Out&gt; Module om_udp Host 192.168.88.30 Port 60002 OutputType\tGELF&lt;/Output&gt;&lt;Route 1&gt; Path EventLog_In =&gt; EventLog_Out&lt;/Route&gt;&lt;Route 2&gt; Path\tVpxd_In =&gt; Vpxd_Out&lt;/Route&gt;If you don’t want to log EventLogs to the Syslog Server just remove route 1 from the file or place # before each line.The Config Explained:The below code will load the module for Gelf communications, if you didn’t want to use gelf this could be changed to syslog.&lt;Extension gelf&gt; Module xm_gelf&lt;/Extension&gt;We then set our inputs, inputs provide information that we want to log to our syslog server, they are then translated by nxlog into a format that our syslog server will understand. As you can see from the code the EventLog is quite simple as there is a plugin specifically for this, but for vCenter Log Files we need to use the im_file module that will allow us to parse a text log file, we can then specify custom parameters to meet our requirements, i have included hostname, message, filename and sourcename but you could also split the $raw_event (your raw data) and log many more fields if required.&lt;Input EventLog_In&gt; Module\tim_msvistalog# For windows 2003 and earlier use the following:# Module im_mseventlog&lt;/Input&gt;&lt;Input Vpxd_In&gt; Module\tim_file File\t\"C:\\\\ProgramData\\\\VMware\\\\VMware VirtualCenter\\\\Logs\\\\vpxd-*.log\" SavePos TRUE Exec\t$Hostname = 'myserver.local.com'; Exec\tif $raw_event =~ /\\s([a-z]+)\\s/ { \t\\\t\t\t$Severity = $1;\t\t\t\\\t\t} Exec\tif $raw_event =~ /\\'([a-zA-Z\\.]+)\\'/ { \t\\\t\t\t$Component = $1;\t\t\\\t\t} Exec\t$FileName = file_name(); Exec\t$SourceName = 'vCenter VPXD';&lt;/Input&gt;We then set where each log should be sent, as you can see i have a different output for each log type, you don’t need to do this but it makes it easier to see what is logging where in GrayLog2. You should only need to change the host to your syslog server and the port to your port (default 514) I change mine as each type of log has its own port.&lt;Output EventLog_Out&gt; Module om_udp Host 192.168.88.30 Port 60001 OutputType\tGELF&lt;/Output&gt;&lt;Output Vpxd_Out&gt; Module om_udp Host 192.168.88.30 Port 60002 OutputType\tGELF&lt;/Output&gt;The Route tells NxLog which output to send inputs to. In my example I have 2 routes the 1st one tells the eventlogs_in to be sent to eventlogs_out and the 2nd does the same but for the VPXD logs, you could use one route if you were only having one output by using a comma to seperate vpxd_in and eventlog_in (e.g. EventLog_in,VPXD_In =&gt; MyCustom_Out)&lt;Output EventLog_Out&gt; Module om_udp Host 10.255.0.38 Port 60001 OutputType\tGELF&lt;/Output&gt;&lt;Output Vpxd_Out&gt; Module om_udp Host 10.255.0.38 Port 60002 OutputType\tGELF&lt;/Output&gt;Once this configuration has been completed we need to configure an output in GrayLog2 for each of our NxLog outputs, My example just shows how to do this for the VPXD log but it is the same for any log. Login to GrayLog2 Web Interface Go To System &gt; Inputs Select GELF UDP from the dropdown Click Launch New Input Tick Global Input or a specific GrayLog2 Server depending on your setup Enter a Title e.g. VPXD Logs Enter a port that you specified in the NxLog configuration (this must be unique) Click LaunchYou should now start to see the logs pouring in, vCenter does generate a LOT of logs so you may want to keep an eye on your syslog server as it could get overloaded with data.Hope this helped you, any issues or questions please let me know over on my DiscordSteve" }, { "title": "vCenter 6.0 VCSA Deployment", "url": "/posts/vcenter-6-0-vcsa-deployment/", "categories": "", "tags": "", "date": "2015-03-23 00:00:00 +0000", "snippet": "This article covers the deployment on the vCenter 6.0 VCSA, you will see that this process is radically different from previous processes. Download VCSA 6.0 from the VMware Website. Mount the ISO...", "content": "This article covers the deployment on the vCenter 6.0 VCSA, you will see that this process is radically different from previous processes. Download VCSA 6.0 from the VMware Website. Mount the ISO on your computer. go to the VCSA folder and install the VMware Client Intergration Plugin. launch vcsa-setup.html from the ISO. you will be prompted to Install or Upgrade, Choose Install Accept the terms and click next Enter the FQDN / IP and user details for an ESXi Host wait for validation then click yes on the certificate warning. Enter the appliance name and root password. Select the install type, there are now 2 choices, you can either deploy the appliance as one virtual machine or two, when deploying as two virtual machines one would be the platform services controller and the second vCenter Server. Select the SSO type. You have the choice of setting up a new SSO Domain or joining an existing one if you already have one in place. select the size of the appliance, this ranges from Tiny (10 hosts, 100 VMs) to Large (1,000 hosts and 10,000 VMs) Select the datastore you would like vCenter to reside on, tick “Enable Thin Disk Mode” if you want the Appliance to be Thin Provisioned. Select the database type, either Use an embedded database or Oracle database. Fill in the Network settings as required, choosing the correct network / IP addressing required for your network. vCenter will now begin to deploy.You should now have a fully working vCenter Server Appliance 6.0, this install is much improved from previous versions and makes it much easier for basic users to get the appliance deployed." }, { "title": "NUMA and vNUMA made simple!", "url": "/posts/numa-and-vnuma-made-simple/", "categories": "", "tags": "", "date": "2015-03-16 00:00:00 +0000", "snippet": "What is NUMA?Most modern CPU’s, Intel new Nehalem’s and AMD’s veteran Opteron are NUMA architectures. NUMA stands for Non-Uniform Memory Access. Each CPU get assigned its own “local” memory, CPU an...", "content": "What is NUMA?Most modern CPU’s, Intel new Nehalem’s and AMD’s veteran Opteron are NUMA architectures. NUMA stands for Non-Uniform Memory Access. Each CPU get assigned its own “local” memory, CPU and memory together form a NUMA node (as shown in the diagram below).Memory access time can differ due to the memory location relative to a processor, because a CPU can access it own memory faster than remote memory thus creating higher latency if remote memory is required.In short NUMA links multiple small high performing nodes together inside a single server.What is vNUMAvNUMA stands for Virtual Non-Uniform Memory Access, ESX has been NUMA-aware singe 2002, with VMware ESX 1.5 Introducing memory management features to improve locality on NUMA hardware. This works very well for placing VMs on local memory for resources being used by that VM, particularly for VMs that are smaller than the NUMA node. Large VMs, however, will start to see performance issues as they breach a single node, these VMs will require some additional help with resource scheduling.When enabled vNUMA exposes the VM OS to the physical NUMA. This provides performance improvements with the VM by allowing the OS and programs to best utilise the NUMA optimisations. VMs will then benefit from NUMA, even if the VM itself is larger than the physical NUMA nodes An administrator can enable / disable vNUMA on a VM using advanced vNUMA Controls If a VM has more than eight vCPUs, vNUMA is auto enabled If CPU Hot Add is enabled, vNUMA is Disabled The operating system must be NUMA AwareHow to determine the size of a NUMA nodeIn most cases the easiest way to determine a NUMA nodes boundaries is by dividing the amount of physical RAM by the number of logical processoes (cores), this is a very loose guideline. Further information on determining the specific NUMA node setup can be found here: VMware Microsoft LinuxWhat happens with vNUMA during vMotion?A VM will initially have its vNUMA topology built when it is powered on, each time it reboots this will be reapplied depending on the host it sits upon, In the case of a vMotion the vNUMA will stay the same until the VM is rebooted and it will re-evaluate its vNUMA topology. This is another great argument to make sure all hardware in a cluster is the same as it will avoid NUMA mismatched which could cause severe performance issues.Check if a VM is using resources from another NUMA nodeIf you start to see performance issues with VMs then I would recommend running this test to make sure that the VM isnt using resources from other Nodes. SSH to the ESXi host that the VM resides on Type esxtop and press enter Press “m” Press “f” Press “G” until a * shows next to NUMA STATS look at the column N%L this shows the numa usage if it is lower than 100 it is sharing resources from another numa node, see the example shown below:As you can see we have multiple VMs using different NUMA nodes, these VMs were showing slower performance than expected, once we sized them correctly they stopped sharing NUMA nodes and this resolved our issues.ConclusionNUMA plays a vital part in understanding performance within virtual environments, VMware ESXi 5.0 and above have extended capabilities for VMs with intelligent NUMA scheduling and improved VM-Level optimisation with vNUMA. It is important to understand how both NUMA and vNUMA work when sizing any virtual machines as this can have a detremental effect on your environments performance" }, { "title": "Offline Upgrade ESXi 5.5 to 6.0", "url": "/posts/offline-upgrade-esxi-5-5-to-6-0/", "categories": "", "tags": "", "date": "2015-03-15 00:00:00 +0000", "snippet": "This is a very short and sweet article documenting the offline upgrade process from 5.5 to 6.0 Download the ESXi 6.0 Offline Bundle from the VMware website. Upload the file to the local datastore...", "content": "This is a very short and sweet article documenting the offline upgrade process from 5.5 to 6.0 Download the ESXi 6.0 Offline Bundle from the VMware website. Upload the file to the local datastore of the ESXi Host. Enable SSH on the ESXi Host Connect to the ESXi Host and run the below command:esxcli storage filesystem list you should now have a list of your datastores, copy the mount point and add it to the below command:esxcli software vib install -d &lt;path_to_bundle.zip&gt; wait until the upgrade has completed then enter the follwoing to reboot the host:rebootThe host will reboot and you will now be able to connect with your client, you will be prompted to download the latest client and then you will be away!" }, { "title": "VMware Distributed Switches (dvSwitch)", "url": "/posts/vmware-distributed-switches-dvswitch/", "categories": "", "tags": "", "date": "2015-02-18 00:00:00 +0000", "snippet": "In this article I am going to take you through what a Distributed switch or dvSwitch is and how it is used, I will also talk about why backing them up is so important, then show you how to backup b...", "content": "In this article I am going to take you through what a Distributed switch or dvSwitch is and how it is used, I will also talk about why backing them up is so important, then show you how to backup by hand and with the help of some PowerShell scripts I have created for you.What is a distributed switch?A distributed switch (dvSwitch) is very similar to a standard vSwitch, the main difference is that the switch is managed by vCenter instead of the individual ESXi Hosts, the ESXi/ESX 4.x and ESXi 5.x hosts that belong to a dvSwitch do not need further configuration to be compliant.Distributed switches provide similar functionality to vSwitches. dvPortgroups is a set of dvPorts. The dvSwitch equivalent of portgroups is a set of ports in a vSwitch. Configuration is inherited from dvSwitch to dvPortgroup, just as from vSwitch to Portgroup.Virtual machines, Service Console interfaces (vswif), and VMKernel interfaces can be connected to dvPortgroups just as they could be connected to portgroups in vSwitches.This means that if you have 100 ESXi Hosts you only need to configure the PortGroups once and then add the ESXi Hosts to the dvSwitch rather than configuring the networking individually on each host.How Do You Use a dvSwitch?Below I have created an example of a two host cluster using a dvSwitch, the dvSwitchis first configured on vCenter and then hosts are added to the dvSwitch. Adding a host to a dvSwitch will then push the network configuration to the host.Once a host is added to the dvSwitchyou only need to assign the VMK’s and IP Addresses for it to begin functioning correctly. If you have migrated from a vSwitch you can migrate the VMK’s across saving additional configuration.As you can see from the image there are a few differences from a standard switch, you now have “dvUplinks” these are virtual vmnic’s for the physical network cards that are associated to the same service. e.g. management on host A could be vmnic0 where as on host B it could be vmnic8 without dvUplinks we would not be able to assign the same service to different vmnics on each host.After you get your head around dvUplinks everything else falls into place, the rest of the dvSwitch is the same as a standard switch (other than features)VMK’s are host specific due to the requirement for an IP Address, these cannot be allocated on a pool basis which is a shame. You have to manually add VMK’s by going to the host network configuration, selecting vSphere Distributed Switch and then select Manage Virtual adapters, this will then allow you to add / remove / migrate VMK’s to and from specific port groups.Pros &amp; ConsThere are only a few pros and cons to distributed switches, I have listed all the ones I am aware of below: (if you know any more please leave a comment!)Pros Private VLAN&#8217;s Netflow – ability for NetFlow collectors to collect data from the dvSwitch to determine what network device is talking and what protocols they are using SPAN and LLDP – allows for port mirroring and traffic analysis of network traffic using protocol analyzers Easy to add a new host Easy to add a new port group to all hosts Load Based Teaming, Load Balancing without the IP Hash worry. Cons If vCenter fails there is no way to manage your dvSwitch Requires an Enterprise Plus License Different FeaturesThese features are available with both types of virtual switches: Can forward L2 frames Can segment traffic into VLANs Can use and understand 802.1q VLAN encapsulation Can have more than one uplink (NIC Teaming) Can have traffic shaping for the outbound (TX) traffic These features are available only with a Distributed Switch: Can shape inbound (RX) traffic Has a central unified management interface through vCenter Server Supports Private VLANs (PVLANs) Provides potential customization of Data and Control Planes vSphere 5.x provides these improvements to Distributed Switch functionality: Increased visibility of inter-virtual machine traffic through Netflow Improved monitoring through port mirroring (dvMirror) Support for LLDP (Link Layer Discovery Protocol), a vendor-neutral protocol. The enhanced link aggregation feature provides choice in hashing algorithms and also increases the limit on number of link aggregation groups Additional port security is enabled through traffic filtering support. Improved single-root I/O virtualization (SR-IOV) support and 40GB NIC support. Automated dvSwitch Backup ScriptBelow is a script that I have written that allows automated backups of your dvSwitches.Get-dvSwitchBackupI have also got many other scripts available for use here on my GitHub.Final ThoughtsvSphere Distributed Virtual Switches are definitely the correct choice for companies that have the license, is it worth buying the licensing just for dvSwitch? I wouldn’t say so unless you require one of the specific features only dvSwitch supports. When you environment starts to grow I would say they are vital to saving time deploying hosts and re-configuring networks. I would recommend that you only use one or the other and don’t use a Hybrid configuration, in a Hybrid mode you are adding more configuration for your team and also added complexity that is not required. As long as you always have a backup of your dvSwitch you will not have any issues with loss of configuration.If you have anything to add please comment below, all feedback is appreciated." }, { "title": "Understanding Resource Pools in VMware", "url": "/posts/understanding-resource-pools-vmware/", "categories": "Virtualisation, VMware", "tags": "resource, pools, vmware, configuration", "date": "2015-02-09 00:00:00 +0000", "snippet": "It is my experience that resource pools are nearly a four letter word in the virtualization world. Typically I see a look of fear or confusion when I bring up the topic, or I see people using them ...", "content": "It is my experience that resource pools are nearly a four letter word in the virtualization world. Typically I see a look of fear or confusion when I bring up the topic, or I see people using them as folders. Even with some other great resources out there that discuss this topic, a lack of education remains on how resource pools work, and what they do. In this post, I’ll give you my spin on some of the ideals behind a resource pool, and then discuss ways to properly balance resource pools by hand and with the help of some PowerShell scripts I have created for you.What is a Resource Pool?A VMware resource pool is a way of guaranteeing or providing higher priority on a VM’s CPU and/or Memory, the priority set at the pool is then split between each of the individual VM’s in that pool equally.Who Needs Resource Pools?You can’t make a resource pool on a cluster unless you have DRS running. So, if your license level excludes DRS, you can’t use resource pools. If you are graced with the awesomeness of DRS, you may need a resource pool if you want to give different types of workloads different priorities for two scenarios: For when memory and CPU resources become constrained on the cluster. For when a workload needs a dedicated amount of resources at all times.Now, this isn’t to say that a resource pool is the only way to accomplish these things – you can use per VM shares and reservations. But, these values sometimes reset when a VM vMotions to another host, and frankly it’s a bit of an administrative nightmare to manage resource settings on the VMs individually.I personally like resource pools and use them often in a mixed workload environment. If you don’t have the luxury of a dedicated management cluster, resource pools are an easy way to dedicate resources to your vCenter, VUM, DB, and other “virtual infrastructure management” (VIM) VMs.Why People Fear Resource PoolsPeople fear resource pools because they are mysterious. Ok, maybe not that mysterious, but they are a bit awkward at first, one common misuse of resource pools that I see quite a lot is as folders, to sort VM’s rather than as a performance control. Also, they are easy to misunderstand, and thus misuse.Where Did I Get The Numbers?Let’s start with the resource pools. You’ll notice 3 points for each pool – the shares (high, normal or low), the amount of shares for RAM, and the amount of shares for CPU. Here is the math (supporting document): RAM is calculated like this: [Cluster RAM in MB] * [20 for High 10 for Normal 5 for Low] Our cluster has 100 GB of RAM (grey section) and so the math is: 102,400 MB of RAM * 20 = 2,048,000 for High and 102,400 MB of RAM * 5 = 512,000 for Low CPU is calculated like this: [Cluster CPU Cores] * [2,000 for High, 1,000 for Normal, 500 for Low] Our cluster has 100 CPU cores (grey section) and so the math is: 100 * 2,000 = 200,000 for High and 100 * 500 = 50,000 for LowBased on this, the Production resource pool has roughly 80% of the shares. However, when you divide those shares for the resource pool by the number of VMs that live in the resource pool, you start to see the problem. The bottom part of the graphic shows the entitlements at a Per VM level. Test has more than twice what Production has when looking at individual VMs.This script will calculate the Per VM resource allocation for you:Get-ResourcePoolSharesReportThe script has many options and will calculate what the share value should be by using the -RecommendedSharesMaintaining the BalanceSo now you are thinking oh no! My resource pools are totally wrong and this could be causing all my performance issues, so how do you keep the balance?The trick to keeping your resource pools balanced is to work it out backwards and never, ever use the default high, normal, and low shares values. Decide the weight of your per VM shares first. Let’s say that I want my Test VMs to receive half as much share weight as Production. Shares are an arbitrary value that just determine weight, they aren’t a magic number so you could create your own values. I prefer to stick with the VMware defaults, this way you know where you stand. So, let’s give Test VMs 500 shares per CPU and Per MB Ram each, and Production VMs 2000 shares per CPU and Per MB Ram. I would change the resource pools to this:Calculations:[Total amount of VM RAM in Pool] * [shares] = [Required RAM Shares][Total amount of VM vCPU in Pool] * [shares] = [Required CPU Shares]I would recommend having all virtual machines in a resource pool to avoid any issues with balancing your load. If you don’t want to do that then make sure you set your custom shares according to the VMware standards.Our resource pools:Production would get 90,000 * 20 = 180,000 shares of RAM, 90 * 2000 = 180,000 shares of CPUTest would get 10,000 * 5 = 50000 shares of RAM, 10 * 500 = 5000 Shares of CPUMuch easier, right? Note! If the number of VMs in the resource pool change, you’ll need to update the resource pool shares value to reflect the added VMs. Your options are to manually update the pool when the number of VMs inside change (no fun) or use PowerCLI!Using PowerCLI to Balance Resource Pool SharesNow let’s do some coding. This very basic script will connect to the vCenter server and cluster specified and look at the resource pools within. It then reports on the number of VMs contained within and offers to adjust the shares value based on an input you provide. It confirms before making any changes:Set-ResourcePoolSharesScript Usage:.\\Set-ResourcePoolShares.ps1 -vcenter \"vCenter.domain.com\" -cluster \"your-cluster\"I am also in the process of writing some more resource pool scripts that will email a report should you have any pools not at the correct resource levels.You can find all of my various scripts in my GitHub repositoryFinal ThoughtsI hope this has helped you understand when to use and cleared some confusion around resource pools, although it is a big chunk of information to swallow in one bite, and I’m sure there are a lot of other opinions floating around out there that won’t agree with mine. I’m OK with that. One thing that would be a great feature would be the ability to set per VM shares on the resource pool, and let the pool automatically adjust for membership values.Any comments and views are appreciated so please share." }, { "title": "Graylog2 Cisco ASA / Cisco Catalyst", "url": "/posts/graylog2-cisco-asa-cisco-catalyst/", "categories": "", "tags": "", "date": "2015-01-21 00:00:00 +0000", "snippet": "In order to correctly log Cisco device in Graylog2 setup the below configuration.This has now been added to the Graylog Marketplace https://marketplace.graylog.org/Cisco ASA Configuration:logging e...", "content": "In order to correctly log Cisco device in Graylog2 setup the below configuration.This has now been added to the Graylog Marketplace https://marketplace.graylog.org/Cisco ASA Configuration:logging enablelogging trap informationallogging asdm informationallogging device-id hostnamelogging host &lt;network&gt; &lt;ip-address&gt; &lt;udp-tcp&gt;/&lt;port&gt;Create a Raw/PlainText input with the settings you require.Then select action -&gt; Manage Extractors.Now select actions -&gt; Import Extractors, in the box add the below configuration. This will format the messages correctly with the IP Address of the firewall as the source.If you would like the Source to be the IP Address Change this line:\"regex_value\": \"&gt;(.+?)%\"To this:\"regex_value\": \"&amp;gt;: (.+?):\"Cisco-ASA-Extractor.json" }, { "title": "Graylog2 CentOS Installation", "url": "/posts/graylog2-centos-installation/", "categories": "", "tags": "", "date": "2015-01-20 00:00:00 +0000", "snippet": "I recently required a syslog server that was easy to use with a web interface to monitor some customers firewalls. I had been looking at Splunk but due to the price of this product it was not a via...", "content": "I recently required a syslog server that was easy to use with a web interface to monitor some customers firewalls. I had been looking at Splunk but due to the price of this product it was not a viable option for what I required.After a little searching I came across Graylog2 which is an open source alternative to Splunk and is totally free! You only need to pay if you would like support from them.So here is how I setup the server and got it working on my CentOS Server.Install &amp; Configure Elastic SearchDownload and install the Public Signing Key:rpm --import https://packages.elasticsearch.org/GPG-KEY-elasticsearchCreate the following file /etc/yum.repos.d/elasticsearch.repo[elasticsearch-1.4]name=Elasticsearch repository for 1.4.x packagesbaseurl=http://packages.elasticsearch.org/elasticsearch/1.4/centosgpgcheck=1gpgkey=http://packages.elasticsearch.org/GPG-KEY-elasticsearchenabled=1And your repository is ready for use. You can install it with :yum install elasticsearchConfigure Elasticsearch to automatically start during boot:chkconfig --add elasticsearchTo configure ElasticSearch for use with Graylog2 edit /etc/elasticsearch/elasticsearch.ymlcluster.name: graylog2node.data: truebootstrap.mlockall: trueES_HEAP_SIZE: 2048discovery.zen.ping.multicast.enabled: falsediscovery.zen.ping.unicast.hosts: [\"127.0.0.1\", \"IP_ADDR\"]Start the ElasticSearch service:service elasticsearch startInstall &amp; Graylog2 Server and Web ClientGet the latest RPM for Graylog2 here and run changing to the correct url:sudo rpm -Uvh https://packages.graylog2.org/repo/packages/graylog2-x.xx-repository-el6_latest.rpmInstall Graylo2-Server and Graylog2-Web:yum install graylog2-server graylog2-webEdit the file /etc/graylog2.conf and change only the below:password_secret =root_password_sha2 =elasticsearch_discovery_zen_ping_multicast_enabled = falseelasticsearch_discovery_zen_ping_unicast_hosts = IP_ADDR:9300Edit the file /etc/graylog2/web/graylog2-web-interface.conf and change only the below:graylog2-server.uris=\"\"application.secret=\"\"timezone=\"Europe/London\"Set Services to start at boot:chkconfig --add graylog2-serverchkconfig --add graylog2-webStart the services:service graylog2-server startservice graylog2-web startTroubleshootingLogs are stored in the following locations:/var/log/elasticsearch/*.log/var/log/graylog2-server/*.log/var/log/graylog2-web/*.logany errors in here should be quite easy to resolve. if you have any issues please let me know and I will assist where possible." }, { "title": "Teamspeak 3 with MySQL on CentOS 6.x (3.0.11.1 Onwards)", "url": "/posts/teamspeak-3-mysql-centos-6-x-3-0-11-1-onwards/", "categories": "", "tags": "", "date": "2014-10-31 00:00:00 +0000", "snippet": "By default Teamspeak 3 uses a SQLite database, most people tend to use this however for those of us that prefer MySQL there is a way to change it.Follow this small tutorial to create a Teamspeak 3 ...", "content": "By default Teamspeak 3 uses a SQLite database, most people tend to use this however for those of us that prefer MySQL there is a way to change it.Follow this small tutorial to create a Teamspeak 3 Server on CentOS 6.x using a MySQL Database!First we need to install or upgrade MySQL:Install:rpm -Uvh http://download.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpmrpm -Uvh http://rpms.famillecollet.com/enterprise/remi-release-6.rpmyum --enablerepo=remi,remi-test list mysql mysql-serveryum --enablerepo=remi,remi-test install mysql mysql-serverUpgrade:rpm -Uvh http://download.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpmrpm -Uvh http://rpms.famillecollet.com/enterprise/remi-release-6.rpmyum --enablerepo=remi,remi-test list mysql mysql-serveryum --enablerepo=remi,remi-test update mysql mysql-servermysql_upgrade -u root -pNow we need to create a new user on our server, this user will be used for the installation and running of teamspeak. For security reasons this user will not have sudo etc.useradd ts3userpasswd ts3userWe are now in a position where we can configure MySQL with a Database and User for Teamspeakservice mysqld startchkconfig mysqld onmysql_secure_installationmysql -uroot -pcreate database ts3db;grant all on ts3db.* to 'ts3user'@'127.0.0.1' identified by 'ts3password';flush privileges;Once the MySQL Database is set-up along with a user we will create an init script for Teamspeak so that we can start the server as a service, create the script: vi /etc/init.d/teamspeak#!/bin/bash# /etc/init.d/teamspeak### BEGIN INIT INFO# Provides: teamspeak# Required-Start: $local_fs $remote_fs# Required-Stop: $local_fs $remote_fs# Should-Start: $network# Should-Stop: $network# Default-Start: 2 3 4 5# Default-Stop: 0 1 6# Short-Description: Teamspeak 3 Server# chkconfig: 2345 94 05# Description: Starts the Teamspeak 3 server### END INIT INFO# Source function library.. /etc/rc.d/init.d/functions# SettingsSERVICENAME='Teamspeak 3 Servers'SPATH='/home/ts3user/teamspeak3-server/'SERVICE='/home/ts3user/teamspeak3-server/ts3server_startscript.sh'OPTIONS='inifile=ts3server.ini'USERNAME='ts3user'ME=`whoami`as_user() {if [ $ME == $USERNAME ] ; thenbash -c \"$1\"elsesu - $USERNAME -c \"$1\"fi}mc_start() {echo \"Starting $SERVICENAME...\"cd $SPATHas_user \"cd $SPATH &amp;&amp; $SERVICE start ${OPTIONS}\"}mc_stop() {echo \"Stopping $SERVICENAME\"as_user \"$SERVICE stop\"}mc_status(){ # run checks to determine if the service is running or use generic status status -p /home/ts3user/teamspeak3-server/ts3server.pid $SERVICENAME}mc_status_q(){ rh_status &amp;gt;/dev/null 2&amp;gt;&amp;1}# Start and stop the service herecase \"$1\" instart)mc_start;;stop)mc_stop;;restart)mc_stopmc_start;;status)mc_status;;*)echo \"Usage: /etc/init.d/teamspeak {start|stop|restart|status}\"exit 1;;esacexit 0Now we will login with our new ts3user created at the beginning of this tutorial, download Teamspeak Server 3 64-bit for Linux and extract in your home directory, get the latest version here: http://www.teamspeak.com/?page=downloadsAs TS3User:cd /home/ts3user/wget http://dl.4players.de/ts/releases/3.0.11.1/teamspeak3-server_linux-amd64-3.0.11.1.tar.gztar -xvf teamspeak3-server_linux-amd64-3.0.11.1.tar.gzmv teamspeak3-server_linux-amd64 teamspeak3-servercd teamspeak3-serverAs Root:cp redist/libmariadb.so.2 /lib64/libmariadb.so.2ldd libts3db_mariadb.soAs Ts3user:ts3server.ini stores the configuration for the teamspeak server, we need to edit this to work with MySQL instead of SQLite: vi ts3server.inimachine_id=1default_voice_port=9987voice_ip=0.0.0.0licensepath=filetransfer_port=30033filetransfer_ip=0.0.0.0query_port=10011query_ip=0.0.0.0dbplugin=ts3db_mariadbdbpluginparameter=ts3db_mariadb.inidbsqlpath=sql/dbsqlcreatepath=create_mariadb/logpath=logslogquerycommands=0dbclientkeepdays=30logappend=0query_skipbruteforcecheck=0We must now create a file called ts3db_mariadb.ini, this will hold your database login details:[config]host=127.0.0.1port=3306username=ts3userpassword=ts3passworddatabase=ts3dbsocket=Start Teamspeak:./ts3server_startscript.sh startYou should now see that teamspeak 3 is installed and you will see a message on screen with a privilege token and your server query admin account details, it is important to copy these as you will need them to administer your server.Stop the server:./ts3server_startscript.sh stopCheck the logs in the log directory. if everything is OK, log back in as root, enable the service and start it:chmod =x /etc/init.d/teamspeakchkconfig --add teamspeakchkconfig teamspeak onservice teamspeak start" }, { "title": "vCloud Director and vCenter Proxy Service Failure", "url": "/posts/vcloud-director-vcenter-proxy-service-failure/", "categories": "", "tags": "", "date": "2014-07-11 00:00:00 +0100", "snippet": "Over the past couple of weeks I have spent some time working with VMware vCloud Director 5.1. I will also be producing multiple other guides for vCloud Director as I use it more over the coming mon...", "content": "Over the past couple of weeks I have spent some time working with VMware vCloud Director 5.1. I will also be producing multiple other guides for vCloud Director as I use it more over the coming months.One issue that we have hit a few times was the vCD cell stopped working properly (Multi-cell environment). I could log into the vCD provider and organization portals but the deployment of vApps would run for an abnormally long time and then fail after 20 minutes.The first thing I tried to do to resolve this issue was reconnect vCenter to vCloud, in the past this has been the solution to this type of problem, however I noticed two problems:Problem #1: Performing a Reconnect on the vCenter Server object resulted in Error performing operation and Unable to find the cell running this listener.Problem #2: None of the cells have a vCenter proxy service running on the cell server.I then stumbled upon some SQL Queries that I wasn’t too sure about, I passed these over to VMware and they confirmed this is the correct action to take and it is none destructive. The below steps take you through resolving this issue: Stop all your Cellsservice vmware-vcd stop Backup the entire vCloud SQL Database. This is just a precaution. run the below query in SQL Management StudioUSE [vcloud]GO– shutdown all cells before executingdelete from QRTZ_SCHEDULER_STATEdelete from QRTZ_FIRED_TRIGGERSdelete from QRTZ_PAUSED_TRIGGER_GRPSdelete from QRTZ_CALENDARSdelete from QRTZ_TRIGGER_LISTENERSdelete from QRTZ_BLOB_TRIGGERSdelete from QRTZ_CRON_TRIGGERSdelete from QRTZ_SIMPLE_TRIGGERSdelete from QRTZ_TRIGGERSdelete from QRTZ_JOB_LISTENERSdelete from QRTZ_JOB_DETAILSgo Start one of your Cells and verify that the issue is resolvedservice vmware-vcd start Start the remaining cells.The script should run successfully wiping out all rows in each of the named tables.I was now able to restart the vCD cell and my problems were gone. Everything was working again. All errors have vanished.These [vCenter Proxy Service] issues are usually caused by a disconnect from the database, causing the tables to become stale. vCD constantly needs the ability to write to the database and when it cannot, the cell ends up in a state that is similar to the one that you have seen.The qrtz tables contain information that controls the coordinator service, and lets it know when the coordinator to be dropped and restarted, for cell to cell fail over to another cell in multi cell environment.When the tables are purged it forces the cell on start up to recheck its status and start the coordinator service. In your situation the cell, due to corrupt records in the table was not allowing this to happen. So by clearing them forced the cell to recheck and to restart the coordinator." }, { "title": "How to setup an NFS mount on CentOS 6", "url": "/posts/setup-nfs-mount-centos-6/", "categories": "", "tags": "", "date": "2014-06-24 00:00:00 +0100", "snippet": "About NFS (Network File System) MountsNFS mounts allow sharing a directory between several servers. This has the advantage of saving disk space, as the directory is only kept on one server, and oth...", "content": "About NFS (Network File System) MountsNFS mounts allow sharing a directory between several servers. This has the advantage of saving disk space, as the directory is only kept on one server, and others can connect to it over the network. When setting up mounts, NFS is most effective for permanent fixtures that should always be accessible.SetupAn NFS mount is set up between at least two servers. The machine hosting the shared directory is called the server, while the ones that connect to it are clients.This tutorial will take you through setting up the NFS server.The system should be setup as rootsudo su -Setting up the NFS Server1. Install the required software and start servicesFirst we use yum to install the required nfs programs.yum install nfs-utils nfs-utils-libNow we will run the startup scripts for the NFS Server:chkconfig nfs onchkconfig portmap onservice portmap startservice nfs start2. Export the shared directoryThe next step is to make the directory we want to share available to our clients. The chosen directory should then be added to /etc/exports, which specifies both the directory to be shared and the share permissions.If we wanted to share the directory /home we would do the following:We would edit Exports:vi /etc/exportsAdd the following lines to the file, sharing the directory with the client:/home 12.33.44.555(rw,sync,no_root_squash,no_subtree_check)These settings achieve the following: rw: This option allows the client to both read and write within the shared directory sync: Sync confirms requests to the shared directory only once the changes have been committed. no_subtree_check: This option prevents the subtree checking. When a shared directory is the subdirectory of a larger filesystem, nfs performs scans of every directory above it, in order to verify its permissions and details. Disabling the subtree check may increase the reliability of NFS, but reduce security. no_root_squash: This phrase allows root to connect to the designated directoryOnce completed save the file and exit it, then run the following command to export the settings:exportfs -aYou now have a fully functioning NFS server. If there is anything you think I have missed from this tutorial please comment below." }, { "title": "Email Report Virtual Machines with Snapshots", "url": "/posts/email-report-virtual-machines-snapshots/", "categories": "", "tags": "", "date": "2014-06-18 00:00:00 +0100", "snippet": "I have recently had an issue with people leaving snapshots on VM’s for too long causing large snapshots and poor performance on Virtual Machines.I decided that I needed a way of reporting on which ...", "content": "I have recently had an issue with people leaving snapshots on VM’s for too long causing large snapshots and poor performance on Virtual Machines.I decided that I needed a way of reporting on which virtual machines had snapshots present, when they were created and how big they are.The attached PowerCLI script does just that! It will logon to vCenter check all of the virtual machines for snapshots and then send an email report to the email address specified. This script support’s the get-help command and tab completion of parameters.Get-VMSnapshotReportTo use this script simply use the following commands:./Get-VMSnapShotReport.ps1 -vCenter \"my.vcenter.com\" -user username -password YourPassword -OlderThan 48 -EmailTo \"user@domain.com\" -EmailFrom \"user@domain.com\" -EmailSubject \"My Snapshot Report\" -EmailServer \"mail.domain.com\"The only thing that I ask is that when using this script you keep my name and website present in the notes, if there are any improvements you think I could make please let me know." }, { "title": "CentOS Server Hardening Tips", "url": "/posts/centos-server-hardening-tips/", "categories": "Linux, Security", "tags": "hardening, security, vuln", "date": "2014-05-29 00:00:00 +0100", "snippet": "This article provides various hardening tips for your Linux server.1. Minimise Packages to Minimise VulnerabilityDo you really want all sorts of services installed?. It’s recommended to avoid insta...", "content": "This article provides various hardening tips for your Linux server.1. Minimise Packages to Minimise VulnerabilityDo you really want all sorts of services installed?. It’s recommended to avoid installing packages that are not required to avoid vulnerabilities. This may minimise risks that compromise other services on your server. Find and remove or disable unwanted services from the server to minimize vulnerability. Use the chkconfig command to find out services which are running on runlevel 3./sbin/chkconfig --list |grep '3:on'Once you’ve found any unwanted services that are running, disable them using the following command:chkconfig serviceName offUse the RPM package managers such as yum or apt-get tools to list all installed packages on a system and remove them using the following command:yum -y remove package-namesudo apt-get remove package-name2. Check Listening Network PortsWith the help of the netstat networking command you can view all open ports and associated programs. As I said above use the chkconfig command to disable all unwanted network services on the system.netstat -tulpn3. Use Secure Shell(SSH)Telnet and rlogin protocols use plain text, a non encrypted format which is prone to security breaches. SSH is a secure protocol that uses encryption technologies during communication.Never login directly as root unless necessary. Use sudo to execute commands. sudo users are specified in the /etc/sudoers file which can be edited with the visudo utility which opens in the VI editor.It’s also recommended to change default SSH (22) port number with a different unassigned port number. Open the main SSH configuration file and change the following parameters to restrict users to access. (vi /etc/ssh/sshd_config)Disable root LoginThis will disable users from logging in over SSH with the Root account, Obviously if you don’t have console access don’t change this setting.PermitRootLogin nOnly Allow Specific UsersChange this to only allow users that you want to be using SSH Access (one line per user)AllowUsers usernameAllowUsers usernameUse SSH Protocol 2 VersionThis should be used in all cases!Protocol 24. Keep Your System updatedAlways keep your system updated with latest releases, patches, security fixes and kernel updates when available.yum updatesyum check-update5. Lockdown CronjobsCron has a built in feature, where it allows you to specify who can and can not run cron jobs. This is controlled by the use of the following files called /etc/cron.allow and /etc/cron.deny. To stop a user using cron, simply add their user names to cron.deny and to allow a user to run cron add them to cron.allow. If you would like to disable all users from using cron, add the ALL line to cron.deny file.echo username &gt;&gt;/etc/cron.allowecho ALL &gt;&gt;/etc/cron.deny6. Remove KDE/GNOME DesktopsThere is no need to run X Window desktops like KDE or GNOME on your LAMP server. You can remove or disable them to increase security and performance. To disable simple open the file /etc/inittab and set the run level to 3. If you wish to remove it completely from the system use the below command.yum groupremove \"X Window System\"7. Turn Off IPv6If you’re not using a IPv6 protocol, then you should disable it. Most of the applications or policies don’t required IPv6 protocol and currently it isn’t required on the server. Go to the network configuration file and add followings lines to disable it.(vi /etc/sysconfig/network)NETWORKING_IPV6=noIPV6INIT=no8. Restrict Users from using Old PasswordsThis is very useful if you want to stop users using the same old passwords. The old password file is located at /etc/security/opasswd. This can be achieved by using the PAM module.vi /etc/pam.d/system-authAdd the following line to the auth sectionauth sufficient pam_unix.so likeauth nullokAdd the following line to password section to disallow a user from re-using the last 5 password.password sufficient pam_unix.so nullok use_authtok md5 shadow remember=5Only the last 5 passwords are remembered by the server, if you try to use any of the last 5 old passwords you will receive an error.9. Enforce Stronger PasswordsA number of users use soft or weak passwords and their password might be hacked with a dictionary based or brute-force attack. The pam_cracklib module is available in the PAM (Pluggable Authentication Modules) module stack which will force a user to set a strong passwords. Add the following to this file (vi /etc/pam.d/system-auth)/lib/security/$ISA/pam_cracklib.so retry=3 minlen=8 lcredit=-1 ucredit=-2 dcredit=-2 ocredit=-110. Enable Iptables (Firewall)It’s highly recommended to enable the Linux firewall to secure access to your servers. Apply rules in iptables to filter incoming, outgoing and forwarding packets. We can specify the source and destination address to allow and deny specific udp/tcp port number.The following are rules i would highly recommend adding to yours (vi /etc/sysconfig/iptables) the first block of rules drop connections that are not recognised, the second allows local network access to certain ports e.g. SSH port 22.Generated by iptables-save v1.4.7 on Mon Mar 31 14:07:57 2014*filter:INPUT DROP [0:0]:FORWARD DROP [0:0]:OUTPUT ACCEPT [7:747]-A INPUT -i lo -j ACCEPT-A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT-A INPUT -p tcp -m tcp --tcp-flags FIN,SYN,RST,PSH,ACK,URG NONE -j DROP-A INPUT -p tcp -m tcp ! --tcp-flags FIN,SYN,RST,ACK SYN -m state --state NEW -j DROP-A INPUT -p tcp -m tcp --tcp-flags FIN,SYN,RST,PSH,ACK,URG FIN,SYN,RST,PSH,ACK,URG -j DROP-A INPUT -f -j DROP-A INPUT -s 192.168.0.0/24 -p icmp --icmp-type any -j ACCEPT-A INPUT -s 192.168.0.0/24 -i eth0 -p tcp -m tcp --dport 22 -j ACCEPT11. Keep /boot as read-onlyThe Linux kernel and its related files are in /boot directory which is by default read-write. Changing it to read-only reduces the risk of unauthorised modification of critical boot files. To do this, vi /etc/fstab and add the following to the bottom.LABEL=/boot /boot ext2 defaults,ro 1 2 You need to reset the change to read-write if you need to upgrade the kernel in future.12. Setup NTPHaving an accurate system clock is important for reviewing log files and determining when an event occurred. Often system clocks can become out of sync or be reset to an older date and this can cause havoc with tracking of errors. Consider creating a Cron job rather than running ntpd to update the time daily or hourly with a common source for all servers.13. Install DDoS DeflateDDoS Deflate is a lightweight bash shell script designed to assist in the process of blocking a denial of service attack. It uses the command below to create a list of IP addresses connected to the server, along with their total number of connections.netstat -ntu | awk ‘{print $5}’ | cut -d: -f1 | sort | uniq -c | sort -nFeatures of (D)DoS DeflateConfiguration file: /usr/local/ddos/ddos.confTo whitelist IP address, use /usr/local/ddos/ignore.ip.list.It is having a feature to unblock IP addresses automatically after a preconfigured time limit.You can execute the script at a chosen frequency.You will get email alerts when IP addresses are blocked.Installationwget http://www.inetbase.com/scripts/ddos/install.shchmod 0700 install.sh./install.shUninstallationwget http://www.inetbase.com/scripts/ddos/uninstall.ddoschmod 0700 uninstall.ddos./uninstall.ddosScript to check No of connected ip’s.sh /usr/local/ddos/ddos.shRestart DDos Deflatesh /usr/local/ddos/ddos.sh -c14. Install DenyHostsDenyHosts is a security tool written in python that monitors server access logs to prevent brute force attacks on a virtual server. The program works by banning IP addresses that exceed a certain number of failed login attempts.This list is still not completed, I am constantly adding new security tips to it, should you have any you think I should include please comment below and I will add them." }, { "title": "Teamspeak 3 with MySQL on CentOS 6.x (before 3.0.11.1)", "url": "/posts/teamspeak-3-mysql-centos-6-x/", "categories": "Linux, Teamspeak", "tags": "teamspeak, server, mysql", "date": "2014-05-06 00:00:00 +0100", "snippet": " As of Version 3.0.11.1 this tutorial is no longer applicable. I will soon re-write this to accommodate the latest version.By default Teamspeak 3 uses a SQLite database, most people tend to use th...", "content": " As of Version 3.0.11.1 this tutorial is no longer applicable. I will soon re-write this to accommodate the latest version.By default Teamspeak 3 uses a SQLite database, most people tend to use this however for those of us that prefer MySQL there is a way to change it.Follow this small tutorial to create a Teamspeak 3 Server on CentOS 6.x using a MySQL Database!VVIDEO AVAILABLE HERE First we need to have mysql installed:yum install mysql-server mysql-commonTo use a MySQL database, you need to install additional libraries not available from the default repositories. Download MySQL-shared-compat-6.0.11-0.rhel5.x86_64.rpm (This is 64 bit version. If you are on a 32 bit system, you’ll need to find it somewhere) and installyum localinstall MySQL-shared-compat-6.0.11-0.rhel5.x86_64.rpmNow we need to create a new user on our server, this will be used for the installation and running of teamspeak. For security reasons this user will not have sudo etc.useradd ts3userpasswd ts3userWe are now in a position where we can configure MySQL with a Database and User for Teamspeakservice mysqld startchkconfig mysqld onmysql -uroot -pUPDATE mysql.user SET Password = PASSWORD('password') WHERE User = 'root';create database ts3db;grant all on ts3db.* to 'ts3user'@'localhost' identified by 'ts3password';flush privileges;Once the MySQL Database is setup along with a user we will create an init script for Teamspeak so that we can start the server as a service, create the script: vi /etc/init.d/teamspeak#!/bin/bash# /etc/init.d/teamspeak### BEGIN INIT INFO# Provides: teamspeak# Required-Start: $local_fs $remote_fs# Required-Stop: $local_fs $remote_fs# Should-Start: $network# Should-Stop: $network# Default-Start: 2 3 4 5# Default-Stop: 0 1 6# Short-Description: Teamspeak 3 Server# chkconfig: 2345 94 05# Description: Starts the Teamspeak 3 server### END INIT INFO# Source function library.. /etc/rc.d/init.d/functions# SettingsSERVICENAME='Teamspeak 3 Servers'SPATH='/home/ts3user/teamspeak3-server/'SERVICE='/home/ts3user/teamspeak3-server/ts3server_startscript.sh'OPTIONS='inifile=ts3server.ini'USERNAME='ts3user'ME=`whoami`as_user() {if [ $ME == $USERNAME ] ; thenbash -c \"$1\"elsesu - $USERNAME -c \"$1\"fi}mc_start() {echo \"Starting $SERVICENAME...\"cd $SPATHas_user \"cd $SPATH &amp;&amp; $SERVICE start ${OPTIONS}\"}mc_stop() {echo \"Stopping $SERVICENAME\"as_user \"$SERVICE stop\"}mc_status(){ # run checks to determine if the service is running or use generic status status -p /home/ts3user/teamspeak3-server/ts3server.pid $SERVICENAME}mc_status_q(){ rh_status &amp;gt;/dev/null 2&amp;gt;&amp;1}# Start and stop the service herecase \"$1\" instart)mc_start;;stop)mc_stop;;restart)mc_stopmc_start;;status)mc_status;;*)echo \"Usage: /etc/init.d/teamspeak {start|stop|restart|status}\"exit 1;;esacexit 0Now we will login with our new ts3user created at the beginning of this tutorial, download Teamspeak Server 3 64-bit for Linux and extract in your home directory, get the latest version herewget http://dl.4players.de/ts/releases/3.0.10.3/teamspeak3-server_linux-amd64-3.0.10.3.tar.gztar -xf teamspeak3-server_linux-amd64-3.0.10.3.tar.gzmv teamspeak3-server_linux-amd64-3.0.10.3 teamspeak3-servercd teamspeak3-serverldd libts3db_mysql.sots3server.ini stores the configuration for the teamspeak server, we need to edit this to work with MySQL instead of SQLite: vi ts3server.inimachine_id=default_voice_port=9987voice_ip=0.0.0.0licensepath=filetransfer_port=30033filetransfer_ip=0.0.0.0query_port=10011query_ip=0.0.0.0query_ip_whitelist=query_ip_whitelist.txtquery_ip_blacklist=query_ip_blacklist.txtdbplugin=ts3db_mysqldbpluginparameter=ts3db_mysql.inidbsqlpath=sql/dbsqlcreatepath=create_mysql/dbconnections=10logpath=logslogquerycommands=0dbclientkeepdays=30logappend=0We must now create a file called ts3db_mysql.ini, this will hold your database login details:[config]host=localhostport=3306username=ts3userpassword=ts3passworddatabase=ts3dbsocket=Start Teamspeak with a few additional parameters, one tells it where the configuration file is and the other tells it to change the serveradmin password:./ts3server_startscript.sh start inifile=ts3server.ini serveradmin_password=passwordhereYou should now see that teamspeak 3 is installed and you will see a message on screen with a privilege token and your server query admin account details, it is important to copy these as you will need them to administer your server.Stop the server:./ts3server_startscript.sh stopCheck the logs in the log directory. if everything is OK, log back in as root, enable the service and start it:chmod =x /etc/init.d/teamspeakchkconfig --add teamspeakchkconfig teamspeak onservice teamspeak start" }, { "title": "Migrate TeamSpeak 3 from SQLite to MySQL", "url": "/posts/migrate-teamspeak-3-sqlite-mysql/", "categories": "", "tags": "", "date": "2014-04-28 00:00:00 +0100", "snippet": "One of the things I wanted to do was migrate my teamspeak server from SQLite to MySQL so I created the below which makes the migration easy. Stop the TeamSpeak Server2.Run the following command to...", "content": "One of the things I wanted to do was migrate my teamspeak server from SQLite to MySQL so I created the below which makes the migration easy. Stop the TeamSpeak Server2.Run the following command to export configuration:sqlite3 ts3server.sqlitedb .dump | grep -v \"sqlite_sequence\" |grep -v \"COMMIT;\" | grep -v \"BEGIN TRANSACTION;\" | grep -v \"PRAGMA \" | sed 's/autoincrement/auto_increment/Ig' | sed 's/\"/`/Ig' &amp;gt; ts3_export.sqlThis will export the SQLite configuration in MySQL Format to a file called ts3_export.sql Import the configuration to MySQL:mysql -u username -p database_name &amp;lt; ts3_export.sqlThis will import the ts3_export.sql file into a database of your choosing. edit ts3server.ini to the following:machine_id=default_voice_port=9987voice_ip=0.0.0.0licensepath=filetransfer_port=30033filetransfer_ip=0.0.0.0query_port=10011query_ip=0.0.0.0query_ip_whitelist=query_ip_whitelist.txtquery_ip_blacklist=query_ip_blacklist.txtdbplugin=ts3db_mysqldbpluginparameter=ts3db_mysql.inidbsqlpath=sql/dbsqlcreatepath=create_mysql/dbconnections=10logpath=logslogquerycommands=0dbclientkeepdays=30logappend=0 Create a file called ts3db_mysql.ini which contains:[config]host=localhostport=3306username=ts3userpassword=ts3passworddatabase=ts3dbsocket= Start TeamSpeak and it should now be working on MySQL" }, { "title": "Cisco ASDM Java Runtime Device Conenction", "url": "/posts/cisco-asdm-java-runtime-device-conenction/", "categories": "", "tags": "", "date": "2014-02-21 00:00:00 +0000", "snippet": "I have recently had a lot of issues with Cisco ASDM on new installs of Windows 7 and upwards.After lots of research and a bit of digging I have found a way to resolve this issue. Install Java ...", "content": "I have recently had a lot of issues with Cisco ASDM on new installs of Windows 7 and upwards.After lots of research and a bit of digging I have found a way to resolve this issue. Install Java Runtime Environment 6 Update 7 Install ASDM onto the computer Edit the properties it the ASDM Shortcut. change the beginning of the target from: C:\\windows\\system\\java.exeTO:C:\\Program Files (x86)\\Java\\jre1.6.0_07\\bin\\javaw.exeThis should resolve the issue with version 7.1(1) not connecting to devices." }, { "title": "Setup rSnapshot backups on CentOS", "url": "/posts/setup-rsnapshot-backups-centos/", "categories": "Linux, Backups", "tags": "rsnapshot, rsync, centos, backups, configuration", "date": "2014-01-21 00:00:00 +0000", "snippet": "In this article I will be talking you through how to use rSnapshot and rSync to backup your server with an email alert when the backup has been completed and what has been backed up. You must firs...", "content": "In this article I will be talking you through how to use rSnapshot and rSync to backup your server with an email alert when the backup has been completed and what has been backed up. You must first have rSync and rSnapshot installed:yum -y install rsync rsnapshot Once installed you will then need to create the correct configuration files for your server. Here is an example of what I use (save as backup_config.conf):config_version 1.2snapshot_root /data/backups/snapshots/server/cmd_cp /bin/cpcmd_rm /bin/rmcmd_rsync /usr/bin/rsynccmd_ssh /usr/bin/sshcmd_logger /usr/bin/logger#cmd_du /usr/bin/duinterval daily 7interval weekly 4interval monthly 3verbose 3loglevel 4logfile /var/log/rsnapshot/backups.logexclude_file /etc/rsnapshot/backup_config.excludersync_long_args --delete --numeric-ids --delete-excluded --statslockfile /var/run/rsnapshot.pidbackup root@spitfiredev.com:/ mp-vps01rsync_long_args --stats --delete --numeric-ids --delete-excludedretain daily 14 It is important to use tabs between each argument otherwise you will receive errors. Now we need to create an exclude file, this will exclude any directories that you don’t want to backup. This needs to be placed in the location specified on you conf file above(save as backup_config.exclude):+ /var+ /var/www- /var/*+ /home- /* When adding a sub directory e.g. + /var/www you must first include + /var and then your sub directory, you can then exclude the existing directories as I have in my example. (you don’t need to use tab in this file) Create the directory for backups and logs to be stored:mkdir -p /data/backups/snapshots/server/mkdir -p /var/log/rsnapshot/ Test the backup by running:/usr/bin/rsnapshot -c /etc/rsnapshot/mp-vps01.confIf successful move on if not troubleshoot, ask below if you get stuck. Schedule the backup with crontab:crontab -e0 0 * * * /usr/bin/rsnapshot -c /etc/rsnapshot/mp-vps01.conf dailyIf you would like email alerts use the following:0 0 * * * /usr/bin/rsnapshot -c /etc/rsnapshot/mp-vps01.conf daily | mail -s \"My Backup Job\" your@email.co.uk If the backup fails the email will be empty, I still haven’t figured out how to resolve this to email the errors, If you know please let me know in the comments!" }, { "title": "CentOS Use Public/Private Keys for Authentication", "url": "/posts/centos-public-private-key-auth/", "categories": "Linux", "tags": "authentication, public key, private key, ssh", "date": "2014-01-20 00:00:00 +0000", "snippet": "The following Tutorial walks you through how to setup authentication using a key pair to negotiate the connection, stopping the requirement for passwords.1.First, create a public/private key pair o...", "content": "The following Tutorial walks you through how to setup authentication using a key pair to negotiate the connection, stopping the requirement for passwords.1.First, create a public/private key pair on the client that you will use to connect to the server (you will need to do this from each client machine from which you connect):ssh-keygen -t rsaLeave the passphrase blank if you dont want to receive a prompt for this.This will create two files in your ~/.ssh directory called: id_rsa and id_rsa.pub The first: id_rsa is your private key and the second: id_rsa.pub is your public key. Now set permissions on your private key:chmod 700 ~/.sshchmod 600 ~/.ssh/id_rsa Copy the public key (id_rsa.pub) to the server and install it to the authorized_keys list:cat id_rsa.pub &amp;gt;&amp;gt; ~/.ssh/authorized_keys Once you’ve imported the public key, you can delete it from the server. Set file permissions on the server:chmod 700 ~/.sshchmod 600 ~/.ssh/authorized_keysThe above permissions are required if StrictModes is set to yes in /etc/ssh/sshd_config (the default). Ensure the correct SELinux contexts are set:restorecon -Rv ~/.sshNow when you login to the server you shouldn’t be prompted for a password (unless you entered a passphrase). By default, ssh will first try to authenticate using keys. If no keys are found or authentication fails, then ssh will fall back to conventional password authentication. If you want access to and from some servers you would need to complete this process on each client server and master serverIf you have any issues with setting this up, please let me know over on my Discord." }, { "title": "How to view which Virtual Machines have Snapshots in VMware", "url": "/posts/view-virtual-machines-snapshots-vmware/", "categories": "", "tags": "", "date": "2014-01-16 00:00:00 +0000", "snippet": "This is a question that I have been asked quite a lot recently. I have found multiple ways to do this but 2 are ones that I have used and find the most suitable. Using vSphere Client In v...", "content": "This is a question that I have been asked quite a lot recently. I have found multiple ways to do this but 2 are ones that I have used and find the most suitable. Using vSphere Client In vCenter go to: Home &gt; Inventory &gt; Datastores and Datastore Clusters Select your cluster in the left panel Choose “Storage Views” tab in the right pane. Sort by “Snapshot Space” Anything with more than 0.00b has a snapshot present Using Power CLI Connect to vCenter with PowerCLI Run this command: get-vm get-snapshot format-list vm,name You may also be interested in this article: Email Report Virtual Machine Snapshots" }, { "title": "Use Google Authenticator for 2FA with SSH", "url": "/posts/use-google-authenticator-ssh/", "categories": "Linux, Authentication", "tags": "hot-to, 2fa, linux, auth", "date": "2014-01-08 00:00:00 +0000", "snippet": "By default, SSH uses password authentication, most SSH hardening instructions recommend using SSH keys instead. However, SSH keys still only provide a single factor authentication, even though it i...", "content": "By default, SSH uses password authentication, most SSH hardening instructions recommend using SSH keys instead. However, SSH keys still only provide a single factor authentication, even though it is much more secure. But like someone can guess a password or get it from alternative sources, they can also steal your private SSH key and then access all data that key has access to.In this guide, We will setup Two-Factor authentication (2FA) meaning that more than one factor is required to authenticate or log in. This means any hackers would need to compromise multiple devices, like your computer and your phone to get access.PrerequisitesTo follow this tutorial, you will need: One CentOS 8 or Ubuntu server with a sudo non-root user and SSH key A phone or tablet with an OATH-TOTP app, like Authy or Google AuthenticatorInstall chrony to synchronize the system clockThis step is very important, due to the way 2FA works, the time must be accurate on the server. Run the following commands to setup and install chrony:timedatectl set-timezone Europe/Londondnf install chrony -ysystemctl start chronydsystemctl enable chronydtimedatectl set-timezone Europe/Londonsudo apt install chronysudo systemctl enable chrony.servicesudo systemctl restart chrony.serviceTo change the nameservers, edit the configuration file: CentOS 8 - /etc/chrony.conf Ubuntu - /etc/chrony/chrony.confInstall the Google PAMIn order to begin with configuring 2FA we will need to install the google authenticator PAMCentOS 8First, CentOS requires adding the EPEL repo:sudo yum install epel-releaseIf you don’t have the package epel-release you can install this manually:sudo yum install https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpmNow, install the PAM:sudo yum install google-authenticator qrencode-libsUbuntusudo apt install libpam-google-authenticatorConfigure SSH to use 2FA with Google Authenticator PAM Whilst making the following changes, it is important not to close the initial SSH connection. This could lock you out if anything goes wrong.To begin, back up the SSH configuration and then edit itBackup the sshd configuration:sudo cp /etc/pam.d/sshd /etc/pam.d/sshd.bakUpdate the configuration in /etc/pam.d/sshd:auth required pam_google_authenticator.so secret=/home/${USER}/.ssh/google_authenticator nullokauth required pam_permit.soThe configuration for CentOS is slightly different as it uses SELINUX which doesn’t allow the SSH Daemon to write files outside of the .ssh directory in your home folder.Due to this, we need to specify where the configuration file location is with hte secret option.auth required pam_google_authenticator.soauth required pam_permit.so nullok at the end of the line tells the PAM that this authentication method is optional. This allows users without a OATH-TOTP token to still log in just using their SSH key. Once all users have an OATH-TOTP token, you can remove nullok from this line to make MFA mandatory.you can now save and close the file.We now need to configure SSH to support this authentication method:First backup the SSH configuration:sudo cp /etc/ssh/sshd_config /etc/ssh/sshd_config.bakNow look for the line ChallengeResponseAuthentication. change this to yes# Change to no to disable s/key passwordsChallengeResponseAuthentication yesSave and close this file, then restart SSH to reload the configuration. This wont close your current open connections so you wont lose access.sudo systemctl restart sshdCreate your tokenWe now need to create our 2FA token, to do this we run the following command:google-authenticator -s ~/.ssh/google_authenticatorgoogle-authenticatorAfter you run this command you will be asked a few questions. The first question asks if authentication should be time based:Do you want authentication tokens to be time-based (y/n) yThis PAM allows time-based or sequential-based tokens, Using sequential based means the token increases by 1 after each use. Using time-based means that the token is based on a certain point in time.After answering this question a large QR Code will appear, using your phone authenticator app, take a picture of the QR Code to add it to your application. A URL is also provided above the QR Code to open in a browser if required.The remaining options inform PAM how to function, I have listed these below with the options that I chose, you can change these as you see fit for your requirements.Do you want me to update your \"~/.google_authenticator\" file (y/n) yDo you want to disallow multiple uses of the same authenticationtoken? This restricts you to one login about every 30s, but it increasesyour chances to notice or even prevent man-in-the-middle attacks (y/n) yBy default, a new token is generated every 30 seconds by the mobile app.In order to compensate for possible time-skew between the client and the server,we allow an extra token before and after the current time. This allows for atime skew of up to 30 seconds between authentication server and client. If youexperience problems with poor time synchronization, you can increase the windowfrom its default size of 3 permitted codes (one previous code, the currentcode, the next code) to 17 permitted codes (the 8 previous codes, the currentcode, and the 8 next codes). This will permit for a time skew of up to 4 minutesbetween client and server.Do you want to do so? (y/n) nIf the computer that you are logging into isn't hardened against brute-forcelogin attempts, you can enable rate-limiting for the authentication module.By default, this limits attackers to no more than 3 login attempts every 30s.Do you want to enable rate-limiting (y/n) yOnce finished you can backup your secret key by copying the ~/.ssh/google-authenticator file to a trusted location. You can also copy this to other servers to use on multiple machines, however it is worth keeping in mind that then if one server is attacked there is potential for an attacker to gain access to other servers.Make SSH Aware of 2FARe-open the sshd configuration file and add the following line to the bottom of the file. This tells SSH whcih authentication methods are required:AuthenticationMethods publickey,password publickey,keyboard-interactiveSave and close the file.Next, open the PAM sshd configuration file again and find the line auth substack password-auth. comment it out by adding a # to the start of the line:# auth substack password-auth If you want three factors of authentication, you can skip commenting this lineSave and close the file, then restart SSH:sudo systemctl restart sshdNow login to the server with a different terminal session/window. Unlike last time, SSH should ask for your verification code. Upon entering it, you’ll be logged in. Even though you don’t see any indication that your SSH key was used, your login attempt used two factors. If you want to verify, you can add -v (for verbose).Avoid 2FA in certain situations (optional)User AccountsThere may be some situations where a specific user or service account needs SSH access without 2FA enabled. For example, if an application doesn’t have a way to request the verification code, the request may get stuck until the SSH connection times out.To allow MFA for some accounts and SSH key only for others, make sure the configuration in /etc/pam.d/sshd contains the nullok option as shown in previous steps.After setting this, simply run the google-authenticatior command for any users that require 2FA.Alternatively, if you only want to allow a single user to bypass 2FA you can add the following:auth [success=done new_authtok_reqd=done default=die] pam_google_authenticator.soauth [success=done new_authtok_reqd=done default=die] pam_succeed_if.so user = johnSpecific NetworksThere may be some situations where specific networks are trusted and dont require 2FA to be used, in these situations we can update the configuration with the following:auth [success=1 default=ignore] pam_access.so accessfile=/etc/security/access-local.confauth required pam_google_authenticator.so nullokCreate the device authentication configuration file for the specified ip addresses:# Two-factor can be skipped on local network+ : ALL : 10.0.0.0/24+ : ALL : LOCAL- : ALL : ALLLocal login attempts from 10.0.0.0/24 will not require two-factor authentication, while all others do.Now we need to edit the ssh daemon configuration file. Please keep in mind that this could add a security risk if not locked down sufficientlyRestart the SSH daemon:systemctl restart sshdFinal ThoughtsThis how-to guide has taken you through how to add 2FA authentication using google authentication via your computer and your phone making your system considerably more secure. It is now much more difficult for a brute force attack via SSH." }, { "title": "PHP Notice: Undefined index", "url": "/posts/php-notice-undefined-index/", "categories": "PHP", "tags": "undefined, index, php", "date": "2014-01-05 00:00:00 +0000", "snippet": "I have had a few times when coding where I get the error PHP Notice: Undefined Index, I found the below solution to this issue which is an extremely simple fix!How to FixOne simple answer – isset()...", "content": "I have had a few times when coding where I get the error PHP Notice: Undefined Index, I found the below solution to this issue which is an extremely simple fix!How to FixOne simple answer – isset() !isset() function in PHP determines whether a variable is set and is not NULL. It returns a Boolean value, that is, if the variable is set it will return true and if the variable value is null it will return false. More details on this function can be found in PHP ManualExampleLet us consider an example. Below is the HTML code for a comment form in a blog.&lt;form name=\"myform\" id=\"myform\" method=\"post\" action=\"add_comment.php\"&gt; &lt;h2&gt;Please leave a comment:&lt;/h2&gt; &lt;input type=\"text\" id=\"username\" name=\"username\" value=\"Enter Username\" /&gt;&lt;br /&gt; &lt;input type=\"text\" id=\"email\" name=\"email\" value=\"Enter Email\" /&gt;&lt;br /&gt; &lt;textarea id=\"comment\" name=\"comment\"&gt;Enter Comment&lt;/textarea&gt;&lt;br /&gt; &lt;br /&gt;&lt;br /&gt; &lt;input type=\"checkbox\" id=\"notify_box\" name=\"notify_box\" value=\"Y\"&gt; Notify me when a new post is published. &lt;br /&gt; &lt;br /&gt; &lt;input type=\"submit\" value=\"Post Comment\"&gt;&lt;/form&gt;Here is the PHP file ‘add_comment.php’ which takes the data passed from the comment form.&lt;?php $uName = $_POST['username']; $eMail = $_POST['email']; $comment= $_POST['comment']; $notify = $_POST['notify_box']; // send the data to the database?&gt;What happens is, when the check-box is CHECKED, the code works fine. But when it is not, then I am getting the warning as mentioned above. Warning: Undefined index:So to fix this, let us make use of the magic function. Now the code appears like this.&lt;?php $notify = \"\"; $uName = $_POST['username']; $eMail = $_POST['email']; $comment= $_POST['comment']; if(isset($_POST['notify_box'])){ $notify = $_POST['notify_box']; } // send the data to the database?&gt;What happens here is, I am checking first whether the check box is CHECKED (or set) using a condition. And if the condition is true I am getting the value passed.The same fix can be used for the above warning when working with $\\_SESSION, $\\_POST arrays.But, there instances where harmless notices can be ignored.For an example, I have a page which can be accesses in below 3 ways.www.someexample.com/comments.phpwww.someexample.come/comments.php?action=addwww.someexample.com/comments.php?action=deleteAll these URL’s go to the same page but each time performs a different task.So when I try to access the page through the first URL, it will give me the ‘Undefined index’ notice since the parameter ‘action’ is not set.We can fix this using the isset() function too. But on this instance, we can just ignore it by hiding the notices like this.error\\_reporting(E\\_ALL ^ E_NOTICE);You can also turn off error reporting in your php.ini file or .htaccess file, but it is not considered as a wise move if you are still in the testing stage.This is another simple solution in PHP for a common complex problem. Hope it is useful. This is an example only my form has no security hardening. Use at own risk." }, { "title": "Managing Application Settings in PHP", "url": "/posts/managing-application-settings-in-php/", "categories": "PHP", "tags": "settings, php, application", "date": "2014-01-05 00:00:00 +0000", "snippet": "There are multiple ways to save application settings/configurations in PHP. You can save them in INI, XML or PHP files as well as a database table. I prefer a combination of the latter two; saving ...", "content": "There are multiple ways to save application settings/configurations in PHP. You can save them in INI, XML or PHP files as well as a database table. I prefer a combination of the latter two; saving the database connection details in a PHP file and the rest in a database table.The advantage of using this approach over the others will be apparent when developing downloadable scripts, as updates will not need to modify a configuration file of an already setup script.To start create a table containing 3 fields: auto increment ID, setting name and setting value:CREATE TABLE IF NOT EXISTS `settings` ( `setting_id` int(11) NOT NULL AUTO_INCREMENT, `setting` varchar(50) NOT NULL, `value` varchar(500) NOT NULL, PRIMARY KEY (`setting_id`), UNIQUE KEY `setting` (`setting`)) ENGINE=InnoDB DEFAULT CHARSET=latin1 AUTO_INCREMENT=1 ;The following function updates the value of a setting by supplying it with the setting name and the new value.&lt;?phpfunction SaveSetting($settingname, $settingvalue) { mysql_query(\"UPDATE settings SET `value`='$settingvalue' WHERE `setting`='$settingname'\");}?&gt;The GetSetting function returns 1 or more values depending on the parameter passed. You can pass a setting name in a string or multiple settings in an array of strings. The latter will save you multiple trips to the database to perform a task that requires more than 1 setting like sending an email which needs the email server, protocol and credentials.&lt;?phpfunction GetSetting($settingname) { if(!is_array($settingname)) { //user passed a single setting name $result = mysql_query(\"SELECT * FROM settings WHERE setting='$settingname'\"); $setting = mysql_fetch_array($result); if(mysql_num_rows($result)) { return $setting['value']; } else { return null; } } else { //user passed multiple setting names in an array $params = ''; for($c = 0 ; $c &amp;lt; count($settingname) ; $c++) { $params .= \"setting='{$settingname[$c]}'\"; if($c != (count($settingname) - 1)) $params .= ' OR '; } $result = mysql_query(\"SELECT * FROM settings WHERE \" . $params); if(mysql_num_rows($result)) { $settings_array = array(); while($setting = mysql_fetch_array($result)) { $settings_array[$setting['setting']] = $setting['value']; } return $settings_array; } else { return null; } }}?&gt;Here is an example of using the GetSetting function to initialize an email object.&lt;?php$setting = GetSetting(array('email_protocol', 'email_smtpauth', 'email_server', 'email_port'));$mail = new PHPMailer();if($setting['email_protocol'] == 1) $mail-&amp;gt;IsSMTP();if($setting['email_smtpauth'] == 1) $mail-&amp;gt;SMTPAuth = true;$mail-&amp;gt;Host = $setting['email_server'];$mail-&amp;gt;Port = $setting['email_port'];?&gt; This code does not filter the values sent to SaveSetting(). To prevent SQL injection and XSS attacks please make sure you check the values before saving them and also after reading them using GetSetting()." }, { "title": "How to recreate all Virtual Directories for Exchange 2007", "url": "/posts/how-to-recreate-all-virtual-directories-for-exchange-2007/", "categories": "Microsoft, Exchange", "tags": "exchange, 2007, virtual, directories", "date": "2012-11-22 00:00:00 +0000", "snippet": "Here you will find all commands what would help you to recreate all Virtual Directories for Exchange 2007. You can also use just a few of them. But never delete or create it in IIS. This has to be ...", "content": "Here you will find all commands what would help you to recreate all Virtual Directories for Exchange 2007. You can also use just a few of them. But never delete or create it in IIS. This has to be done under Exchange Management Shell (don’t get mixed with the Windows Powershell):Here you will find all commands what would help you to recreate all Virtual Directories for Exchange 2007. You can also use just a few of them. But never delete or create it in IIS. This has to be done under Exchange Management Shell (don’t get mixed with the Windows Powershell):First you shall write down the information what you will get (for example: if it “Default Web Site” or “SBS Web Applications” and if they have the information, what INTERNURL or External URL is configured):– Open Exchange Management Shell with elevated permission– Run the following commands:Get-AutodiscoverVirtualDirectoryGet-OABVirtualDirectoryGet-OWAVirtualDirectoryGet-WebServicesVirtualDirectoryGet-ActiveSyncVirtualDirectoryGet-UMVirtualDirectoryThen you can remove the Virtual Directories but change the XXXXXXX to the information you got earlier. DON’’‘T JUST COPY AND PASTE INTO POWERSHELL! COPY IT INTO NOTEPAD AND THEN READ THE COMMAND AND CHANGE SERVERNAME OR OTHER RELATED INFORMATION)**Remove-OWAVirtualDirectory -Identity \"Owa (XXXXXXX)\" -Confirm:$falseRemove-OWAVirtualDirectory -Identity \"Exadmin (XXXXXXX)\" -Confirm:$falseRemove-OWAVirtualDirectory -Identity \"Exchange (XXXXXXX)\" -Confirm:$falseRemove-OWAVirtualDirectory -Identity \"Exchweb (XXXXXXX)\" -Confirm:$falseRemove-OWAVirtualDirectory -Identity \"Public (XXXXXXX)\" -Confirm:$falseRemove-WebServicesVirtualDirectory -Identity \"EWS (XXXXXXX)\" -Confirm:$falseRemove-ActiveSyncVirtualDirectory -Identity \"Microsoft-Server-ActiveSync (XXXXXXX)\" -Confirm:$falseRemove-OabVirtualDirectory -Identity \"OAB (XXXXXXX)\" -Force:$true -Confirm:$falseRemove-UMVirtualDirectory -Identity \"UnifiedMessaging (XXXXXXX)\" -Confirm:$falseRemove-AutodiscoverVirtualDirectory -Identity \"Autodiscover (XXXXXXX)\" -Confirm:$falseTo verify that the directories have been removed, run the following commands. You should receive no output:Get-AutodiscoverVirtualDirectoryGet-OABVirtualDirectoryGet-OWAVirtualDirectoryGet-WebServicesVirtualDirectoryGet-ActiveSyncVirtualDirectoryGet-UMVirtualDirectoryTo properly create these virtual directories, run the following commands (Please keep the information what you got earlier for XXXXXXX and change it here to):Open Exchange Management Shell with elevated permission and run the following commands:New-OWAVirtualDirectory -WebsiteName \"XXXXXXX\" -OwaVersion \"Exchange2007\" -ExternalAuthenticationMethods FbaSet-OWAVirtualDirectory -InternalUrl \"https://INTERNAL_FQDN_OF_EXCHANGE/owa/\" -ClientAuthCleanupLevel \"Low\" -LogonFormat \"UserName\" -DefaultDomain \"NETBIOSDOMAINNAME\" -Identity \"Owa (XXXXXXX)\"New-OWAVirtualDirectory -WebsiteName \"XXXXXXX\" -OwaVersion \"Exchange2003or2000\" -VirtualDirectoryType \"Exadmin\" -ExternalAuthenticationMethods FbaNew-OWAVirtualDirectory -WebsiteName \"XXXXXXX\" -OwaVersion \"Exchange2003or2000\" -VirtualDirectoryType \"Mailboxes\" -ExternalAuthenticationMethods FbaNew-OWAVirtualDirectory -WebsiteName \"XXXXXXX\" -OwaVersion \"Exchange2003or2000\" -VirtualDirectoryType \"Exchweb\" -ExternalAuthenticationMethods FbaNew-OWAVirtualDirectory -WebsiteName \"XXXXXXX\" -OwaVersion \"Exchange2003or2000\" -VirtualDirectoryType \"PublicFolders\" -ExternalAuthenticationMethods FbaNew-WebServicesVirtualDirectory -WebsiteName \"XXXXXXX\" -InternalUrl \"https://INTERNAL_FQDN_OF_EXCHANGE/EWS/Exchange.asmx\" -basicauthentication 1 -windowsauthentication 1New-ActiveSyncVirtualDirectory -WebsiteName \"XXXXXXX\" -InternalUrl \"https://INTERNAL_FQDN_OF_EXCHANGE/Microsoft-Server-ActiveSync\" -ExternalAuthenticationMethods Basic -InternalAuthenticationMethods BasicNew-OabVirtualDirectory -WebsiteName \"XXXXXXX\" -InternalUrl \"https://INTERNAL_FQDN_OF_EXCHANGE/OAB\"Set-OabVirtualDirectory -PollInterval \"30\" -Identity \"oab (XXXXXXX)\"New-UMVirtualDirectory -WebsiteName \"XXXXXXX\"-InternalUrl \"https://INTERNAL_FQDN_OF_EXCHANGE/UnifiedMessaging/Service.asmx\"New-AutodiscoverVirtualDirectory -WebsiteName \"XXXXXXX\" -InternalUrl \"https://INTERNAL_FQDN_OF_EXCHANGE/Autodiscover/Autodiscover.xml\" -BasicAuthentication 1 -WindowsAuthentication 1Set-ClientAccessServer -Identity \"Servername\" -AutoDiscoverServiceInternalUri \"https://INTERNAL_FQDN_OF_EXCHANGE/Autodiscover/Autodiscover.xml\"Set-OfflineAddressBook \"Default Offline Address Book\" -VirtualDirectories \"Servername\\OAB (XXXXXXX)\" -Versions Version2,Version3,Version4)\"To check if we were successful in creating the virtual directories correctly type in the commands:Get-AutodiscoverVirtualDirectoryGet-OABVirtualDirectoryGet-OWAVirtualDirectoryGet-WebServicesVirtualDirectoryGet-ActiveSyncVirtualDirectoryGet-UMVirtualDirectoryFor example, you should receive the following for Get-OWAVirtualDirectoryName Server OwaVersion——– ——- ———–Owa (XXXXXXX) Server Name Exchange2007Exadmin (XXXXXXX) Server Name Exchange2003or2000Public (XXXXXXX) Server Name Exchange2003or2000Exchweb (XXXXXXX) Server Name Exchange2003or2000Exchange(XXXXXXX) Server Name Exchange2003or2000Then run the following commands to disable the Kernel Mode Authentication on EWS, Autodiscover, and OAB virtual directories:cd $env:windir\\system32\\inetsrv.\\appcmd.exe unlock config \"-section:system.webserver/security/authentication/windowsauthentication\".\\appcmd.exe set config \"XXXXXXX/ews\" \"-section:windowsAuthentication\" \"-useKernelMode:False\" /commit:apphost.\\appcmd.exe set config \"XXXXXXX/AutoDiscover\" \"-section:windowsAuthentication\" \"-useKernelMode:False\" /commit:apphost.\\appcmd.exe set config \"XXXXXXX/oab\" \"-section:windowsAuthentication\" \"-useKernelMode:False\" /commit:apphostRun: iisreset /noforceYou must rerun the Internet Address Management Wizard to stamp the new virtual directories with the proper external URL and maybe you have to check the certificates.======================================Troubleshooting for useKernelMode%windir%\\system32\\inetsrv\\appcmd.exe set config /section:system.webServer/security/authentication/windowsAuthentication /useKernelMode:falseWith the following command you should see something like this:%windir%\\system32\\inetsrv\\appcmd.exe list config /section:system.webServer/security/authentication/windowsAuthentication" }, { "title": "Your client does not support opening this list with windows explorer", "url": "/posts/your-client-does-not-support-opening-this-list-with-windows-explorer/", "categories": "", "tags": "", "date": "2012-09-07 00:00:00 +0100", "snippet": "When using Office 365 and sharepoint 2010 you may find that trying to open a library in explorer will result in this error:“Your client does not support opening this list with windows explorer”I ha...", "content": "When using Office 365 and sharepoint 2010 you may find that trying to open a library in explorer will result in this error:“Your client does not support opening this list with windows explorer”I have written a few simple things to check and once these are met the issue should be resolved.There are a few things to check: Use IE x86 not x64. Make sure the URL is in the trusted sites list within internet options and security. If it is Windows Server make sure Desktop Experience is installed. Make sure the WebClient service is started.If all of these things are met then your issue should now be resolved." }, { "title": "Folder redirection permissions. My Documents / Start Menu / Desktop", "url": "/posts/folder-redirection-permissions-my-documents-start-menu-desktop/", "categories": "Microsoft, Windows", "tags": "windows, redirect, documents, start", "date": "2012-07-30 00:00:00 +0100", "snippet": "How to correctly set-up folder redirection permissions for My Documents, Start Menu and Desktop. I have worked on many company computer systems where this hadn’t been done correctly resulting in fu...", "content": "How to correctly set-up folder redirection permissions for My Documents, Start Menu and Desktop. I have worked on many company computer systems where this hadn’t been done correctly resulting in full access to all files and folders, as an outsider I had access to other peoples my documents from my laptop without being on the domain! Following this article will stop that happening to your data.When creating the redirection share, limit access to the share to only users that need access.Because redirected folders contain personal information, such as documents and EFS certificates care should be taken to protect them as well as possible. In general: Restrict the share to only users that need access. Create a security group for users that have redirected folders on a particular share, and limit access to only those users. When creating the share, hide the share by putting a $ after the share name. This will hide the share from casual browsers; the share will not be visible in My Network Places. Only give users the minimum amount of permissions needed. The permissions needed are shown in the tables below:Table 12 NTFS Permissions for Folder Redirection Root Folder User Account Minimum permissions required Creator/Owner Full Control, Subfolders And Files Only Administrator None Security group of users needing to put data on share. List Folder/Read Data, Create Folders/Append Data &#8211; This Folder Only Everyone No Permissions Local System Full Control, This Folder, Subfolders And Files Table 13 Share level (SMB) Permissions for Folder Redirection Share User Account Default Permissions Minimum permissions required Everyone Full Control No Permissions Security group of users needing to put data on share. N/A Full Control, Table 14 NTFS Permissions for Each Users Redirected Folder User Account Default Permissions Minimum permissions required %Username% Full Control, Owner Of Folder Full Control, Owner Of Folder Local System Full Control Full Control Administrators No Permissions No Permissions Everyone No Permissions No Permissions Always use the NTFS Filesystem for volumes holding users data.For the most secure configuration, configure servers hosting redirected files to use the NTFS File System. Unlike FAT, NTFS supports Discretionary access control lists (DACLs) and system access control lists (SACLs), which control who can perform operations on a file and what events will trigger logging of actions performed on a file.Let the system create folders for each user.To ensure that Folder Redirection works optimally, create only the root share on the server, and let the system create the folders for each user. Folder Redirection will create a folder for the user with appropriate security.If you must create folders for the users, ensure that you have the correct permissions set, also note that if pre-creating folders you must clear the “grant the user exclusive rights to XXX checkbox on the settings tab of the Folder Redirection page. If you don’t clear this checkbox, then Folder Redirection will first check a pre-existing folder to ensure the user is the owner. If the folder is pre-created by the administrator, this check will fail and redirection will be aborted. Folder Redirection will then log an event in the Application event log:Error: Folder RedirectionEvent ID: 101Event Message:Failed to perform redirection of folder XXXX. The new directories for the redirected folder could not be created. The folder is configured to be redirected to \\server\\share, the final expanded path was \\server\\share\\XXX .The following error occurred:This security ID may not be assigned as the owner of this object.It is strongly recommended that you do not pre-create folders, and allow Folder Redirection to create the folder for the user.Ensure correct permissions are set if redirecting to a users home directory.Windows Server 2003 and Windows XP allow you to redirect a users My Documents folder to their home directory. When redirecting to the home directory, the default security checks are not made – ownership and the existing directory security are not checked and any existing permissions are not changed – it is assumed that the permissions on the users home directory are set appropriately.If you are redirecting to a users home directory, be sure that the permissions on the users home directory are set appropriately for your organization." }, { "title": "How to turn on automatic logon to a domain with Windows XP, Windows 7 and Server 2008", "url": "/posts/how-to-turn-on-automatic-logon-to-a-domain-with-windows-xp-windows-7-and-server-2008/", "categories": "", "tags": "", "date": "2012-07-04 00:00:00 +0100", "snippet": "I had a requirement for some of our security camera servers to login automatically now on a normal standalone computer this is easy but on a domain it gets more complicated.So how did I overcome th...", "content": "I had a requirement for some of our security camera servers to login automatically now on a normal standalone computer this is easy but on a domain it gets more complicated.So how did I overcome this?I found a very useful Microsoft KB article and adapted it to work with a domain account, see below for my adapted version. Click Start, click Run, type regedit, and then click OK. Locate the following registry key: HKEY\\_LOCAL\\_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\\Winlogon Double click DefaultDomainName, and type your domain name in here, Click OK Double click DefaultUserName, and type your username in here, Click OK Double click DefaultPassword, and type your password in here, Click OK Double click the AutoAdminLogon entry, type 1 in the Value Data box, and then click OK. If any of the above done exist create them as below In Registry Editor, click Edit, click New, and then click String Value. Type **** as the value name, and then press ENTER. Double-click the newly created key, and then type your password in the Value Data box. Restart the computer / server and watch it logon!" }, { "title": "Upgrading a Cisco Catalyst 3560 Switch", "url": "/posts/upgrading-a-cisco-catalyst-3560-switch/", "categories": "Networking, Cisco", "tags": "cisco, networking, upgrade, catalyst, 3560", "date": "2012-06-27 00:00:00 +0100", "snippet": "Here are my notes on upgrading a Catalyst 3560. I plugged in a laptop to the serial console and an ethernet cable into port 1 (technically interface Gigabit Ethernet 0/1). Here is the official Cisc...", "content": "Here are my notes on upgrading a Catalyst 3560. I plugged in a laptop to the serial console and an ethernet cable into port 1 (technically interface Gigabit Ethernet 0/1). Here is the official Cisco documentation I followed. It’s for the 3550, but the Cisco support engineer said that it’s close enough.First Hurdle: VLAN Mismatch ErrorI quickly got bunch of errors that stated “Native VLAN Mismatch: discovered on Gigabit Ethernet 0/1.” The far end of the new switch is VLAN1. To fix this error, I moved port 1 from VLAN 3 to VLAN 1. These are the commands I ran.switch&gt;show vlanVLAN Name Status Ports———- ———– ————- ——————–1 default active Gi0/283 server active Gi0/1, Gi0/2, Gi0/3 ….switch&gt; enableswitch# config termswitch (config)# interface Gi0/1switch (config-if)# switchport access vlan 1switch (config-if)# exitswitch (config)# exitswitch# show vlanVLAN Name Status Ports——— ———– ————- —————1 default active Gi0/1, Gi0/283 server active Gi0/2, Gi0/3 ….Second Hurdle: Couldn’t Reach TFTP ServerTo upgrade the image, I had to put the image on a TFTP server and pull it down. Unfortunately I wasn’t able to ping my FTP server. I quickly figured out that I didn’t have an IP address for VLAN 1. To set the IP address:switch(config)#int vlan 1switch(config-if)# ip address 172.16.0.10 255.255.0.0switch# ping 172.16.1.27Finally the upgrade!Cisco offers a .tar and a .bin image for the upgrades. Use the .tar file if you want to use the CMS software. I just wanted to use the command line interface (CLI), so I grabbed the .bin file (c3560-ipbasek9-mz.122-44.SE.bin) and placed it on the root directory of my TFTP server.1. Verified that there’s enough space. The .bin file is ~8MB. In the 3560’s flash:switch#dir flash&lt;——- Tuncated except for last line ——-&gt;24998976 bytles total (16349184 bytes free)Since there’s ~ 16 MB free, we’re good to go. Otherwise delete by issuing this command:switch#delete /force /recursive flash:directory\\_of\\_old\\_image\\_here2. Downloaded image onto the switch and verified its integrityswitch#copy tftp flashAddress or name of remote host []? 172.16.1.27Source filename []? c3560-ipbasek9-mz.122-44.SE.binDestination filename [c3560-ipbasek9-mz.122-44.SE.bin]?switch#dir flashswitch#verify flash:c3560-ipbasek9-mz.122-44.SE.bin3. Changed the boot image in the switchswitch#show bootswitch#configure terminalswitch(config)#boot system flash:c3560-ipbasek9-mz.122-44.SE.binswitch#exitswitch#show boot4. Saved, verified, and rebootedswitch#write memoryswitch#show versionswitch#reloadUpon reboot:switch#show ver5. Drank a celebratory drink. Coffee of course, because I was still at work." }, { "title": "Deploy .exe using batch check os version and if the update is already installed.", "url": "/posts/deploy-exe-using-batch-check-os-version-and-if-the-update-is-already-installed/", "categories": "", "tags": "", "date": "2012-02-08 00:00:00 +0000", "snippet": "OK so I had an issue that Microsoft released an update for Windows XP that I needed to install but they didn’t do an MSI so I couldn’t deploy is using GPO which was a real pain.Instead I created a ...", "content": "OK so I had an issue that Microsoft released an update for Windows XP that I needed to install but they didn’t do an MSI so I couldn’t deploy is using GPO which was a real pain.Instead I created a script that would check the OS Version and see if the update was already installed. First we hide the script from users: @ECHO Off Then we check they are running the correct OS (for windows 7 “Version 6.1”) ver | find \"Windows XP\" &gt;NUL if errorlevel 1 goto end Check to see if the update is installed (chance the reg location depending on the install) reg QUERY &amp;#8220;HKEY\\_LOCAL\\_MACHINE\\SOFTWARE\\Microsoft\\Updates\\Windows XP\\SP20\\KB943729&amp;#8221; &gt;NUL 2&gt;NUL if errorlevel 1 goto install_update goto end Then if it is the correct OS and the update isn’t installed run the exe :install_update \\\\PUT\\_YOUR\\_SHARE\\_PATH\\_HERE\\Windows-KB943729-x86-ENU.exe /passive /norestart End (this is added so that the script will stop if the criteria are not met before the update is installed stopping errors. :end you can then add this to a group policy to allow it to be deployed" }, { "title": "Active Sync Error EventID 3005 Unexpected Exchange Mailbox Server Error", "url": "/posts/active-sync-error-eventid-3005-unexpected-exchange-mailbox-server-error/", "categories": "", "tags": "", "date": "2012-02-01 00:00:00 +0000", "snippet": "One of our customers was getting the below error and it took ages to find a solution so I thought I would post it here.Unexpected Exchange mailbox Server error: Server: [server.domain] User: [usere...", "content": "One of our customers was getting the below error and it took ages to find a solution so I thought I would post it here.Unexpected Exchange mailbox Server error: Server: [server.domain] User: [useremail] HTTP status code: [503]. Verify that the Exchange mailbox Server is working correctly.For more information, see Help and Support Center at http://go.microsoft.com/fwlink/events.asp.This is how I fixed the issue: Open IIS Right click Default-Website Click Properties Click advanced Review sites. most likely you will see host headers and ip address. Click add IP address = (all unassigned) TCP Port = 80 Host Header Value = (Blank) Click OK delete the entry with the host headers and ip address assignerd.This should resolve the issue please comment if you have any issues doing this." }, { "title": "Office 365 Scan to Email", "url": "/posts/office-365-scan-to-email/", "categories": "", "tags": "", "date": "2012-01-06 00:00:00 +0000", "snippet": "Ok so this one had me stumped for a LONG time trying to figure out how to get scanners to authenticate to office 365 in the end i found out that the scanner i was using wasnt supported in this form...", "content": "Ok so this one had me stumped for a LONG time trying to figure out how to get scanners to authenticate to office 365 in the end i found out that the scanner i was using wasnt supported in this format so i found this work around hope it helps you!You basically need to create an smtp relay on a local server / computer to forward your scans to then set the smtp relay up as below which will then do the authentication part for you. SMTP relay settings for Office 365To configure an SMTP relay in Office 365, you need the following: A user who has an Exchange Online mailbox The SMTP set to port 587 Transport Layer Security (TLS) encryption enabled The mailbox server nameTo obtain SMTP settings information, follow these steps: Sign in to Outlook Web App. Click Options, and then click See All Options. Click Account, click My Account, and then in the Account Information area, click Settings for POP, IMAP, and SMTP access.Note the SMTP settings information that is displayed on this page.Configure Internet Information Services (IIS)To configure Internet Information Services (IIS) so that your LOB programs can use the SMTP relay, follow these steps: Create a user who has an Exchange Online mailbox. To do this, use one of the following methods: Create the user in Active Directory Domain Services, run directory synchronization, and then activate the user by using an Exchange Online license.Note The user must not have an on-premises mailbox. Create the user by using the Office 365 portal or by using Microsoft Online Services PowerShell Module, and then assign the user an Exchange Online license. Configure the IIS SMTP relay server. To do this, follow these steps: &lt;li type=\"a\"&gt; Install IIS on an internal server. During the installation, select the option to install the SMTP components. &lt;/li&gt; &lt;li type=\"a\"&gt; In Internet Information Services (IIS) Manager, expand the Default SMTP Virtual Server, and then click Domains. &lt;/li&gt; &lt;li type=\"a\"&gt; Right-click Domains, click New, click Domain, and then click Remote. &lt;/li&gt; &lt;li type=\"a\"&gt; In the Name box, type *.com, and then click Finish. &lt;/li&gt; Double-click the domain that you just created. Click to select the Allow incoming mail to be relayed to this domain check box. In the Route domain area, click Forward all mail to smart host, and then in the box, type the mailbox server name. Click Outbound Security, and then configure the following settings: &lt;li type=\"a\"&gt; Click Basic Authentication. &lt;/li&gt; &lt;li type=\"a\"&gt; In the User name box, type the user name of the Office 365 mailbox user. &lt;/li&gt; &lt;li type=\"a\"&gt; In the Password box, type the password of the Office 365 mailbox user. &lt;/li&gt; &lt;li type=\"a\"&gt; Click to select the TLS encryption check box, and then click OK. &lt;/li&gt; Right-click the Default SMTP Virtual Server node, and then click Properties. On the Delivery tab, click Outbound Connections. In the TCP Port box, type 587, and then click OK. Click Outbound Security, and then configure the following settings: &lt;li type=\"a\"&gt; Click Basic Authentication. &lt;/li&gt; &lt;li type=\"a\"&gt; In the User name box, type the user name of the Office 365 mailbox user. &lt;/li&gt; &lt;li type=\"a\"&gt; In the Password box, type the password of the Office 365 mailbox user. &lt;/li&gt; &lt;li type=\"a\"&gt; Click to select the TLS encryption check box, and then click OK. &lt;/li&gt; On the Access tab, click Authentication, click to select the Anonymous access check box, and then click OK. On the Relay tab, select Only the list below, type the IP addresses of the client computers that will be sending the email messages, and then click OK." }, { "title": "Assigning Send As Permissions to a user", "url": "/posts/assigning-send-as-permissions-to-a-user/", "categories": "Microsoft", "tags": "email, send as, exchange", "date": "2011-12-06 00:00:00 +0000", "snippet": "It was brought to my attention that following the steps listed in KB327000, which applies to Exchange 2000 and 2003, to assign a user Send As permission as another user did not appear to work.  I t...", "content": "It was brought to my attention that following the steps listed in KB327000, which applies to Exchange 2000 and 2003, to assign a user Send As permission as another user did not appear to work.  I too tried to follow the steps and found that they did not work. I know this feature works, so I went looking around for other documentation on this and found KB281208 which applies to Exchange 5.5 and 2000.  Following the steps in KB281208 properly gave an user Send As permission as another user. But I found the steps listed in KB281208 were not complete either. The additional step that I performed was to remove all other permissions other than Send As.  Here are the modified steps for KB281208 that I performed: Start Active Directory Users and Computers; click Start, point to Programs, point to Administrative Tools, and then click Active Directory Users and Computers. On the View menu, make sure that Advanced Features is selected. Double-click the user that you want to grant send as rights for, and then click theSecurity tab. Click Add, click the user that you want to give send as rights to, and then check send as under allow in the Permissions area. Remove all other permissions granted by default so only the send as permission is granted. Click OK to close the dialog box.So after I verified that the steps for KB281208 worked, I was curious as to why the steps for KB327000 did not work.  What I found was that Step #7 of KB327000 applied to the permission to User Objects instead of This Object Only.  Here are the modified steps for KB327000 that I performed: On an Exchange computer, click Start, point to Programs, point to Microsoft Exchange, and then click Active Directory Users and Computers. On the View menu, click to select Advanced Features. Expand Users, right-click the MailboxOwner object where you want to grant the permission, and then click Properties. Click the Security tab, and then click Advanced. In the Access Control Settings for MailboxOwner dialog box, click Add. In the Select User, Computer, or Group dialog box, click the user account or the group that you want to grant Send as permissions to, and then click OK. In the Permission Entry for MailboxOwner dialog box, click This Object Only in theApply onto list. In the Permissions list, locate Send As, and then click to select the Allow check box. Click OK three times to close the dialog boxes.The KB articles were updated to include correct information. But, if you had problems with this in the past, this might be why!" }, { "title": "How To View and Kill Processes On Remote Windows Computers", "url": "/posts/how-to-view-and-kill-processes-on-remote-windows-computers/", "categories": "Microsoft, Windows", "tags": "task, kill, taskkill, process, remote", "date": "2011-09-14 09:00:00 +0100", "snippet": "Windows provides several methods to view processes remotely on another computer. Terminal Server is one way or you can use the command line utility pslist from Microsoft Sysinternals site. While bo...", "content": "Windows provides several methods to view processes remotely on another computer. Terminal Server is one way or you can use the command line utility pslist from Microsoft Sysinternals site. While both options are good alternatives, Windows XP and Vista provides a built in utility for viewing and killing process on remote Computers using Tasklist and Taskkill commands.Both tasklist.exe and taskkill,exe can be found in %SYSTEMROOT%\\System32 (typically C:\\Windows\\System32) directory.To view processes on a remote Computer in your home, you will need to know the username and password on the Computer you want to view the processes. Once you have the user account information, the syntax for using tasklist follows:_tasklist.exe /S SYSTEM /U USERNAME /P PASSWORD_(To view all tasklist options, type tasklist /? at the command prompt)To execute, click on Start \\ Run… and in the run window type cmd to open a command prompt. Then type the tasklist command, substituting SYSTEM for the remote computer you want to view processes, USERNAME and PASSWORD with an account/password on the remote Computer. if you are in a Domain environment and have Administrator rights to the remote Computer, you will may not need to specify a Username and Password" }, { "title": "Fortigate and LDAP 4.0 MR3 Patch1", "url": "/posts/fortigate-and-ldap-4-0-mr3-patch1/", "categories": "", "tags": "", "date": "2011-09-12 00:00:00 +0100", "snippet": "Hi Guys,I have been setting up a lot of Fortigate’s recently and on my first few had issues with the settings for LDAP i found that it was tricky to remember the correct settings and also typing ou...", "content": "Hi Guys,I have been setting up a lot of Fortigate’s recently and on my first few had issues with the settings for LDAP i found that it was tricky to remember the correct settings and also typing out the long LDAP Strings can be a bit tricky and cause typo’s. Logon to the fortigate and go to the Users -&gt; Remote -&gt; LDAP (Create New) Fill in a Name for the connector Fill in the IP Address of the server that has LDAP Installed Change the Common Name Identifier to: sAMAccountName Enter the Distinguished Name if your domain was domain.local the distinguished name would be: DC=domain,DC=local Make your Bind Type Regular In the User DN Box you must type the full path to the user e.g. if you user is domain.local/users/service accounts/fortigate you would need the following: CN=fortigate,OU=Service Accounts,OU=Users,OU=MyBusiness,DC=domain,DC=local type the password for your service accountThis should be all that you require. one thing to keep an eye on is typo’s when doing the User DN this will stop you from being able to logon with an SSL-VPN or anything for that matter!If you get an error in the logs for SSL-VPN saying no_matching_policy then you will have a typo somewhere." }, { "title": "Server 2003 Reinstall Terminal Services Licensing.", "url": "/posts/server-2003-reinstall-terminal-services-licensing/", "categories": "Microsoft, Windows, Terminal Services", "tags": "terminal, rds, licensing", "date": "2011-08-18 10:00:00 +0100", "snippet": "I came across an issue today where I needed to reinstall terminal services licensing but when you do this licensing is lost and needs to be re-applied.I managed to resolve this issue by copying the...", "content": "I came across an issue today where I needed to reinstall terminal services licensing but when you do this licensing is lost and needs to be re-applied.I managed to resolve this issue by copying the licensing db to a different folder and then re-installing terminal services and copying it back. stop Terminal Services Licensing service Copy c:\\windows\\system32\\LServer\\TLSLic.edb Paste the db to a different location Uninstall Terminal Services Licensing from add remove components Re-Install Terminal Services Licensing stop Terminal Services Licensing service copy the TLSLic.edb back to c:\\windows\\system32\\LServer\\ overwriting the new db that is in there start Terminal Services Licensing serviceNow you will notice that TS Licensing is working and all of your licences still work. You CANNOT move this to another server it is registered to that Licensing server!!!" }, { "title": "Warning: Cannot modify header information &#8211; headers already sent by&#8230;", "url": "/posts/warning-cannot-modify-header-information-headers-already-sent-by/", "categories": "", "tags": "", "date": "2011-08-08 00:00:00 +0100", "snippet": "Ok so today i was doing some PHP coding and get the dreaded header error caused me a bit of a headache as i needed to redirect some pages. After a bit of searching i managed to find an alternative ...", "content": "Ok so today i was doing some PHP coding and get the dreaded header error caused me a bit of a headache as i needed to redirect some pages. After a bit of searching i managed to find an alternative to using:header(location:\"index.php\");So to get rid of the error that this produces simply change it to any of the below:ob_start();//scriptheader(\"Location:file.php\");ob\\_end\\_flush();ORif ($success){echo '&lt;META HTTP-EQUIV=\"Refresh\" Content=\"0; URL=success.php\"&gt;';exit;}else{echo '&lt;META HTTP-EQUIV=\"Refresh\" Content=\"0; URL=retry.php\"&gt;';exit;}ORprintf(\"&lt;script&gt;location.href=&amp;#8217;errorpage.html'&lt;/script&gt;\");I used the last option as I found this worked best compared to the others with my program however they may all work well for your application" }, { "title": "Mapping a network drive in NT4 with logon credentials", "url": "/posts/mapping-a-network-drive-in-nt4-with-logon-credentials/", "categories": "", "tags": "", "date": "2011-07-28 00:00:00 +0100", "snippet": "Ok so today I had a customer come to me saying that when they map a network drive in NT4 the user details don’t get remembered when the pc is rebooted.Here is a simple solution to the issue we have...", "content": "Ok so today I had a customer come to me saying that when they map a network drive in NT4 the user details don’t get remembered when the pc is rebooted.Here is a simple solution to the issue we have been having:[crayon lang=”cmd”]net use I: \\SERVERNAME\\SHARENAME /User:DOMAIN\\username password[/crayon]run this at startup or as a logon script and the issue will be no more." }, { "title": "Send on Behalf and Send As", "url": "/posts/send-on-behalf-and-send-as/", "categories": "Microsoft, Exchange", "tags": "send, behalf, exchange, outlook", "date": "2011-07-27 10:20:00 +0100", "snippet": "Send on Behalf and Send As are similar in fashion. Send on Behalf will allow a user to send as another user while showing the recipient that it was sent from a specific user on behalf of another us...", "content": "Send on Behalf and Send As are similar in fashion. Send on Behalf will allow a user to send as another user while showing the recipient that it was sent from a specific user on behalf of another user. What this means, is that the recipient is cognitive of who actually initiated the sending message, regardless of who it was sent on behalf of. This may not be what you are looking to accomplish. In many cases, you may want to send as another person and you do not want the recipient to be cognitive about who initiated the message. Of course, a possible downside to this, is that if the recipient replies, it may go to a user who did not initiate the sent message and might be confused depending on the circumstances. Send As can be useful in a scenario where you are sending as a mail-enabled distribution group. If someone replies, it will go to that distribution group which ultimately gets sent to every user who is a part of that distribution group. This article will explains how to use both methods.Send on BehalfThere are three ways to configure Send on Behalf. The first method is by using Outlook Delegates which allows a user to grant another user to Send on Behalf of their mailbox. The second method is having an Exchange Administrator go into the Exchange Management Shell (EMS) and grant a specific user to Send on Behalf of another user. The third and final method is using the Exchange Management Console (EMC).Outlook DelegatesThere are major steps in order to use Outlook Delegates. The first is to select the user and add him as a delegate. You then must share your mailbox to that user. Go to Tools and choose Options Go to the Delegates Tab and click Add Select the user who wish to grant access to and click Add and then Ok There are more options you can choose from once you select OK after adding that user. Nothing in the next window is necessary to grant send on behalf. When back at the main Outlook window, in the Folder List, choose your mailbox at the root level. This will appear as Mailbox – Full Name Right-click and choose Change Sharing Permissions Click the Add button Select the user who wish to grant access to and click Add and then Ok In the permissions section, you must grant the user at minimum, Non-editing Author.Exchange Management Shell (EMS)This is a fairly simple process to complete. It consists of running only the following command and you are finished. The command is as follows:Set-Mailbox UserMailbox -GrantSendOnBehalfTo UserWhoSendsExchange Management Console (EMC) Go to Recipient Management and choose Mailbox Choose the mailbox and choose Properties in Action Pane Go to the Mail Flow Settings Tab and choose Delivery Options Click the Add button Select the user who wish to grant access to and click Add and then OkSend AsAs of Exchange 2007 SP1, there are two ways to configure SendAs. The first method is having an Exchange Administrator go into the Exchange Management Shell (EMS) and grant a specific user to SendAs of another user. The second and final method (added in SP1) is using the Exchange Management Console (EMC).Exchange Management Shell (EMS)The first method is to grant a specific user the ability to SendAs as another user. It consists of running only the following command and you are finished. The command is as follows:```powershellAdd-ADPermission UserMailbox -ExtendedRights Send-As -user UserWhoSends```Exchange Management Console (EMC) Go to Recipient Management and choose Mailbox Choose the mailbox and choose Manage Send As Permissions in Action Pane Select the user who wish to grant access to and click Add and then OkMiscellaneous InformationNo “From:” ButtonIn order for a user to Send on Behalf or Send As another user, their Outlook profile must be configured to show a From: button. By default, Outlook does not show the From: button. In order to configure a user’s Outlook profile to show the From: button:RepliesIf you are sending as another user, the recipient user might reply. By default, Outlook is configured to set the reply address to whoever is configured as the sending address. So if I am user A sending on behalf of user B, the reply address will be set to user B. If you are the user initiating the sending message, you can configure your Outlook profile to manually configure the reply address.Conflicting MethodsIf you are configuring Send on Behalf permissions on the Exchange Server, ensure that the user is not trying to use the Outlook delegates at the same time. Recently, at a client, I was given the task to configure Send As as well as Send on Behalf. As I was configuring Send As on the server, I found out that the client was attempting to use Outlook Delegates at the same time. Send As would not work. Once the user removed the user from Outlook Delegates and removed permissions for that user at the root level of your mailbox that appears as Mailbox – Full Name, Send As began to work. So keep in mind, if you are configuring Send As or Send on Behalf, use only one method for a specific user.SendAs DisappearingIf you are in a Protected Group, something in Active Directory called SDProp will come by every hour and remove SendAs permissions on users in these protected groups.  What security rights are configured on these security accounts are determined based on what security rights are assigned on the adminSDHolder object which exists in each domain.  The important part for you to remember is that every hour, inheritance on these protected groups will be removed and SendAs will be wiped away.A good blog article explaining what adminSDHolder and SDprop are and what Protected Groups  is located here." }, { "title": "The Missing Manual Part 1: Veeam B & R Direct SAN Backups", "url": "/posts/the-missing-manual-part-1-veeam-b-r-direct-san-backups/", "categories": "", "tags": "", "date": "2011-07-22 00:00:00 +0100", "snippet": "One thing that I had problems with the first time I installed Veeam was the ability to backup Virtual Machines directly from the SAN. Meaning that instead of proxying the data through an ESXi host,...", "content": "One thing that I had problems with the first time I installed Veeam was the ability to backup Virtual Machines directly from the SAN. Meaning that instead of proxying the data through an ESXi host, the data would flow from SAN to backup server directly. The benefits of this process are very clear… reduced CPU and network load on the ever so valuable ESXi resources.The problem is that by default this just doesn’t work with Veeam if you haven’t properly setup your backup server. I will try and keep this process simple, and vendor agnostic ( from a SAN point of view).The first step to making the vStorage API “SAN backup” work is to make sure your backup server has the Microsoft iSCSI initiator installed. It is already installed by default on Windows 2008 server, however for windows 2003 server you will need to go to Microsoft to download the latest version. You will need to configure your SAN to allow the IQN address of the iSCSI initiator to have access to the volumes on the SAN… this process is different for each vendor. See screen shot on how to find this in the Configuration tab of the iscsi initiatorAfter installing MS iSCSI initiator, and setting up your SAN, we need to configure it to see the SAN volumes; do this by opening the “iSCSI initiator” option from control panel. At the top of the main tab there is a field where you can put your SAN’s IP address, enter that now and then press Quick Connect. Shortly a list of all of the volumes that your backup server has access to should appear, once they do select each one and press the “connect” button. Because the volumes are formatted VMFS windows will not show them in My Computer, but if you go to Disk Management inside of Computer manager you should now see that the backup server can see these volumes.Update: A note from the Veeam Team “One thing that we (Veeam) recommends is to disable automount on your Windows backup server. To do this open up a command prompt and enter in diskpart. Hit enter and then type “Automount disable”. This is to ensure that the Windows server doesn’t try and format the volumes at all. However, before any of this is done if you can through your SAN software, give the Veeam Backup server Read-Only access to your VMFS volumes.”After preforming these steps go ahead and configure Veeam to use the SAN backup option, and you should notice (especially if you have separate NICs for the SAN network) that all of your data is moving through the SAN directly to the backup server without proxying through the ESXi hosts." }, { "title": "Killing a Windows service that hangs on \"stopping\"", "url": "/posts/killing-a-windows-service-that-seems-to-hang-on-stopping/", "categories": "", "tags": "", "date": "2011-07-19 00:00:00 +0100", "snippet": "It sometimes happens (and it’s not a good sign most of the time): you’d like to stop a Windows Service, and when you issue the stop command through the SCM (Service Control Manager) or by using the...", "content": "It sometimes happens (and it’s not a good sign most of the time): you’d like to stop a Windows Service, and when you issue the stop command through the SCM (Service Control Manager) or by using the ServiceProcess classes in the .NET Framework or by other means (net stop, Win32 API), the service remains in the state of stopping and never reaches the stopped phase. It’s pretty simple to simulate this behaviour by creating a Windows Service in C# (or any .NET language whatsoever) and adding an infinite loop in the Stop method. The only way to stop the service is by killing the process then. However, sometimes it’s not clear what the process name or ID is (e.g. when you’re running a service hosting application that can cope with multiple instances such as SQL Server Notification Services). The way to do it is as follows: Go to the command-prompt and query the service (e.g. the SMTP service) by using sc: sc queryex SMTPSvc This will give you the following information: &gt;SERVICE_NAME: SMTPSvc TYPE               : 20  WIN32_SHARE_PROCESS STATE              : 4  RUNNING (STOPPABLE, PAUSABLE, ACCEPTS_SHUTDOWN) WIN32_EXIT_CODE    : 0  (0x0) SERVICE_EXIT_CODE  : 0  (0x0) CHECKPOINT         : 0x0 WAIT_HINT          : 0x0 PID                : 388 FLAGS              : or something like this (the state will mention stopping). Over here you can find the process identifier (PID), so it’s pretty easy to kill the associated process either by using the task manager or by using taskkill: taskkill /PID 388 /F where the /F flag is needed to force the process kill (first try without the flag).&lt;/li&gt; &lt;/ol&gt; " }, { "title": "Synchronise time with external NTP server on Windows Server", "url": "/posts/synchronize-time-with-external-ntp-server-on-windows-server/", "categories": "Microsoft, Windows", "tags": "ntp, time, sync, windows, server", "date": "2011-07-08 10:15:00 +0100", "snippet": "Time synchronization is an important aspect for all computers on the network. By default, the clients computers get their time from a Domain Controller and the Domain Controller gets his time from ...", "content": "Time synchronization is an important aspect for all computers on the network. By default, the clients computers get their time from a Domain Controller and the Domain Controller gets his time from the domain’s PDC Operation Master. Therefore the PDC must synchronize his time from an external source. I usually use the servers listed at the NTP Pool Project website. Before you begin, don’t forget to open the default UDP 123 port (in- and outbound) on your firewall.First, locate your PDC Server. Open the command prompt and type:netdom /query fsmoLog in to your PDC Server and open the command prompt.run the following command:net stop w32timeConfigure the external time sources, type:w32tm /config /manualpeerlist:&amp;#8221;0.pool.ntp.org 1.pool.ntp.org 2.pool.ntp.org&amp;#8221;,0x8 /syncfromflags:MANUALrun the following command:net start w32timeRe sync the time services, type:w32tm /resync /nowaitTo check that the command has worked run the following:**_w32tm /query /configuration_** When doing this on SBS you may get an access denied error if you do remove: /reliable:yes from the line on number 3." }, { "title": "How to Make the Shutdown Button Unavailable with Group Policy", "url": "/posts/how-to-make-the-shutdown-button-unavailable-with-group-policy/", "categories": "Microsoft, Windows, GPO", "tags": "gpo, windows, shutdown, group, policy", "date": "2011-07-08 10:15:00 +0100", "snippet": "You can use Group Policy Editor to make the Shutdown button unavailable in the Log On to Windows dialog box that appears when you pressCTRL+ALT+DELETE on the Welcome to Windows screen.To Edit the L...", "content": "You can use Group Policy Editor to make the Shutdown button unavailable in the Log On to Windows dialog box that appears when you pressCTRL+ALT+DELETE on the Welcome to Windows screen.To Edit the Local Policy on a Windows 2000-Based ComputerTomake the Shutdown button unavailable on a standalone Windows2000-based computer: Click Start, and then click Run. In the Open box, type gpedit.msc, and then click OK. Expand Computer Configuration, expand Windows Settings, expand Security Settings, expand Local Policies, and then click Security Options. In the right pane, double-click Shutdown:Allow system to be shut down without having to log on. Click Disabled, and then click OK.NOTE: If domain-level policy settings are defined, they may override this local policy setting. Quit Group Policy Editor. Restart the computer.To Edit the Group Policy in a DomainTo edit a domain-widepolicy to make the Shutdownbutton unavailable:: Start the Active Directory Users and Computers snap-in. To do this, click Start, point toPrograms, point to Administrative Tools, and then click Active Directory Users and Computers. In the console, right-click your domain, and then click Properties. Click the Group Policytab. In the Group Policy Object Links box, click the group policy for which you want to apply this setting. For example, click Default Domain Policy. Click Edit. Expand User Configuration, expand Administrative Templates, and then clickStart Menu &amp; Taskbar. In the right pane, double-click Disable and remove the Shut Down command. Click Enabled, and then click OK. Quit the Group Policy editor, and then click OK.TroubleshootingGroup Policy changes are not immediatelyenforced. Group Policy background processing can take up to 5 minutes to berefreshed on domain controllers, and up to 120 minutes to be refreshed on clientcomputers. To force background processing of Group Policy settings, use theSecedit.exe tool. To do this: Click Start, and then click Run. In the Open box, type cmd, and then click OK. Type secedit /refreshpolicy user_policy, and then press ENTER. Type secedit /refreshpolicy machine_policy, and then press ENTER. Type exit, and then press ENTER to quit the command prompt." } ]
